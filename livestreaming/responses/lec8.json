[
    {
        "Conversation History": "\t",
        "GPT Response": "No, for this specific assignment, you should avoid restarting the kernel and rerunning all cells unless absolutely necessary. Due to finite computing resources on DataHub, certain cells, especially those involving the Natural Language Processing (NLP) model, may take a long time to run. It's important to be patient and wait for the cells to complete rather than rerunning them, as this can cause resource allocation issues and potentially crash the system.",
        "Recent Audio": "each one of those tabs will be paying. And then it's not that the pay is that important. It's also important that our budget is only. For 1200 students, so if all of you opens up a second tab, it's as if that we have 2400 students and we don't have budget for it, right? Budget in terms of resource, resource allocation and the date and that we might crash data hub again. So don't reopen, wait for each one of the cells to run as usually it will take a long time and then also a lot of time. We also want to make sure that you don't restart the kernel and run everything over and over again. Only restart kernel only if you. That because every time that you run everything end to end, it will cost us, it will allocate resources to you and then it will it will cost us. Question in the back. This is just, I mean, generally the amount of resources that you're going to allocate that you're going to basically use is depending on the number of tabs that you have open. But because all of the other homeworks are cheaper in a sense and they don't need resources that much. So it's not going to be that much of A concern for us. It's just going to be a couple of bucks more per student. But this one is going to be very expensive if you all start doing that. So because of that for this specific.",
        "Recent Video": "v\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nX\n\u20ac\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g304406b67a2_0_0\nNarges Norouzi\n0\n7\nV\n:\nab\nT\n3529573\nImportant Note on HW 03:\n.\nDue to a finite computing resources\non DataHub, certain cells that import,\nload, and/or use the Natural\nLanguage Processing (NLP) model\nmay take a long time to run.\nPlease be patient, wait, and avoid\nrestarting the kernel or rerunning\nthese cell more than necessary.\nDo NOT open this assignment in\nHW Logistics\nmultiple tabs or windows. It can\ncause your notebook to crash, as well\nas affect Datahub.\nLecture 8, Data 100 Fall 2024\n2\n...\nV\n2\nS\n2024-09-24 11:13:30\n\nv\nLec 08 - DS100 Fa24 - Visualiza\nx\nlab? - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g304406b67a2_0_0\nNarges Norouzs\n:\n0\nab\nT\n3529573\nImportant Note on HW 03:\n.\nDue to a finite computing resources\non DataHub, certain cells that import,\nload, and/or use the Natural\nLanguage Processing (NLP) model\nmay take a long time to run.\nPlease be patient, wait, and avoid\nrestarting the kernel or rerunning\nthese cell more than necessary.\nDo NOT open this assignment in\nHW Logistics\nmultiple tabs or windows. It can\ncause your notebook to crash, as well\nas affect Datahub.\nLecture 8, Data 100 Fall 2024\n2\n...\n2\n>\nS\nV\n2024-09-24 11:13:45\n",
        "Retrieved Audio": "We can keep Data Hub active because last semester data Hub, we basically crashed data Hub for almost a day up until the point that we could bump up the resources and then restart the homework so that that doesn't happen. I want all of you to think about all of the precautions that we have here. Note that each one of the cells that you're running, they might take a, it might take a long time for those cells to run. Don't rerun stuff. Just wait patiently until you get the output of each one of the cells.\nWe can keep Data Hub active because last semester data Hub, we basically crashed data Hub for almost a day up until the point that we could bump up the resources and then restart the homework so that that doesn't happen. I want all of you to think about all of the precautions that we have here. Note that each one of the cells that you're running, they might take a, it might take a long time for those cells to run. Don't rerun stuff. Just wait patiently until you get the output of each one of the cells.\nFor 1200 students, so if all of you opens up a second tab, it's as if that we have 2400 students and we don't have budget for it, right? Budget in terms of research resource allocation and the date and that we might crash data hub again. So don't reopen, wait for each one of the cells to run as usually it will take a long time and then also a lot of time. We also want to make sure that you don't restart the kernel and run everything over and over again. Only restart kernel only if you.",
        "Retrieved Notes": "e server, the kernel will \u00e2\u0080\u009cdie\u00e2\u0080\u009d. This is an indication that there is a mistake in your code that you need to fix.\n\n### [](https://dsc10.com/debugging/#how-do-i-quickly-run-all-the-cells-in-a-notebook)How do I quickly run all the cells in a notebook?\n\nGo to the Cell menu in the top toolbar, then \u00e2\u0080\u009cRun All.\u00e2\u0080\u009d You can also select a certain cell and run all cells before this point, or run all cells after this point. You should run all the cells in your notebook before submitting to confirm that you pa\ncan do to get the notebook running again: Make sure your internet connection is working. If you can, restart your server by clicking the button at the top right labeled \u00e2\u0080\u009cControl Panel\u00e2\u0080\u009d, then select \u00e2\u0080\u009cStop My Server\u00e2\u0080\u009d, followed by \u00e2\u0080\u009cStart My Server\u00e2\u0080\u009d. If that doesn\u00e2\u0080\u0099t work, try restarting your computer and using a different browser. Whenever you resume working on a notebook, run all cells you\u00e2\u0080\u0099ve previously completed. If your problem persists after trying all these steps, please notify us o\ndoes-running-a-particular-cell-cause-my-kernel-to-die)\n    2.  [How do I quickly run all the cells in a notebook?](https://dsc10.com/debugging/#how-do-i-quickly-run-all-the-cells-in-a-notebook)\n    3.  [Why does `grader.check_all()` fail, if all previous tests passed?](https://dsc10.com/debugging/#why-does-gradercheck_all-fail-if-all-previous-tests-passed)\n    4.  [Why does a notebook test fail now, when it passed before and I didn\u00e2\u0080\u0099t change my code?](https://dsc10.com/debugging/#why-does-a-notebook-test-",
        "Retrieved Video": "this assignment in\nHW Logistics\nmultiple tabs or windows. It can\ncause your notebook to crash, as well\nas affect Datahub.\nLecture 8, Data 100 Fall 2024\n2\n...\nV\n2\nS\n2024-09-24 11:13:30\n\nthis assignment in\nHW Logistics\nmultiple tabs or windows. It can\ncause your notebook to crash, as well\nas affect Datahub.\nLecture 8, Data 100 Fall 2024\n2\n...\nV\n2\n>\nS\n2024-09-24 11:13:25\n\nen this assignment in\nHW Logistics\nmultiple tabs or windows. It can\ncause your notebook to crash, as well\nas affect Datahub.\nLecture 8, Data 100 Fall 2024\n2\n(8\n...\n<\n2\nS\n2024-09-24 11:11:16\n",
        "Question": "Should we restart kernel and rerun all cells before submitting as in the other hws?",
        "Timestamp": 242,
        "TA Response": "This is okay! We just want you to try and avoid rerunning all cells more often than is required, due to the amount of resources we have.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "It seems there might be an issue with the audio on Zoom. If you are experiencing difficulties hearing the lecture, it might be helpful to check your audio settings or try reconnecting to the Zoom session. If the problem persists, consider reaching out to the course staff for assistance.",
        "Recent Audio": "also want to make sure that you don't restart the kernel and run everything over and over again. Only restart kernel only if you. That because every time that you run everything end to end, it will cost us, it will allocate resources to you and then it will it will cost us. Question in the back. This is just, I mean, generally the amount of resources that you're going to allocate that you're going to basically use is depending on the number of tabs that you have open. But because all of the other homeworks are cheaper in a sense and they don't need resources that much. So it's not going to be that much of A concern for us. It's just going to be a couple of bucks more per student. But this one is going to be very expensive if you all start doing that. So because of that for this specific. Homework, because it has a language model that we're hosting and you're hosting actually local in your instance or your instance in the hub. We want to be mindful of the resources. Good question. But other homeworks are not as expensive. This is the most expensive homework that probably you will see in your entire data science kind of course work here at Berkeley. So with that out of the way. We are going into visualization too, so this is the second lecture in this sequence, but I'm flying. What I'm planning to do is to go a little bit slower.",
        "Recent Video": "v\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g112e46d2196_1_207\nNarges Norouzi\n:\nO\nabg\nT\n3529573\nLECTURE 8\nVisualization II\nTransformations, and Visualization Theory\nData 100/Data 200, Fall 2024 @ UC Berkeley\nNarges Norouzi and Joseph E. Gonzalez\nContent credit: Acknowledgments\n3\nCC\n...\n3\n>\nS\n2024-09-24 11:14:57\n\nv\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g2555b8b1865_0_0\nNarges Norouzi\n...\n0\nA\nabp\nT\n3529573\nLearning more visualization functions\nVisualizing relationships\n.\nTransforming data for conveying a\n.\nmore coherent message in\nvisualization\nGoals for this\nVisualization theory and information\nchannel\nLecture\nLecture 8, Data 100 Fall 2024\n4\nCC\n4\n> :\nS\n2024-09-24 11:15:06\n",
        "Retrieved Audio": "All right, good morning, Data 100. Welcome to our Lecture 8. Please log into a slide. Oh, there's going to be a number of fun polls today because it's all going to be about visualization theory to make sense of like how can we evaluate how good or sensitive our visual system is and then how to adapt the visualizations that we're creating based on that perception?\nBefore I start jumping into the content of today's lecture, I want to make some reminders here.\nWe are going into visualization too, so this is the second lecture in this sequence, but I'm flying. What I'm planning to do is to go a little bit slower.",
        "Retrieved Notes": "lectures** held on Tuesdays and Thursdays, 11:00am-12:30pm, hybrid: in-person in Wheeler 150 and broadcasted live on Zoom. All session recordings, slides, activities, and examples will be uploaded to the course website within 24 hours of the lecture.\n\n**Lecture participation**: While synchronous lecture attendance (in-person or through Zoom) is not mandatory, we expect you to \u00e2\u0080\u009cparticipate\u00e2\u0080\u009d in lecture by answering lecture poll questions in one of two ways: (1) synchronously during the lecture as poll q\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
        "Retrieved Video": "V\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n->\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g304406b67a2_0_0\nNarges Norouzi\nO\n7\n:\nab\nT\nslido\nA\nSynchronous\nJoin at\nslido.com\n#3529 573\n2024-09-24 11:11:11\n\nen this assignment in\nHW Logistics\nmultiple tabs or windows. It can\ncause your notebook to crash, as well\nas affect Datahub.\nLecture 8, Data 100 Fall 2024\n2\n(8\n...\n<\n2\nS\n2024-09-24 11:11:16\n\nv\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g2555b8b1865_0_0\nNarges Norouzi\n...\n0\nA\nabp\nT\n3529573\nLearning more visualization functions\nVisualizing relationships\n.\nTransforming data for conveying a\n.\nmore coherent message in\nvisualization\nGoals for this\nVisualization theory and information\nchannel\nLecture\nLecture 8, Data 100 Fall 2024\n4\nCC\n4\n> :\nS\n2024-09-24 11:15:06\n",
        "Question": "Can anyone on the zoom hear anything?",
        "Timestamp": 282,
        "TA Response": "yes",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The lecture notes and audio suggest that HW03 is resource-intensive due to hosting a language model locally on your instance in the hub. While the lecture doesn't specifically address older laptops, it emphasizes being mindful of resources and suggests that the homework is the most expensive in terms of resource usage. Therefore, it might be challenging for older laptops to handle HW03 efficiently, and it's recommended to use a device with adequate resources or access device lending options if needed.",
        "Recent Audio": "don't need resources that much. So it's not going to be that much of A concern for us. It's just going to be a couple of bucks more per student. But this one is going to be very expensive if you all start doing that. So because of that for this specific. Homework, because it has a language model that we're hosting and you're hosting actually local in your instance or your instance in the hub. We want to be mindful of the resources. Good question. But other homeworks are not as expensive. This is the most expensive homework that probably you will see in your entire data science kind of course work here at Berkeley. So with that out of the way. We are going into visualization too, so this is the second lecture in this sequence, but I'm flying. What I'm planning to do is to go a little bit slower. Lower today compared to what we've seen in the previous lectures, kind of and kind of take a look at how we can build relationships or visualize relationships between variables. And then how can we make sense of different types of transformations that we need to have in order to have a very solid data set and solid kind of visualization moving forward in the modelling and also sharing that broadly. In in your reports or sharing it with the stakeholders. At the end, we're going to come and talk a little bit about visualization theory. How do you how you can use?",
        "Recent Video": "v\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g2555b8b1865_0_0\nNarges Norouzi\n...\n0\nA\nabp\nT\n3529573\nLearning more visualization functions\nVisualizing relationships\n.\nTransforming data for conveying a\n.\nmore coherent message in\nvisualization\nGoals for this\nVisualization theory and information\nchannel\nLecture\nLecture 8, Data 100 Fall 2024\n4\nCC\n4\n> :\nS\n2024-09-24 11:15:06\n\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\n+\nX\n->\nC\n% docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g2686122ddf1_0_3\nNarges Norouzi\n...\n0\nWhere Are We?\nab\nT\n3529573\nQuestion &\nData\nProblem\nAcquisition\nFormulation\nPrediction and\nExploratory\nInference\nData Analysis\nReports, Decisions,\nand Solutions\n(today)\nData Wrangling\nWorking with Text Data\nPlots and variables\nIntro to EDA\nRegular Expressions\nSeaborn\nViz principles\nKDE\nTransformations\n(Part I: Processing Data)\n(Part II: Visualizing and Reporting Data)\n5\n...\nCC\n5\nS\n2024-09-24 11:15:49\n",
        "Retrieved Audio": "We can keep Data Hub active because last semester data Hub, we basically crashed data Hub for almost a day up until the point that we could bump up the resources and then restart the homework so that that doesn't happen. I want all of you to think about all of the precautions that we have here. Note that each one of the cells that you're running, they might take a, it might take a long time for those cells to run. Don't rerun stuff. Just wait patiently until you get the output of each one of the cells.\nFor 1200 students, so if all of you opens up a second tab, it's as if that we have 2400 students and we don't have budget for it, right? Budget in terms of research resource allocation and the date and that we might crash data hub again. So don't reopen, wait for each one of the cells to run as usually it will take a long time and then also a lot of time. We also want to make sure that you don't restart the kernel and run everything over and over again. Only restart kernel only if you.\nSo don't rerun the stuff. Also, the budget that we have is going to be depending on each number of Windows tabs that are open on your laptop, right? So if you want to work on the homework, only open it in one tab. Don't open it in multiple tabs because the number of tabs that you're creating is like as if we have more students working on that homework. And per each one of those tabs will be paying. And then it's not that the pay is that important. It's also important that our budget is only.",
        "Retrieved Notes": "erences. However, despite our best efforts, we might slip up, hopefully inadvertently. If you are concerned about classroom environment issues created by the staff or overall class dynamic, please feel free to talk to us about it. The instructors in particular welcome any comments or concerns regarding conduct of the course and the staff. See [below](https://ds100.org/fa24/syllabus/#office-hours-and-communication) for how to best reach us.\n\n### Device Lending options\n\nStudents can access device lending opti\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
        "Retrieved Video": "his assignment in\nHW Logistics\nmultiple tabs or windows. It can\ncause your notebook to crash, as well\nas affect Datahub.\nLecture 8, Data 100 Fall 2024\n2\n...\n2\n>\nS\nV\n2024-09-24 11:13:45\n\nhis assignment in\nHW Logistics\nmultiple tabs or windows. It can\ncause your notebook to crash, as well\nas affect Datahub.\nLecture 8, Data 100 Fall 2024\n2\n...\n2\n>\nS\nV\n2024-09-24 11:13:45\n\nthis assignment in\nHW Logistics\nmultiple tabs or windows. It can\ncause your notebook to crash, as well\nas affect Datahub.\nLecture 8, Data 100 Fall 2024\n2\n...\nV\n2\n>\nS\n2024-09-24 11:13:25\n",
        "Question": "Will hw03 run on older laptops ok? ",
        "Timestamp": 323,
        "TA Response": "yes this should be fine.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, you can plot scatter plots for two quantitative discrete variables. Scatter plots are used to visualize the relationship between two quantitative variables, whether they are continuous or discrete. However, when dealing with discrete variables, overplotting can become an issue, as many data points may overlap, making it difficult to discern the actual number of points. Techniques such as jittering or using transparency can help mitigate overplotting in such cases.",
        "Recent Audio": "very similar. So remember that what we had was with the math plug lab. We could just call the plotting function and we would pass the sequences that need to be plotted. So for example, in this case I have, I can pass a sequence, which means it can be an area, it can be. Is there can be a list object of X values and Y values that need to be plotted? With this, with the seaborn, their relationship with this was a slightly different. You just need to remember about plotting function. What's the name of that plotting function that will do the thing that you want. So here in this case is the scatter plot and the outline was that we always pass the data frame, we always pass the column name. So that's the string value, the column name that goes on the X axis, the column name that goes on the Y axis. And then if you're feeling fancy, we could also say that there is 1/3 dimension that I want you to plot and that's the color. So Hugh references the colour, which basically means that OK, depending on different categories of values that you have in the column, that's called hue column, go ahead and color different stuff. And in that case, you're effectively using tree information channels, X information channel, Y information channel and also hue. So you're using the color as well. So if I go ahead and just plot a scatter plot using the Seabourne version.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_14\nNarges Norouzi\n...\n0\nK\nScatter Plots\nab\nT\nScatter plots are used to reveal relationships between two quantitative variables\n3529573\nDocumentation].\nPlot one quantitative continuous variable on the x-axis, and second quantitative\ncontinuous variable on the y-axis.\nEach scatter point represents one datapoint in the dataset.\nplt.scatter(x_values, y_values)\nsns.scatterplot(data=df, x=\"x_column\",\n1\ny=\"y_column\", hue=\"hue_column\")\n12\nNC\n2024-09-24 11:22:57\n\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_14\nNarges Norouzi\n-\n...\n7\nScatter Plots\nab\nT\nScatter plots are used to reveal relationships between two quantitative variables\n3529573\nDocumentation].\nPlot one quantitative continuous variable on the x-axis, and second quantitative\ncontinuous variable on the y-axis.\nEach scatter point represents one datapoint in the dataset.\n100\nplt.scatter(x_values, y_values)\n80\nContinent\nAfrica\n60\nAsia\nEurope\nsns. scatterplot(data=df, x=\"x_column\",\nN. America\nOceania\ny=\"y_column\", hue=\"hue_column\")\n40\nS. America\nFemale adult literacy rate\n20-\n-10\n-5\n0\n5\n% growth per capita\n12\n2024-09-24 11:23:25\n",
        "Retrieved Audio": "40,000 GNI So that's kind of how we try to plot distributions. But moving away from distributions, what if we want to model the relationship between multiple different variables as opposed to just looking at one variable? So what we can do is something that you had seen before in data 8IN data 8, you had seen a scatter plots and scatter plots were kind of showing that what's the what's the association between two different variables so.\nHere I call this variable X1, this variable, or this dimension X2, and for every given data point, it's the relationship between X1 and X2 that you're plotting as a single point on the plot. Usually with a scatter plot, we could basically plot the entire data set or the association between X1 and X2 or two variables and it will show us the trend. So in this case, for example, you're seeing that I have a simple linear.\nSo if I go ahead and just plot a scatter plot using the Seabourne version.",
        "Retrieved Notes": "distributions. Going beyond this, we want to understand the relationship between pairs of numerical variables.\n#### Scatter Plots\n**Scatter plots** are one of the most useful tools in representing the relationship between **pairs** of quantitative variables. They are particularly important in gauging the strength, or correlation, of the relationship between variables. Knowledge of these relationships can then motivate decisions in our modeling process.\nIn `matplotlib`, we use the function `plt.scatter` to g\nth\");\n```\n##### Overplotting \nAlthough the plots above communicate the general relationship between the two plotted variables, they both suffer a major limitation \u00e2\u0080\u0093 **overplotting**. Overplotting occurs when scatter points with similar values are stacked on top of one another, making it difficult to see the number of scatter points actually plotted in the visualization. Notice how in the upper righthand region of the plots, we cannot easily tell just how many points have been plotted. This makes our visu\ny each unique value.\nSpecifically, we can say this histogram suffers from **overplotting** as we are unable to interpret the plot and gain any meaningful insight. \nRather than bar plots, to visualize the distribution of a continuous variable, we use one of the following types of plots:\n- Histogram\n- Box plot\n- Violin plot\n### Box Plots and Violin Plots\nBox plots and violin plots are two very similar kinds of visualizations. Both display the distribution of a variable using information about **quartiles**.\nI",
        "Retrieved Video": "Lec 08 - DS100 Fa24 - Visualiza\nx\nlab? - JupyterLab\nX\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_8\nNarges Norouzi\n...\n0\nFrom Distributions to Relationships\nab\nT\nUp until now, we focused exclusively on visualizing variable distributions.\n3529573\nNow we will visualize relationships between variables. In other words, how do sets of two (or\nmore) variables vary in relation to one another?\n\u00a3\n10\nNC\n2024-09-24 11:20:28\n\nues, y_values)\nsns. scatterplot (data=df, x=\"x_column\", \\\ny=\"y_column\", hue=\"hue_column\")\n12\nC\nNC\n2024-09-24 11:22:13\n\nues, y_values)\nsns. scatterplot (data=df, x=\"x_column\", \\\ny=\"y_column\", hue=\"hue_column\")\n12\nC\nNC\n2024-09-24 11:22:13\n",
        "Question": "can you not plot scatterplots for 2 quantitative discrete variables?",
        "Timestamp": 785,
        "TA Response": "Yes, you can! Scatterplots are typically used for quantitative continuous because it is easier to see visual trends, but we can also plot discrete variables against each other. In these plots, the points will only lie in specific discrete steps instead of covering the whole range. You may suffer from overplotting, as multiple points are likely to lie in the same spots.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The backslash (`\\`) in the code on the slides is used to indicate that the command continues onto the next line. This is a common practice in Python to improve readability when writing long lines of code, allowing the code to be broken into multiple lines without causing syntax errors.",
        "Recent Audio": "data. It will assume that you want to use the X dimension in order to predict the Y dimension. It will find the trend with the linear trend and it will put that you're not seeing that exactly, but there is this kind of confidence interval or this cone area which will show you that OK, this is the trend and this is. 95% confidence interval for where the data points could be if you're modeling the trend using this linear regression. So those are kind of the lines and the small confidence interval that you see on the plot as well. So the function exactly you call LM plot, you pass the data from, you pass the columns that you want to be plotted, the names, and it will plot that for you. The other interesting plot that I like is also joint plot. So joint plot, there are different versions of joint. Kind of similar to this plot that you can plot different kinds of this plot. Here you can also plot different kinds of joint plot, but this is the most basic one. What it does is it will create the joint distribution using the scatter plot and it will also plot the histogram of the distribution of the individual X axis and individual Y axis on the corners as well. So here for example. You're seeing that it shows the histogram or the distribution of the Y axis here on the on the Y scale, and then this is the distribution of.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_34\nNarges Norouzi\n...\n0\nScatter Plot Alternatives\nab\nT\nA\nSeaborn includes several built-in functions for making more complex scatter plots.\n3529573\n-T\n100\n100\nK\n80\n80 -\n60\n60\n40\n40 -\n20\n20 -\nAdult literacy rate: Female: % ages 15 and older: 2005-14\nAdult literacy rate: Female: % ages 15 and older: 2005-14\n-10\n-5\n0\n5\n-10\n-5\n0\n5\nper capita: % growth: 2016\nper capita: % growth: 2016\nsns . Implot (data=df, \\\nsns. jointplot(data=df, \\\nx=\"x_column\", y=\"y_column\")\nx=\"x_column\", y=\"y_column\")\nDocumentation]\n[Documentation]\n14\nNC\n2024-09-24 11:30:28\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\n\u00d7\n1+\n->\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_34\nNarges Norouzi\n...\n0\nScatter Plot Alternatives\nab\nT\nSeaborn includes several built-in functions for making more complex scatter plots.\n3529573\nT\n100\n100\nK\n80\n80\n60\n60\n40\n40 -\n20\n20\nAdult literacy rate: Female: % ages 15 and older: 2005-14\nAdult literacy rate: Female: % ages 15 and older: 2005-14\n-10\n5\n0\n-10\n-5\n0\n5\n-5\nper capita: % growth: 2016\nper capita: % growth: 2016\nsns .Implot (data=df, \\\nsns. jointplot (data=df, \\\nx=\"x_column\", y=\"y_column\")\nx=\"x_column\", y=\"y_column\" )\nDocumentation]\n[Documentation]\n14\nNC\n2024-09-24 11:30:33\n",
        "Retrieved Audio": "So that's basically what's happening. The other way, in order to make sure that the points are not on top of each other is to change the marking slides. So here I'm saying that the size of each one of those dots that you're putting, let that be 15 and 15 is a smaller than the normal value that it said. So it will also help to some extent. But these are kind of general ideas that we're using.\nAll right, good morning, Data 100. Welcome to our Lecture 8. Please log into a slide. Oh, there's going to be a number of fun polls today because it's all going to be about visualization theory to make sense of like how can we evaluate how good or sensitive our visual system is and then how to adapt the visualizations that we're creating based on that perception?\nSo what we had in the previous plot was that there was one major issue, and you'll keep hearing from me that I'll say that, OK, this plot has a lot of over plotting. I can't see this is not a good plot. What over plotting essentially means is that if I go back to the previous slide, you will see that there is a lot of these different points that are basically clumped together in that top right corner. And I don't like that because I can tell.",
        "Retrieved Notes": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
        "Retrieved Video": "V\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n->\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g304406b67a2_0_0\nNarges Norouzi\nO\n7\n:\nab\nT\nslido\nA\nSynchronous\nJoin at\nslido.com\n#3529 573\n2024-09-24 11:11:11\n\nv\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g112e46d2196_1_207\nNarges Norouzi\n:\nO\nabg\nT\n3529573\nLECTURE 8\nVisualization II\nTransformations, and Visualization Theory\nData 100/Data 200, Fall 2024 @ UC Berkeley\nNarges Norouzi and Joseph E. Gonzalez\nContent credit: Acknowledgments\n3\nCC\n...\n3\n>\nS\n2024-09-24 11:14:57\n\nv\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g112e46d2196_1_207\nNarges Norouzi\n:\nO\nabg\nT\n3529573\nLECTURE 8\nVisualization II\nTransformations, and Visualization Theory\nData 100/Data 200, Fall 2024 @ UC Berkeley\nNarges Norouzi and Joseph E. Gonzalez\nContent credit: Acknowledgments\n3\nCC\n...\n3\n>\nS\n2024-09-24 11:14:57\n",
        "Question": "Why is there a \\ in the code on the slides",
        "Timestamp": 1191,
        "TA Response": "The \"\\\" just indicates a new line! So it's the same as if we had those 2 lines of code in one line without the \\.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Deciding the right amount of noise to add depends on the context and purpose of your analysis. If the goal is to prevent overplotting and make trends more visible, you might add enough noise to separate overlapping points without distorting the overall pattern. However, if the noise significantly alters the interpretation of the data or obscures important features, it may be too much. It's important to balance clarity and accuracy, often by experimenting with different levels of noise and assessing their impact on the visualization.",
        "Recent Audio": "version of what we did have before. The way that you can implement it is exactly as what how we would call the KDE plot. Here I would say that I have a KDE plot. But instead of only passing one data point or one column, I would say that I have a data frame and I'm passing you two different columns. So just go ahead and plot it or plot the KDE plot in two dimension using all of these contours that we're seeing. I could also say that fill each one of these KDE contours with the color so I can tell the differences a little bit better. Otherwise they will just give you the outline of these contours and in one color. The inside, so you won't feel the color. So that's kind of the stylistic kind of choice that we made. And if you take a look at where we have the darker regions or darker contours, it's exactly corresponding to where we have most most of the points. So it's pretty interesting. But notice that if I had the issue of over plotting, I could just stick with the scattered plot, jitter it in place and just make sense of what exists in the data or I could go one level advance and then say that I don't care about individual data points but. Want to see the trend? If you want to see the trend, just go with the hexagon kind of plotting in the center or the KDE plot.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nX\n3% docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_52\nNarges Norouzi\n-\n:\n7\nContour Plots\nab\nT\n2-dimensional version of a KDE plot.\n3529573\nSimilar to a topographic map - contour lines represent an area that has the same density of\ndatapoints throughout. Darker colors indicate more datapoints in the region.\n100\n120\n.\n100\n80 -\n80\nDark color - many\n60-\n60 -\ndatapoints\n40\n40\n20\nFemale adult literacy rate\n0 -\n20\n-20 -\n-15\n-10\n-5\n0\n5\n10\nAdult literacy rate: Female: % ages 15 and older: 2005-14\nper capita: % growth: 2016\n-10\n-5\n0\n5\n% growth per capita\nsns.kdeplot(data=df, x=\"x_column\", y=\"y_column\", fill=True)\n16\nCC\nHY\n2024-09-24 11:34:04\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\n\u00d7\n+\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_630\nNarges Norouzi\n...\nSummary\nab T\nVisualization requires a lot of thought!\n3529573\nMany tools for visualizing distributions.\n0\nDistribution of a single variable: rug plot, histogram, density Blot, box, violin.\no Joint distribution of two quantitative variables: scatter plot, hex plot, contour plot.\nThis class primarily uses seaborn and matplotlib.\nO\nPandas also has basic built-in plotting methods.\nO\nMany other visualization libraries exist. plotly is one of them.\nIt very easily creates interactive plots.\nplotly will occasionally appear in lecture code, labs, and assignments!\nNext, we'll go deeper into the theory behind visualization.\n17\nNC\n2024-09-24 11:34:33\n",
        "Retrieved Audio": "You can interpret it. So the question is then going to be that if you're adding a noise, is that going to create inaccuracy in the plot or if you're trying to kind of show some trend to and a stakeholder by adding a noise, you're basically kind of.\nThere are some considerations that you have to have so that.\nBe created for the Y axis to the Y dimension, and then plot the values. OK, so the idea is that because we're adding noise, and because noise is randomly generated from a certain distribution, it will help us kind of move points apart from each other so that they're not exactly plotted on top of each other.",
        "Retrieved Notes": "r model on the noises.\n#### Model Bias\nFinally, the second term is $\\left(g(x)-E\\left[\\hat{Y}(x)\\right]\\right)^2$. What is this? The term $E\\left[\\hat{Y}(x)\\right] - g(x)$ is called the **model bias**.\nRemember that $g(x)$ is the fixed underlying truth and $\\hat{Y}(x)$ is our fitted model, which is random. Model bias therefore measures how far off $g(x)$ and $\\hat{Y}(x)$ are on average over all possible samples.\n$$\\text{model bias} = E\\left[\\hat{Y}(x) - g(x)\\right] = E\\left[\\hat{Y}(x)\\right] - g(x)$$\nThe mo\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of \nDiscussion on Data Granularity\nFrom the description:\n* Monthly measurements are averages of average day measurements.\n* The NOAA GML website has datasets for daily/hourly measurements too.\nThe data you present depends on your research question.\n**How do CO<sub>2</sub> levels vary by season?**\n* You might want to keep average monthly data.\n**Are CO<sub>2</sub> levels rising over the past 50+ years, consistent with global warming predictions?**\n* You might be happier with a **coarser granularity** of average ",
        "Retrieved Video": ".5\n0.00008\n3.0\n0.00006\n2.5\n2.0\nDensity\nDensity\n0.00004\n1.5\n1.0\n0.00002\n0.5\n0.00000\n0.0\n0\n20000\n40000\n60000\n80000\n-20000\n0\n20000\n40000 60000 80000 100000\ngni\ngni\n8\nNC\n2024-09-24 11:18:33\n\n3.5\n0.00008\n3.0\n0.00006\n2.5\n2.0\nDensity\nDensity\n0.00004-\n1.5\n1.0\n0.00002\n0.5\n0.00000\n0.0\n0\n20000\n40000\n60000\n80000\n-20000\n0\n20000\n40000 60000 80000 100000\ngni\ngni\n8\nNC\n2024-09-24 11:19:16\n\n3.5\n0.00008\n3.0\n0.00006\n2.5\n2.0\nDensity\nDensity\n0.00004-\n1.5\n1.0\n0.00002\n0.5\n0.00000\n0.0\n0\n20000\n40000\n60000\n80000\n-20000\n0\n20000\n40000 60000 80000 100000\ngni\ngni\n8\nNC\n2024-09-24 11:19:16\n",
        "Question": "How do you decide how much noise is the right amount?",
        "Timestamp": 1439,
        "TA Response": "This is a subjective choice\u2014 it's really up to you! You want to strike a balance between accurately visualizing the data and avoiding overplotting.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The lecture notes mention that the Gallup Poll is one of the leading polls for election results, and it discusses historical polling methods and challenges. However, there is no specific mention of current or recent polls in the provided context. If you are referring to polls related to the lecture or class activities, it seems there were plans for fun polls during the lecture, but no specific details are provided in the context.",
        "Recent Audio": "want to do visualization and a lot of time what we're interested in doing is to figure out if our data is linearized or not and in order to find trends in our data. So for example, what do I mean by that? If you take a look at this specific data set. It plots GNI with respect to adulthood literacy rate. If we want to go ahead and model this data in your, I think the first reason of why you want to do visualization, you might want to decide about whether your data is ready as in and is in the scale. That is kind of useful for the kind of modeling that we do in Data 100. Specifically, if I want to take a look at the issues that might happen in this data and kind of decide whether the data is ready or not, I might consider. Places that there is a lot of over plotting. When there is over plotting, that means that your data is smooshed in one area. So what do I mean by smooshed in one area? If you take a look at, for example, the area of the data points that I have here, it's really hard to distinguish between the points and know how many data points that I have. And then what's the trend between as I increase the GNI, how does the adult literacy rate for that region is going to change? So we don't want the data to be smashed together in one.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nX\n+\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_73\nNarges Norouzi\nK\n...\n0\n::::\nVisualization Theory\nabo\nT\nA\nRemember our goals of visualization:\n3529573\n1. To help your own understanding of your data/results.\n2. To communicate results/conclusions to others.\nThese are influenced by our choice of visualization and our choices in how to prepare data for\nvisualization.\n200\nWhat problems are there here?\n175\nData is \"smushed\" - hard to interpret,\n150\neven if we jittered.\n125\nDifficult to generalize a clear\n100\nrelationship between the variables.\nAdult literacy rate\n75\n.....\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n19\n2024-09-24 11:38:04\n\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_73\nNarges Norouzi\n...\nO\nVisualization Theory\nabo\nT\nRemember our goals of visualization:\n3529573\n1. To help your own understanding of your data/results.\n2. To communicate results/conclusions to others.\nA\nThese are influenced by our choice of visualization and our choices in how to prepare data for\nvisualization.\n200\nWhat problems are there here?\n175\nData is \"smushed\" - hard to interpret,\n150\neven if we jittered.\n125\nDifficult to generalize a clear\n100\nrelationship between the variables.\nAdult literacy rate\n75\n.....\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n19\nHY\nNC\nBA\n2024-09-24 11:38:28\n",
        "Retrieved Audio": "or not, right? So.\nTell apart what's happening in that area. I can't figure out is there a specific trend for a specific continent or not?\nAll right, good morning, Data 100. Welcome to our Lecture 8. Please log into a slide. Oh, there's going to be a number of fun polls today because it's all going to be about visualization theory to make sense of like how can we evaluate how good or sensitive our visual system is and then how to adapt the visualizations that we're creating based on that perception?",
        "Retrieved Notes": "oters tell the truth? How can we predict turnout? -- still remain, but the *Gallup Poll* uses several tactics to mitigate them. Within their sampling frame of \"civilian, non-institutionalized population\u00e2\u0080\u009d of adults in telephone households in continental U.S., they use random digit dialing to include both listed/unlisted phone numbers and to avoid selection bias. Additionally, they use a within-household selection process to randomly select households with one or more adults. If no one answers, re-call mul\ny had a dismal response rate (about 24\\%); who knows how the other non-respondents would have polled? The *Digest* folded just 18 months after this disaster.\nAt the same time, George Gallup, a rising statistician, also made predictions about the 1936 elections. Despite having a smaller sample size of \u00e2\u0080\u009conly\u00e2\u0080\u009d 50,000 (this is still more than necessary; more when we cover the Central Limit Theorem), his estimate that 56\\% of voters would choose Roosevelt was much closer to the actual result (61\\%). Gallup\n**\"No\"**, then the surveyor knows the **individual didn't cheat**. We assume the individual is comfortable revealing this information. \nGenerally, we can assume that the coin lands heads 50% of the time, masking the remaining 50% of the \"No\" answers. We can therefore **double** the proportion of \"No\" answers to estimate the **true** fraction of \"No\" answers.\n**Election Polls**\nToday, the *Gallup Poll* is one of the leading polls for election results. The many sources of biases -- who responds to polls? Do v",
        "Retrieved Video": "e here?\n175\nData is \"smushed\" - hard to interpret,\n150\neven if we jittered.\n125\nDifficult to generalize a clear\n100\nrelationship between the variables.\nAdult literacy rate\n75\n.....\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n19\n2024-09-24 11:38:04\n\ne here?\n175\nData is \"smushed\" - hard to interpret,\n150\neven if we jittered.\n125\nDifficult to generalize a clear\n100\nrelationship between the variables.\nAdult literacy rate\n75\n.....\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n19\n2024-09-24 11:38:04\n\n09-24 11:36:13\n",
        "Question": "Has there not been any polls yet or did I miss something?",
        "Timestamp": 1690,
        "TA Response": "No polls yet",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "When data is \"mushed\" together on an axis, it becomes difficult to interpret and identify any meaningful patterns or relationships between variables. This compression can obscure differences between data points, making it challenging to generalize or draw conclusions from the data. In the lecture, it was mentioned that transformations like log or square root can help spread out the data, making it easier to analyze and interpret. This is important for ensuring that visualizations accurately represent the underlying data.",
        "Recent Audio": "for example, a sqrt 1600 versus a sqrt 16, this is going to be, for example, 4, this is going to be 40, right? So both of them are going to get a smaller, but proportionally 1600 got way smaller than 16 got smaller. So that's why they're going to get closer to each other every time that we have extreme values. Any of such transformations such as log, square root, things like that would work and will help us to have a uniform kind of distribution in the X axis or in the Y axis. So one problem solved. Now our X dimension is log of the GNI right by the X axis is still the same and you can see that the X axis is still has this issue of all of these data points kind of clump together at the higher range that we. To fix OK, going into fixing that, what kind of transformation can I use so that I can push those large values more upward so that I can see the difference between them is slightly better? This is the opposite problem, right? In the first case, I had the issue that the data points were far apart. I needed to bring them closer. Here the data points are on top of each other, I just need to push them to be farther away from each other. What kind of transformation do you think would be helpful? Make a guess, let's see if there's anything on the slider. Based on.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_105\nNarges Norouzi\nO\n7\n:\nK\n*\nApplying Transformations\nabp\nT\nD\nWhat makes this plot non-linear?\n3529573\n1. A few large outlying x values are distorting the horizontal axis.\n1600 = 40\nResolve by log-transforming the x data:\nTaking the log of a large number decreases its value significantly.\nV 16 = 4\nTaking the log of a small number does not change its value as significantly.\n200\n200\n175\n175\n150\n150\n125\n125\n100\n100\nAdult literacy rate\nAdult literacy rate\n75\n75\n50\n50\n25\n25\n0\n10000 20000 30000 40000 50000 60000\n70000\n6\n8\n9\nGross national income per capita\nLog(gross national income per capita)\n22\nCC\nBY\nNC\nSA\n2024-09-24 11:48:37\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nX\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_117\nNarges Norouzi\n...\n0\nApplying Transformations\nab T\nWhat makes this plot non-linear?\n3529573\n2. Many large y values are all clumped together, compressing the vertical axis.\n200\n175\n150\n125\n100\nAdult literacy rate\n75\n50\n25\n6\n7\n8\n9\n10\n11\nLog(gross national income per capita)\n23\nCC\nBY\nNC\nS.A\n2024-09-24 11:48:57\n",
        "Retrieved Audio": "Such transformations such as log, square root, things like that would work and will help us to have a uniform kind of distribution in the X axis or in the Y axis. So one problem solved, now our X dimension is log of the GNI right by the X axis is still the same and you can see that the X axis is still has this issue of all of these data points kind of clumped together at the higher range that we still need to fix.\nSo there is one thing that the scale between X axis and Y axis are not the same. That's the separate issue, yes. So that the scale thing, we're going to come back to that in the linear regression lectures that you really need to make sure that our dimensions are always normalized for the best performance of to get the best performance of a linear regression model. Did you want to add something?\nAny of such transformations such as log, square root, things like that would work and will help us to have a uniform kind of distribution in the X axis or in the Y axis. So one problem solved. Now our X dimension is log of the GNI right by the X axis is still the same and you can see that the X axis is still has this issue of all of these data points kind of clump together at the higher range that we.",
        "Retrieved Notes": "r reveals the key visual intuition. While the two columns visually span a 2D space as a line, the data does not deviate at all from that singular line. This means that one of the weight columns is redundant! Even given the option to cover the whole 2D space, the data below does not. It might as well not have this dimension, which is why we still do not consider the data below to span more than 1 dimension.\n<center><img src = \"images/dataset3.png\" width=\"400vw\"></center>\nWhat happens when there are outliers?\nesent relationships in our data by manipulating the scale or axes. The visualization below improperly portrays two seemingly independent relationships on the same plot. The authors have clearly changed the scale of the y-axis to mislead their audience.\n<img src=\"images/wrong_scale_viz.png\" alt='wrong_scale_viz' width='350'> \nNotice how the downwards-facing line segment contains values in the millions, while the upwards-trending segment only contains values near three hundred thousand. These lines should not\nded to describe the data reasonably. The singular values that fall in the region of the plot after a large drop-off correspond to principal components that are **not** needed to describe the data since they explain a relatively low proportion of the total variance of the data. This point where adding more principal components results in diminishing returns is called the \"elbow\u00e2\u0080\u009d and is the point just before the line flattens out. Using this \"elbow method\", we can see that the elbow is at the second princi",
        "Retrieved Video": "e here?\n175\nData is \"smushed\" - hard to interpret,\n150\neven if we jittered.\n125\nDifficult to generalize a clear\n100\nrelationship between the variables.\nAdult literacy rate\n75\n.....\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n19\n2024-09-24 11:38:04\n\ne here?\n175\nData is \"smushed\" - hard to interpret,\n150\neven if we jittered.\n125\nDifficult to generalize a clear\n100\nrelationship between the variables.\nAdult literacy rate\n75\n.....\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n19\n2024-09-24 11:38:04\n\n5\nData is \"smushed\" - hard to interpret,\n150\neven if we jittered.\n125\nDifficult to generalize a clear\n100\nrelationship between the variables.\nAdult literacy rate\n75\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n19\nHY\nNC\n2024-09-24 11:39:35\n",
        "Question": "Why is it a problem if data is mushed together on an axis? Didn't understand what prof said",
        "Timestamp": 2330,
        "TA Response": "It is hard for us to make proper conclusions about data like this\u2014 if all the data looks \"mushed\" together on the y-axis, it tells us that very similar x-values (at least, similar by our current scaling) map to very different y-values. This is not very informative. Transforming our data can help us see if this is actually the case, or if there is some other association between them.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The context provided does not directly address p-hacking, but it does discuss hypothesis testing and transformations in data visualization. P-hacking involves manipulating data or statistical analyses to achieve significant p-values, often by trying multiple analyses until a desired result is found. The lecture content focuses on legitimate statistical methods like hypothesis testing with a clear null hypothesis and transformations to improve data visualization, which are standard practices in data analysis. These methods, when applied correctly and transparently, do not constitute p-hacking.",
        "Recent Audio": "to get closer to each other every time that we have extreme values. Any of such transformations such as log, square root, things like that would work and will help us to have a uniform kind of distribution in the X axis or in the Y axis. So one problem solved. Now our X dimension is log of the GNI right by the X axis is still the same and you can see that the X axis is still has this issue of all of these data points kind of clump together at the higher range that we. To fix OK, going into fixing that, what kind of transformation can I use so that I can push those large values more upward so that I can see the difference between them is slightly better? This is the opposite problem, right? In the first case, I had the issue that the data points were far apart. I needed to bring them closer. Here the data points are on top of each other, I just need to push them to be farther away from each other. What kind of transformation do you think would be helpful? Make a guess, let's see if there's anything on the slider. Based on. Any guesses? You would think that's something that would be opposite of a square root or opposite of log, which is. Inverse of log is the function power. It's the power or exponential. Here would work and the reason for power is that. For example, if you have say.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_105\nNarges Norouzi\nO\n7\n:\nK\n*\nApplying Transformations\nabp\nT\nD\nWhat makes this plot non-linear?\n3529573\n1. A few large outlying x values are distorting the horizontal axis.\n1600 = 40\nResolve by log-transforming the x data:\nTaking the log of a large number decreases its value significantly.\nV 16 = 4\nTaking the log of a small number does not change its value as significantly.\n200\n200\n175\n175\n150\n150\n125\n125\n100\n100\nAdult literacy rate\nAdult literacy rate\n75\n75\n50\n50\n25\n25\n0\n10000 20000 30000 40000 50000 60000\n70000\n6\n8\n9\nGross national income per capita\nLog(gross national income per capita)\n22\nCC\nBY\nNC\nSA\n2024-09-24 11:48:37\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nX\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_117\nNarges Norouzi\n...\n0\nApplying Transformations\nab T\nWhat makes this plot non-linear?\n3529573\n2. Many large y values are all clumped together, compressing the vertical axis.\n200\n175\n150\n125\n100\nAdult literacy rate\n75\n50\n25\n6\n7\n8\n9\n10\n11\nLog(gross national income per capita)\n23\nCC\nBY\nNC\nS.A\n2024-09-24 11:48:57\n",
        "Retrieved Audio": "What is difficult about it?\nAny guesses? You would think that something that would be opposite of a square root or opposite of log, which is.\nAny guesses? You would think that's something that would be opposite of a square root or opposite of log, which is.",
        "Retrieved Notes": "hypothesis test** where our null hypothesis states that the true parameter $\\theta_i$ is 0, and the alternative hypothesis states that the true parameter $\\theta_i$ is *not* 0. If our p-value is smaller than our cutoff value (usually p = 0.05), we reject the null hypothesis in favor of the alternative hypothesis. \n## Review: Bootstrap Resampling\nTo determine the properties (e.g., variance) of the sampling distribution of an estimator, we\u00e2\u0080\u0099d need access to the population. Ideally, we'd want to consider all\ner><img src = \"images/vis_9.png\" width=300px></img></center>\n<br />\nLet's frame this problem through the lens of the data science lifecycle.\n<center><img src = \"images/data_life_cycle.PNG\" width=300px></img></center>\n### 1. Question/Problem Formulation\n::: {.callout-note}\n#### Driving Questions\n- What do we want to know?\n- What problems are we trying to solve?\n- What are the hypotheses we want to test?\n- What are our metrics for success? \n:::\nThe old system was unfair because it was systemically inaccurate;\n(https://inferentialthinking.com/chapters/13/3/Confidence_Intervals.html?highlight=p%20value%20confidence%20interval#care-in-using-the-bootstrap-percentile-method) don\u00e2\u0080\u0099t hold\u00e2\u0080\u00a6\n    * Sample size n is large\n    * Sample is representative of population distribution (drawn i.i.d., unbiased)\n    \n    \u00e2\u0080\u00a6then the results of the bootstrap might not be valid.\n## [Bonus Content] \nNote: the content in this section is out of scope.\n<!-- ### Correlation vs. Causation\nLet us consider some questions in an arbitrary ",
        "Retrieved Video": "re easier to distinguish.\n13\nHY\n2024-09-24 11:24:23\n\nre easier to distinguish.\n13\nHY\n2024-09-24 11:24:23\n\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\nG\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_105\nNarges Norouzi\n...\nApplying Transformations\nabo\nT\nWhat makes this plot non-linear?\n3529573\n22\nCC\nSTA\n2024-09-24 11:45:59\n",
        "Question": "Why is this not p-hacking?",
        "Timestamp": 2360,
        "TA Response": "P-hacking is when someone manipulates data to fit some conclusion. This usually involves excluding specific data, or running repeated tests and only reporting some of them. This is NOT p-hacking, and is instead just a method of exploring relationships within our data. While we are technically manipulating our data, we are doing it consistently and we would hopefully report these transformations in whatever analysis we conduct.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "When the values on the y-axis are raised to the 4th power, it means that the original data has been transformed to spread out the data points, especially the larger values, to better reveal trends. This transformation can help linearize relationships that are not initially linear, allowing for more straightforward analysis using linear regression. To interpret the data, you would need to consider the inverse transformation (taking the 4th root) to relate the findings back to the original scale. This means that any conclusions drawn from the transformed data should be translated back to the original context by considering the 4th root of the y-values.",
        "Recent Audio": "do that, essentially what you're seeing is that now my data points are are spread out a little bit more and now what we're seeing is a semi-linear trend as opposed to that non-linear trend that we had seen before. So what we can do with this is now put a linear regression line. Now we can understand the relationship between GNI and adult literacy rate using a linear regression model, right? What do I mean by that? So if I have a linear regression line from data aid, you can see that I have X dimension, I have Y dimension. It just happens that my X dimension is now log of X and then I have Y dimension that my Y dimension just happens to be now Y to the power of four or that variable to the power of 4, which essentially will help me to use the linear regression line based on the template that we have that was. Y equal to m * X + b. So X right now is log of XY right now is Y to the power of four. So now I have a linear regression line that models the relationship between log of X&Y to the power of 4, which is pretty neat. We couldn't do it, or we couldn't have guessed this kind of association by just looking at the plot. That's why we needed to do transformation, and that's why it's pretty important. So what we can do afterwards is to just. Restructure this equation.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\nG : docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_129\nNarges Norouzi\n4\n...\n0\nInterpreting Transformed Data\nab\nT\nNow, we see a linear relationship between the\nThis tells us about the underlying\n3529573\ny=mx+b\nrelationship between the original x and y!\ny4 = m(log x) + b\ny = [m(log ) + b]1/4\nle9\n2.00\nTransformed data\nLinear regression\n1.75\n1.50\n1.25\ny\n4\n1.00\n0.75\n0.50\nAdult literacy rate (4th power)\n0.25\n0.00\n6\n7\n8\n9\n10\n11\nLog(gross national income per capita)\nlog x\n24\nBY\nNC\nSA\n2024-09-24 11:52:28\n\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n->\n3% docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_129\nNarges Norouzi\n-\n...\n0\nInterpreting Transformed Data\nab\nT\nD\n3529573\nNow, we see a linear relationship between the\nThis tells us about the underlying\ny=mx+b\nrelationship between the original x and y!\ny4 = m(log x) + b\ny = [m(log x) + b 1/4\nle9\n2.00\nTransformed data\nLinear regression\n200\n1.75\n1.50\n175\n1.25\n150\ny\n4\n1.00\ny\n125\n0.75\n100\nAdult literacy rate\n0.50\n75\nAdult literacy rate (4th power)\n0.25\n50\nUntransformed data\n0.00\nModeled relationship\n6\n25 -\n7\n8\n9\n10\n11\nLog(gross national income per capita)\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\nlog x\n24\nNC\n2024-09-24 11:52:47\n",
        "Retrieved Audio": "So here, what if I go crazy and I raise it to the power of 4, right? So you can see that oh, now all of those data points are going to be more spreaded out. Now I can see the trend better. So I did 2 transformations to solve 2 problems. I first did a log transformation or a transformation that would help me reduce the range of the X axis so I don't have an outlier. And then I also raised my Y axis to the power of four. I could do E to the.\nA lock transformation or a transformation that would help me reduce the range of the X axis so I don't have an outlier. And then I also raised my Y axis to the power of four. I could do E to the power of those numbers. Any exponential kind of function would have worked here. But if I do that, essentially what you're seeing is that now my data points are are spread out a little bit more and now what we're seeing is a semi linear trend as opposed to that non-linear trend that.\nY to the power of four, or that variable to the power of 4.",
        "Retrieved Notes": "th power)\")\nplt.suptitle(\"Adult literacy rate (4th power) against Log(GNI per capita)\")\nplt.subplots_adjust(top=0.9);\n```\nOur scatter plot is looking a lot better! Now, we are plotting the log of our original x values on the horizontal axis, and the 4th power of our original y values on the vertical axis. We start to see an approximate *linear* relationship between our transformed variables. \nWhat can we take away from this? We now know that the log of gross national income and adult literacy to the power o\nesent relationships in our data by manipulating the scale or axes. The visualization below improperly portrays two seemingly independent relationships on the same plot. The authors have clearly changed the scale of the y-axis to mislead their audience.\n<img src=\"images/wrong_scale_viz.png\" alt='wrong_scale_viz' width='350'> \nNotice how the downwards-facing line segment contains values in the millions, while the upwards-trending segment only contains values near three hundred thousand. These lines should not\nentration in one region of the y-axis. We need a transformation that will:\n* Increase the magnitude of large values of y so these datapoints are distributed more broadly on the vertical scale,\n* Not substantially alter the scaling of small values of y (we do not want to drastically modify the lower end of the y axis, which is already distributed evenly on the vertical scale).\nIn this case, it is helpful to apply a **power transformation** \u00e2\u0080\u0093 that is, raise our y values to a power. Let's try raising our ad",
        "Retrieved Video": "Lec 08 - DS100 Fa24 - Visualiza\nx\nlab? - JupyterLab\nx +\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_129\nNarges Norouzi\n...\nO\nInterpreting Transformed Data\nabo\nT\n3529573\nNow, we see a linear relationship between the\ny =mx+b\ny4 = m(log x) + b\nle9\n2.00\nTransformed data\nLinear regression\n1.75\n1.50\n1.25\ny\n4\n1.00\n0.75\n0.50\nAdult literacy rate (4th power)\n0.25\n0.00\n6\n7\n8\n9\n10\n11\nLog(gross national income per capita)\nlog x\n24\nNC\n2024-09-24 11:52:04\n\nLec 08 - DS100 Fa24 - Visualiza\nx\nlab? - JupyterLab\nx +\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_129\nNarges Norouzi\n...\nO\nInterpreting Transformed Data\nabo\nT\n3529573\nNow, we see a linear relationship between the\ny =mx+b\ny4 = m(log x) + b\nle9\n2.00\nTransformed data\nLinear regression\n1.75\n1.50\n1.25\ny\n4\n1.00\n0.75\n0.50\nAdult literacy rate (4th power)\n0.25\n0.00\n6\n7\n8\n9\n10\n11\nLog(gross national income per capita)\nlog x\n24\nNC\n2024-09-24 11:52:04\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nX\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_117\nNarges Norouzi\n...\n0\nApplying Transformations\nab T\nWhat makes this plot non-linear?\n3529573\n2. Many large y values are all clumped together, compressing the vertical axis.\n200\n175\n150\n125\n100\nAdult literacy rate\n75\n50\n25\n6\n7\n8\n9\n10\n11\nLog(gross national income per capita)\n23\nCC\nBY\nNC\nS.A\n2024-09-24 11:48:57\n",
        "Question": "how could I interpret the data if the values in y-axis are 4th powered?",
        "Timestamp": 2529,
        "TA Response": "Good question! To understand what that kind of relationship means, you can try to transform the data back to their original values and get a feel for the relationship. You can also model the relationship using an equation like y=4th_root(mx+b), and try to understand it that way.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The lecture mentioned that starting next week, the class will begin building linear models, which suggests that there will be additional lectures focused on linear regression. The current lecture is part of a sequence on visualization, and it seems to be laying the groundwork for understanding how to prepare data for linear modeling. Therefore, you can expect more content related to linear regression in future lectures.",
        "Recent Audio": "which is pretty neat. We couldn't do it, or we couldn't have guessed this kind of association by just looking at the plot. That's why we needed to do transformation, and that's why it's pretty important. So what we can do afterwards is to just. Restructure this equation. And just write the equation as Y equal to the transformed version of the right hand side. So I can say that OK M times log of X + b to the power of 1 / 4 is the equation that we're looking for, which I can go ahead and plot that. We would have never guessed that this is the relationship between between these two variables, but now using linear regression. We fit it and non-linear function or association between the X dimension and the Y dimension. Now you see that I'm living in the XY dimension because I can't because I have the equation for it. It's pretty neat. This is what I want you to take away from this lecture that when we teach, when we teach a linear regression, we teach you way more than linear regression. We teach you techniques that if you do, if you prepare your data well enough and you do the transformation good enough and you have all of the features and kind of linear space and also in the same scale from each other. The modeling that you will do is going to be way more powerful if you transform it back to the original kind of space.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n->\n3% docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_129\nNarges Norouzi\n-\n...\n0\nInterpreting Transformed Data\nab\nT\nD\n3529573\nNow, we see a linear relationship between the\nThis tells us about the underlying\ny=mx+b\nrelationship between the original x and y!\ny4 = m(log x) + b\ny = [m(log x) + b 1/4\nle9\n2.00\nTransformed data\nLinear regression\n200\n1.75\n1.50\n175\n1.25\n150\ny\n4\n1.00\ny\n125\n0.75\n100\nAdult literacy rate\n0.50\n75\nAdult literacy rate (4th power)\n0.25\n50\nUntransformed data\n0.00\nModeled relationship\n6\n25 -\n7\n8\n9\n10\n11\nLog(gross national income per capita)\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\nlog x\n24\nNC\n2024-09-24 11:52:47\n\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n>\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_145\nNarges Norouzi\nK\n...\nO\nK\nTukey-Mosteller Bulge Diagram\n0*\nab\nT\nO\n3529573\nThe Tukey-Mosteller Bulge Diagram is a guide to\n13\npossible transforms to try to get linearity.\n12\nA visual summary of the reasoning we just\nworked through.\nsqrt and log make a value \"smaller\".\nRaising to a value to a power makes it \"bigger\".\nThere are multiple solutions. Some will fit better\nlog X\nx2\nthan others.\nVx\n.3\nX\nYou should still understand the logic we just worked\nthrough to decide how to transform the data. The\nbulge diagram is just a summary.\nlog Y\n25\n2024-09-24 11:53:45\n",
        "Retrieved Audio": "We are going into visualization too, so this is the second lecture in this sequence, but I'm flying. What I'm planning to do is to go a little bit slower.\nBefore I start jumping into the content of today's lecture, I want to make some reminders here.\nAll right, good morning, Data 100. Welcome to our Lecture 8. Please log into a slide. Oh, there's going to be a number of fun polls today because it's all going to be about visualization theory to make sense of like how can we evaluate how good or sensitive our visual system is and then how to adapt the visualizations that we're creating based on that perception?",
        "Retrieved Notes": "t can I do?\n\nMost class materials are available online to all campus students, and you can access the lecture videos, slides, etc. without limit, as well as use the online [Data 100 DataHub (our JupyterHub instance for this class)](https://data100.datahub.berkeley.edu/).\n\nNote however that the class EdStem, our only other class resource, is **reserved for enrolled students and faculty auditors**, as homework details and solutions are discussed in that environment. Furthermore, we do not add auditors to bCou\nsection or on their own time.** The content covered in these sections is designed to solidify understanding of key lecture concepts and prepare students for homework assignments. It is to your benefit to actively participate in all discussions.\n\nEach week, we will provide a conceptual review covering the topics pertinent to the discussion worksheet at [this Youtube playlist](https://www.youtube.com/playlist?list=PLQCcNQgUcDfplNp0itu2QqVjoDE9u5iow). Slides for the content review are linked [here](https://dri\nussion\n\nLive **discussion sections** are one hour long, and held on Tuesdays or Wednesdays. The goal of these TA-led sessions is to work through problems, hone your skills, and flesh out your understanding as part of a team. The problems that you solve and present as part of discussion are important in understanding course material.\n\nThe lectures, assignments, and exams of this course are structured with the **expectation that all students complete the tasks in the discussion worksheet, either by attending ",
        "Retrieved Video": ".\nLinear relationships are simple to interpret - we know how to work with slopes and\nintercepts to understand how two variables are related.\n.\nStarting next week, we will start building linear models - these are more effective with\nlinearized data.\n20\nNC\n2024-09-24 11:41:25\n\n.\nLinear relationships are simple to interpret - we know how to work with slopes and\nintercepts to understand how two variables are related.\n.\nStarting next week, we will start building linear models - these are more effective with\nlinearized data.\n20\nNC\n2024-09-24 11:41:25\n\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nX\n+\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_73\nNarges Norouzi\n...\n0\nK\nVisualization Theory\nabp\nT\nA\nRemember our goals of visualization:\n3529573\n1. To help your own understanding of your data/results.\n2. To communicate results/conclusions to others.\nThese are influenced by our choice of visualization and our choices in how to prepare data for\nvisualization.\n19\nNC\nSA\n2024-09-24 11:36:37\n",
        "Question": "Is this all were gonna do for lin reg? or are there other lectures for it?",
        "Timestamp": 2584,
        "TA Response": "We will be doing a lot more regression later!",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "In the context of the lecture, the choice of transformation depends on the specific shape and bulge of the data. The transformations applied, such as \\(y\\) raised to the power of 4, were chosen to address the particular distribution and bulging observed in the data. The decision not to use \\(\\log(y)\\) or \\(\\sqrt{y}\\) might be because these transformations did not adequately address the specific pattern or bulge in the data for this problem. The Tukey-Mosteller Bulge Diagram helps guide these decisions by suggesting transformations that can linearize the data based on its bulge.",
        "Recent Audio": "you would do is you would take a look at where the how your data bulges. And if your data bulges in a certain way, you would take a look at the transformation on the X axis and on the Y axis. And those are the transformations that you could use. Sometimes only a single transformation might solve your problem. So you might only want to transform X, or you might only want to transform Y. In some cases, you might want to transform both in order to get the result that you want. So kind of to put it in practice, the trend that we saw in our data was that our data was bulging in the direction that we're seeing in this top left corner. And what we did was we basically. Kind of guessed what kind of transformation I need on the X axis, what kind of transformation I need on the Y axis and we went ahead and transformed the data. Again, I did both of the transformation. You might not need both. You might only need one depending on the shape of your data and depending on how deep it bulges into One Direction. Remember that I was telling you that, OK, we do a lot of X transformation, we could have also transformed using a sqrt X. So that's kind of the reason that we could have done that. Or basically we did Y to the power of 4 transformation or any power transformation would have worked in this case.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n>\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_145\nNarges Norouzi\nK\n...\nO\nK\nTukey-Mosteller Bulge Diagram\n0*\nab\nT\nO\n3529573\nThe Tukey-Mosteller Bulge Diagram is a guide to\n13\npossible transforms to try to get linearity.\n12\nA visual summary of the reasoning we just\nworked through.\nsqrt and log make a value \"smaller\".\nRaising to a value to a power makes it \"bigger\".\nThere are multiple solutions. Some will fit better\nlog X\nx2\nthan others.\nVx\n.3\nX\nYou should still understand the logic we just worked\nthrough to decide how to transform the data. The\nbulge diagram is just a summary.\nlog Y\n25\n2024-09-24 11:53:45\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_153\nNarges Norouzi\n0\nK\n:\nTukey-Mosteller Bulge Diagram\nH\nab,\nT\nIf the data bulges like this ...\nApplying to the data from before:\n3529573\n,3\n... transform y by this\nY\nCould have\ntransformed y by y2, y3\n200\n175\n150\nlog X\n125\nVx\nx2\n100\nx3\nAdult literacy rate\n75\n...\n... or transform x by this\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\nCould have transformed x\nlog Y\nby log(x), sqrt(x)\n26\nHY\nNC\nSA\n2024-09-24 11:54:47\n",
        "Retrieved Audio": "Any of such transformations such as log, square root, things like that would work and will help us to have a uniform kind of distribution in the X axis or in the Y axis. So one problem solved. Now our X dimension is log of the GNI right by the X axis is still the same and you can see that the X axis is still has this issue of all of these data points kind of clump together at the higher range that we.\nSo X right now is log of XY right now is Y to the power of four. So now I have a linear regression line that models the relationship between log of X&Y to the power of 4, which is pretty neat. We couldn't do it, or we couldn't have guessed this kind of association by just looking at the plot. That's why we needed to do transformation, and that's why it's pretty important. So what we can do afterwards is to just.\nAgain, I did both of the transformation. You might not need both. You might only need one depending on the shape of your data and depending on how deep it bulges into One Direction. Remember that I was telling you that, OK, we do a lot of X transformation, we could have also transformed using a sqrt X. So that's kind of the reason that we could have done that. Or basically we did Y to the power of 4 transformation or any power transformation would have worked in this case.",
        "Retrieved Notes": "nsformed `\"Age\"`. In other words, we are applying the transformation $z_i = \\log{(y_i)}$. Notice that the resulting model is still **linear in the parameters** $\\theta = [\\theta_0, \\theta_1]$. The SLR model becomes:\n$$\\hat{\\log{y}} = \\theta_0 + \\theta_1 x$$\n$$\\hat{z} = \\theta_0 + \\theta_1 x$$\nIt turns out that this linearized relationship can help us understand the underlying relationship between $x$ and $y$. If we rearrange the relationship above, we find:\n$$\\log{(y)} = \\theta_0 + \\theta_1 x$$\n$$y = e^{\\th\nrtion of the horizontal axis. How can we do this? We need a transformation that will:\n* Decrease the magnitude of large x values by a significant amount.\n* Not drastically change the magnitude of small x values.\nOne function that produces this result is the **log transformation**. When we take the logarithm of a large number, the original number will decrease in magnitude dramatically. Conversely, when we take the logarithm of a small number, the original number does not change its value by as significant o\nis is because $\\text{log}$ is a strictly *increasing* function. It won't change the maximum or minimum of the function it was applied to. From $\\text{log}$ properties, $\\text{log}(a*b)$ = $\\text{log}(a) + \\text{log}(b)$. We can apply this to our equation above to get:\n$$\\underset{p}{\\text{argmax}} \\sum_{i=1}^{n} \\text{log}(p^{y_i} (1-p)^{1-y_i})$$\n$$= \\underset{p}{\\text{argmax}} \\sum_{i=1}^{n} (\\text{log}(p^{y_i}) + \\text{log}((1-p)^{1-y_i}))$$\n$$= \\underset{p}{\\text{argmax}} \\sum_{i=1}^{n} (y_i\\text{log}(p",
        "Retrieved Video": " national income per capita\nCould have transformed x\nlog Y\nby log(x), sqrt(x)\n26\nHY\nNC\nSA\n2024-09-24 11:54:47\n\n national income per capita\nCould have transformed x\nlog Y\nby log(x), sqrt(x)\n26\nHY\nNC\nSA\n2024-09-24 11:54:47\n\nions. Some will fit better\nlog X\nx2\nthan others.\nVx\n.3\nX\nYou should still understand the logic we just worked\nthrough to decide how to transform the data. The\nbulge diagram is just a summary.\nlog Y\n25\n2024-09-24 11:53:45\n",
        "Question": "Why do we not use log(y) or sqrt(y) for this problem",
        "Timestamp": 2716,
        "TA Response": "I'm not too sure which example you are referring to, but we can often use multiple different functions to successfully transform our data into a linear relationship.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Deciding between using transformations like \\(Y^2\\) versus \\(Y^3\\) often involves a combination of exploratory data analysis and understanding the underlying data distribution. There isn't a strict mathematical rule for choosing one over the other; instead, you should consider the shape of your data and how it bulges, as mentioned in the lecture. The bulge diagram can guide you in selecting the appropriate transformation by showing how different transformations can linearize relationships. Ultimately, it's about experimenting with different transformations and evaluating which one best meets the assumptions of your model and improves interpretability.",
        "Recent Audio": "telling you that, OK, we do a lot of X transformation, we could have also transformed using a sqrt X. So that's kind of the reason that we could have done that. Or basically we did Y to the power of 4 transformation or any power transformation would have worked in this case. So why do the power of two, why to the power of 3, all of that would have also worked. So this is a nice kind of reference. You don't need to memorize this plot, this bulge diagram. We're going to give it to you in your reference sheet. But I'm expecting you that after I walk you through all of the steps of the reasoning, you should be able to reason it again before using this plot. OK, how are we doing so far? I think we're done with the difficult part of the lecture. Good. Sideways, not so much. Good. It, it kind of makes sense. All of these visualization topics that we're covering, it's kind of really striking a balance between evaluating whether the data is ready for modeling or we need to do more data preparation, data processing, or if you're ready to move forward to the to the. To the modelling phase. So now I want to switch gear and talk about visualization theory. This is kind of going to be a little bit of a little bit eye opening in a sense that you will see different considerations that you should have when you're creating different plots.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_785\nNarges Norouzi\nD\nK\n...\n0\nabo\nT\nK\nA\n3529573\nPlotting Distributions - Revisited\nRelationships between Quantitative\nVariables\n.\nTransformations\n.\nVisualization Theory\nInformation Channels\nVisualization\nHarnessing X/Y\n.\nHarnessing Color\nTheory\nHarnessing Markings\n.\nHarnessing Conditioning\n.\nHarnessing Context\nLecture 8, Data 100 Fall 2024\n27\nCC\nBY\nNC\nSA\n2024-09-24 11:55:59\n\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n3% docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1536\nNarges Norouzi\nK\n...\n0\nK\nVisualizations Are For Humans\n8\nabo\nT\n3529573\n500\n400\n300\n200\nfare\n\"Looks like older people didn't\n100\nspend more money on tickets\nfor the Titanic than younger\n0\npeople.\"\n0\n20\n40\n60\n80\nage\n(Note: A histogram or KDE would give stronger evidence than a scatter plot.) 28\nCC\nNC\nSA\n2024-09-24 11:56:47\n",
        "Retrieved Audio": "Again, I did both of the transformation. You might not need both. You might only need one depending on the shape of your data and depending on how deep it bulges into One Direction. Remember that I was telling you that, OK, we do a lot of X transformation, we could have also transformed using a sqrt X. So that's kind of the reason that we could have done that. Or basically we did Y to the power of 4 transformation or any power transformation would have worked in this case.\nSo X right now is log of XY right now is Y to the power of four. So now I have a linear regression line that models the relationship between log of X&Y to the power of 4, which is pretty neat. We couldn't do it, or we couldn't have guessed this kind of association by just looking at the plot. That's why we needed to do transformation, and that's why it's pretty important. So what we can do afterwards is to just.\nWe can look at where how your data bulges and if your data bulges in a certain way, you would take a look at the transformation on the X axis and on the Y axis. And those are the transformations that you could use. Sometimes only a single transformation might solve your problem. So you might only want to transform X or you might only want to transform Y. In some cases, you might want to transform both in order to get the result that you want. So kind of to put it in practice.",
        "Retrieved Notes": "{-1}\\mathbb{X}^T \\mathbb{Y}$.\nOne thing to note, however, is that the techniques we used above can only be applied if we make some big assumptions. For the calculus approach, we assumed that the loss function was differentiable at all points and that we could algebraically solve for the zero points of the derivative; for the geometric approach, OLS *only* applies when using a linear model with MSE loss. What happens when we have more complex models with different, more complex loss functions? The techniques\n 3 scores: $Y = (X_1 + X_2 + X_3)/3$.\nAs we're creating this random variable, a few questions arise:\n- What can we say about the distribution of $Y$?\n- How does it depend on the distribution of $X_1$, $X_2$, and $X_3$?\nBut, what exactly is a distribution? Let's dive into this! \n### Distributions\nTo define any random variable $X$, we need to be able to specify 2 things: \n1. **Possible values**: the set of values the random variable can take on.\n2. **Probabilities**: the set of probabilities describing how th\n10$  | \n| Variance | $\\text{Var}(Y_A) = 10^2 (0.25) + 10^2 (0.25) = 50$ | $\\text{Var}(Y_B) = 0.25 + ... + 0.25 = 5$ | $\\text{Var}(Y_C) = 20^2 (0.25) = 100$ | \n| Standard Deviation | $\\text{SD}(Y_A) \\approx 7.07$ | $\\text{SD}(Y_B) \\approx 2.24$ | $\\text{SD}(Y_C) = 10$  | \nAs we can see, all the scenarios have the same expected value but different variances. The higher the variance, the greater the risk and uncertainty, so the \"right\" strategy depends on your personal preference. Would you choose the \"safest\"",
        "Retrieved Video": "ions. Some will fit better\nlog X\nx2\nthan others.\nVx\n.3\nX\nYou should still understand the logic we just worked\nthrough to decide how to transform the data. The\nbulge diagram is just a summary.\nlog Y\n25\n2024-09-24 11:53:45\n\nions. Some will fit better\nlog X\nx2\nthan others.\nVx\n.3\nX\nYou should still understand the logic we just worked\nthrough to decide how to transform the data. The\nbulge diagram is just a summary.\nlog Y\n25\n2024-09-24 11:53:45\n\n.\nLinear relationships are simple to interpret - we know how to work with slopes and\nintercepts to understand how two variables are related.\n.\nStarting next week, we will start building linear models - these are more effective with\nlinearized data.\n20\nNC\n2024-09-24 11:41:25\n",
        "Question": "How do we decided between using Y^2 vs Y^3? Is there a mathematical answer to this or do we just guess which one works better ",
        "Timestamp": 2767,
        "TA Response": "We usually just want to use the transformation that gives us the most \"linear-looking\" plot.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The bulge diagram primarily focuses on transformations that are commonly used to linearize data, such as logarithmic and power transformations. While exponential transformations like \\(e^y\\) can be useful in certain contexts, they are not typically used for linearization purposes, which is why they might not be included in the bulge diagram. Narges mentioned exponential transformations as an option to highlight the variety of transformations available, but the focus of the bulge diagram is on those that directly aid in achieving linearity for linear regression models.",
        "Recent Audio": "We're going to give it to you in your reference sheet. But I'm expecting you that after I walk you through all of the steps of the reasoning, you should be able to reason it again before using this plot. OK, how are we doing so far? I think we're done with the difficult part of the lecture. Good. Sideways, not so much. Good. It, it kind of makes sense. All of these visualization topics that we're covering, it's kind of really striking a balance between evaluating whether the data is ready for modeling or we need to do more data preparation, data processing, or if you're ready to move forward to the to the. To the modelling phase. So now I want to switch gear and talk about visualization theory. This is kind of going to be a little bit of a little bit eye opening in a sense that you will see different considerations that you should have when you're creating different plots. There's not going to be any code involved, but it's going to be interesting for you to see how you can effectively use different information channels. Why are we talking about information channel? Because we need to make sure that the plots that you're creating, that you're creating actually for humans are meaningful. So for example, here, what I plotted is the amount of the amount that people paid for the ticket for Titanic and depending on their age and if you're taking a look at what people are going to.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n3% docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1536\nNarges Norouzi\nK\n...\n0\nK\nVisualizations Are For Humans\n8\nabo\nT\n3529573\n500\n400\n300\n200\nfare\n\"Looks like older people didn't\n100\nspend more money on tickets\nfor the Titanic than younger\n0\npeople.\"\n0\n20\n40\n60\n80\nage\n(Note: A histogram or KDE would give stronger evidence than a scatter plot.) 28\nCC\nNC\nSA\n2024-09-24 11:56:47\n\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\n\u00d7\n+\nG\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1536\nNarges Norouzi\nO\nx :\nVisualizations Are For Humans\nabo\nT\n3529573\n500\n400\n300\nfare\n7\n200\n\"Looks like older people didn't\n100\nspend more money on tickets\nfor the Titanic than younger\n0\npeople.\"\n0\n20\n40\n60\n80\nage\n(Note: A histogram or KDE would give stronger evidence than a scatter plot.) 28\nNC\nSA\n2024-09-24 11:57:16\n",
        "Retrieved Audio": "So why do the power of two, why to the power of 3, all of that would have also worked. So this is a nice kind of reference. You don't need to memorize this plot, this bulge diagram. We're going to give it to you in your reference sheet. But I'm expecting you that after I walk you through all of the steps of the reasoning, you should be able to reason it again before using this plot. OK, how are we doing so far? I think we're done with the difficult part of the lecture.\nOf your data and this is the kind of trend that we will be able to extract using a simple linear regression model. OK, you might tell me that Nargis, this was difficult to just guess what issues we have and how to fix those issues. And it was kind of a guessing game that you use Y to the power for a lot of X for fixing each one of those issues. But I think that was a good practice for you to see how I arrived at each one of those transformations that I did.\nAgain, I did both of the transformation. You might not need both. You might only need one depending on the shape of your data and depending on how deep it bulges into One Direction. Remember that I was telling you that, OK, we do a lot of X transformation, we could have also transformed using a sqrt X. So that's kind of the reason that we could have done that. Or basically we did Y to the power of 4 transformation or any power transformation would have worked in this case.",
        "Retrieved Notes": "ortant:** You should still understand the *logic* we worked through to determine how best to transform the data. The bulge diagram is just a summary of this same reasoning. You will be expected to be able to explain why a given transformation is or is not appropriate for linearization.\n### Additional Remarks\nVisualization requires a lot of thought!\n- There are many tools for visualizing distributions.\n    - Distribution of a single variable:\n        1. Rugplot\n        2. Histogram\n        3. Density plot\n  \n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of \ninear Models\nAt this point, we have an effective method of fitting models to predict linear relationships. Given a feature variable and target, we can apply our four-step process to find the optimal model parameters.\nA key word above is _linear_. When we computed parameter estimates earlier, we assumed that $x_i$ and $y_i$ shared a roughly linear relationship. Data in the real world isn't always so straightforward, but we can transform the data to try and obtain linearity.\nThe **Tukey-Mosteller Bulge Diagra",
        "Retrieved Video": "ions. Some will fit better\nlog X\nx2\nthan others.\nVx\n.3\nX\nYou should still understand the logic we just worked\nthrough to decide how to transform the data. The\nbulge diagram is just a summary.\nlog Y\n25\n2024-09-24 11:53:45\n\nions. Some will fit better\nlog X\nx2\nthan others.\nVx\n.3\nX\nYou should still understand the logic we just worked\nthrough to decide how to transform the data. The\nbulge diagram is just a summary.\nlog Y\n25\n2024-09-24 11:53:45\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nX\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_117\nNarges Norouzi\n...\n0\nApplying Transformations\nab T\nWhat makes this plot non-linear?\n3529573\n2. Many large y values are all clumped together, compressing the vertical axis.\n200\n175\n150\n125\n100\nAdult literacy rate\n75\n50\n25\n6\n7\n8\n9\n10\n11\nLog(gross national income per capita)\n23\nCC\nBY\nNC\nS.A\n2024-09-24 11:48:57\n",
        "Question": "Why does bulge diagram not have exponential eg e^y but Narges mentioned it as an option just now?",
        "Timestamp": 2803,
        "TA Response": "You may use exponential, along with many other functions that are not present on the diagram. The diagram is merely a suggestion for the types of transformations you can apply, but other ones may be appropriate as well!",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The logic behind transformation, such as taking the log or power of variables, is to linearize relationships that are not initially linear, making them more suitable for linear regression modeling. Transformations like the logarithm can reduce the magnitude of large values significantly while having a smaller effect on small values, which helps in stabilizing variance and making the data more homoscedastic. This process allows us to uncover underlying linear relationships that might not be apparent in the original scale of the data. By transforming the data, we can better fit a linear model and interpret the relationship between variables more effectively.",
        "Recent Audio": "between evaluating whether the data is ready for modeling or we need to do more data preparation, data processing, or if you're ready to move forward to the to the. To the modelling phase. So now I want to switch gear and talk about visualization theory. This is kind of going to be a little bit of a little bit eye opening in a sense that you will see different considerations that you should have when you're creating different plots. There's not going to be any code involved, but it's going to be interesting for you to see how you can effectively use different information channels. Why are we talking about information channel? Because we need to make sure that the plots that you're creating, that you're creating actually for humans are meaningful. So for example, here, what I plotted is the amount of the amount that people paid for the ticket for Titanic and depending on their age and if you're taking a look at what people are going to. In this plot, people are not going to be able to gather much information from it because majority of your data is kind of over plotted in the bottom area here. So you can make sense of OK, how much a 20 year old person is going to pay versus a six year old necessarily because the data is just spread it out. So in the 0 to $100 range. So what we need to make sure is that we use information channels more effectively.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n3% docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1536\nNarges Norouzi\nK\n...\n0\nK\nVisualizations Are For Humans\n8\nabo\nT\n3529573\n500\n400\n300\n200\nfare\n\"Looks like older people didn't\n100\nspend more money on tickets\nfor the Titanic than younger\n0\npeople.\"\n0\n20\n40\n60\n80\nage\n(Note: A histogram or KDE would give stronger evidence than a scatter plot.) 28\nCC\nNC\nSA\n2024-09-24 11:56:47\n\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\n\u00d7\n+\nG\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1536\nNarges Norouzi\nO\nx :\nVisualizations Are For Humans\nabo\nT\n3529573\n500\n400\n300\nfare\n7\n200\n\"Looks like older people didn't\n100\nspend more money on tickets\nfor the Titanic than younger\n0\npeople.\"\n0\n20\n40\n60\n80\nage\n(Note: A histogram or KDE would give stronger evidence than a scatter plot.) 28\nNC\nSA\n2024-09-24 11:57:16\n",
        "Retrieved Audio": "So X right now is log of XY right now is Y to the power of four. So now I have a linear regression line that models the relationship between log of X&Y to the power of 4, which is pretty neat. We couldn't do it, or we couldn't have guessed this kind of association by just looking at the plot. That's why we needed to do transformation, and that's why it's pretty important. So what we can do afterwards is to just.\nInverse of log is the function power. It's the power or exponential. Here would work and the reason for power is that.\nAnd things like that. But you need to really have the underlying linear association between the transformed version of the X dimension and the transformed version of the Y dimension. So now the question is, how do we actually do it? So we really want to linearize the data, but then how did I get from this plot to this other plot? And how did I actually decide that, OK, I need to do log transformation on the X axis and I need to actually.",
        "Retrieved Notes": "nsformed `\"Age\"`. In other words, we are applying the transformation $z_i = \\log{(y_i)}$. Notice that the resulting model is still **linear in the parameters** $\\theta = [\\theta_0, \\theta_1]$. The SLR model becomes:\n$$\\hat{\\log{y}} = \\theta_0 + \\theta_1 x$$\n$$\\hat{z} = \\theta_0 + \\theta_1 x$$\nIt turns out that this linearized relationship can help us understand the underlying relationship between $x$ and $y$. If we rearrange the relationship above, we find:\n$$\\log{(y)} = \\theta_0 + \\theta_1 x$$\n$$y = e^{\\th\nrtion of the horizontal axis. How can we do this? We need a transformation that will:\n* Decrease the magnitude of large x values by a significant amount.\n* Not drastically change the magnitude of small x values.\nOne function that produces this result is the **log transformation**. When we take the logarithm of a large number, the original number will decrease in magnitude dramatically. Conversely, when we take the logarithm of a small number, the original number does not change its value by as significant o\ntial* to linearize your dataset, note that this is an iterative process. It's important to try out these transformations and look at the results to see whether you've actually achieved linearity. If not, you'll need to continue testing other possible transformations.\nGenerally:\n* $\\sqrt{}$ and $\\log{}$ will reduce the magnitude of large values.\n* Powers ($^2$ and $^3$) will increase the spread in magnitude of large values.\n<center><img src=\"images/bulge.png\" alt='bulge' width='800'></img></a></center>\n**Imp",
        "Retrieved Video": "ions. Some will fit better\nlog X\nx2\nthan others.\nVx\n.3\nX\nYou should still understand the logic we just worked\nthrough to decide how to transform the data. The\nbulge diagram is just a summary.\nlog Y\n25\n2024-09-24 11:53:45\n\nions. Some will fit better\nlog X\nx2\nthan others.\nVx\n.3\nX\nYou should still understand the logic we just worked\nthrough to decide how to transform the data. The\nbulge diagram is just a summary.\nlog Y\n25\n2024-09-24 11:53:45\n\nLec 08 - DS100 Fa24 - Visualiza\nx\nlab? - JupyterLab\nx +\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_129\nNarges Norouzi\n...\nO\nInterpreting Transformed Data\nabo\nT\n3529573\nNow, we see a linear relationship between the\ny =mx+b\ny4 = m(log x) + b\nle9\n2.00\nTransformed data\nLinear regression\n1.75\n1.50\n1.25\ny\n4\n1.00\n0.75\n0.50\nAdult literacy rate (4th power)\n0.25\n0.00\n6\n7\n8\n9\n10\n11\nLog(gross national income per capita)\nlog x\n24\nNC\n2024-09-24 11:52:04\n",
        "Question": "what is the logic behind transformation? why can we directly take the log and power?",
        "Timestamp": 2816,
        "TA Response": "We transform our data in order to linearize our data\u2014 linear relationships are easy to interpret, and we'll see more of this in the regression unit we will start soon. We can directly apply transformations as long as we keep track of them, for example if we square the y-axis, our \"linear\" relationship equation becomes y^2 = mx + b",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "A 2D rug plot with circles is not exactly the same as a scatterplot, although they share similarities. In a rug plot, the primary focus is on the distribution of data along one or two axes, often using lines or markers to indicate frequency or density. When circles are used in a 2D rug plot, it can resemble a scatterplot, but the intention is typically to emphasize the distribution rather than the relationship between two variables. A scatterplot, on the other hand, is specifically designed to show the relationship between two continuous variables by plotting data points on a Cartesian plane.",
        "Recent Audio": "for me to have a 2D marker in A2 dimensional channel space. So this is what we're. Start talking about and thinking about about how many information channels are being utilized and how to effectively use all the information channels that we have. So for example, in this example, we encoded more than three variables. But what I want to ask you is that how many variables are we encoding here exactly? So the question was how many channels are being used or how many variables are being coding in this plot? Looks like 4 is a winner. Can someone explain what those four channels are? Yes. X and Y2, we have the colour tree, we have the the area of each one of those circles that we have. So we have 4 different information channels that we're utilizing. And this is how we are basically trying to strategize what information channel to use, how to use that information channel. And then what's the priority in terms of which channel is more appropriate for what kind of variable that we need to create? So the answer was four. And then the next thing that we will start talking about in the in the rest of the lecture is that if you're thinking about all of those channels and all of those encodings that we can have, how to effectively use it? I'm going to show you this plot basically showing you the worst example of when the X axis or the Y axis could be utilized.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1700\nNarges Norouzi\n...\nK\nAbusing Encodings: Length\nab\nT\nThere are many things that can go wrong in a visualization. For example, the visualization\n3529573\nbelow abuses the length channel:\nCar\nAccord\nAMC Pacer\nAudi 5000\nBMW 320i\nChamp\nChev Nova\nCivic\nDatsun 210\nDatsun 810\nFor the next huge chunk of today's\nDeville\nLe Car\nlecture, we'll dive into ways to properly\nLinc Cont\nHorizon\nMustang\nuse other aspects of a visualization:\nPeugeot\nSaab 900\nSubaru\nx/y\nVolvo 260\nColor\nVW Dasher\nUSA Japan Germany France\nSweden\nNation\nMarkings\nCar nationality for 1979\nConditioning\napt\nFig. 11. Incorrect use of a bar chart for the Nation relation. The\nContext\nlengths of the bars suggest an ordering on the vertical axis, as if the\nUSA cars were longer or better than the other cars, which is not true\nfor the Nation relation.\nLink\n?? This is a very famous paper, but I'm not sure why Mackinlay\nthinks the bar chart would suggest USA cars are longer ??\n38\n2024-09-24 12:04:33\n\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n>\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1700\nNarges Norouzi\n...\nAbusing Encodings: Length\nH\nabo\nT\nThere are many things that can go wrong in a visualization. For example, the visualization\n3529573\nbelow abuses the length channel:\nCar\nAccord\nAMC Pacer\nAudi 5000\nBMW 320i\nChamp\nChev Nova\nCivic\nDatsun 210\nDatsun 810\nFor the next huge chunk of today's\nDeville\nLe Car\nlecture, we'll dive into ways to properly\nLinc Cont\nHorizon\nMustang\nuse other aspects of a visualization:\nPeugeot\nSaab 900\nSubaru\nx/y\nVolvo 260\nVW Dasher\nColor\nUSA Japan Germany France\nSweden\nNation\nCar nationality for 1979\nMarkings\nConditioning\napt\nFig. 11. Incorrect use of a bar chart for the Nation relation. The\nContext\nlengths of the bars suggest an ordering on the vertical axis, as if the\nUSA cars were longer or better than the other cars, which is not true\nfor the Nation relation.\nLink\n?? This is a very famous paper, but I'm not sure why Mackinlay\nthinks the bar chart would suggest USA cars are longer ??\n38\n...\n38\nS\n2024-09-24 12:05:06\n",
        "Retrieved Audio": "Curve on top of a histogram with symbolic plot, violin plot. But we have also seen all of those kind of and today we've seen like obviously scatter plot hex blood and contour plot, but all of the things that we've seen we've seen it in seaborne and math floodlit. So you might also see frequently us using a plot leaf framework for labs or for lecture kind of visualization and so on and so forth but the plot lay is going to be a little bit more interactive so we can.\nThese circles, what I can create is what you're seeing on the right hand side, so the position is still the same, the encoding is the same, but the marker is different. So you can see that the choice of marker here is also important because it was I was able to tell the differences between different ages using the normal rock plot, but with the marker being different, now I can basically see anything because I'm plotting it. I'm using a 2D marker in a 1D.\nThe Y axis, X axis and also it has the IT grabs also the Y axis. So it has 2 dimensions in order to encode only one dimension information that we have. So the more appropriate place to use this specific 2D marker for us would have been a 2D plot or a plot that has A2 dimensional encoding. So A2 dimensional encoding could be that. OK, I want to do a scatter plot. I want to take a look at the association.",
        "Retrieved Notes": "r reveals the key visual intuition. While the two columns visually span a 2D space as a line, the data does not deviate at all from that singular line. This means that one of the weight columns is redundant! Even given the option to cover the whole 2D space, the data below does not. It might as well not have this dimension, which is why we still do not consider the data below to span more than 1 dimension.\n<center><img src = \"images/dataset3.png\" width=\"400vw\"></center>\nWhat happens when there are outliers?\nortant:** You should still understand the *logic* we worked through to determine how best to transform the data. The bulge diagram is just a summary of this same reasoning. You will be expected to be able to explain why a given transformation is or is not appropriate for linearization.\n### Additional Remarks\nVisualization requires a lot of thought!\n- There are many tools for visualizing distributions.\n    - Distribution of a single variable:\n        1. Rugplot\n        2. Histogram\n        3. Density plot\n  \nrough various encodings. In the remainder of this lecture, we'll look at the use of color, scale, and depth, to name a few. \n#### Encodings in Rugplots\nOne detail that we may have overlooked in our earlier discussion of rugplots is the importance of encodings. Rugplots are effective visuals because they utilize line thickness to encode frequency. Consider the following diagram:\n<img src=\"images/rugplot_encoding.png\" alt='rugplot_encoding' width='600'> \n#### Multi-Dimensional Encodings\nEncodings are also use",
        "Retrieved Video": "Lec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n3% docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1615\nNarges Norouzi\n...\n0\nI\nRug Plot: Different Marks\nabo\nT\nO\n3529573\n80\nO\nage\n0 22.0\n10px\n70\n1 38.0\n16px\n60\n2 26.0\n11px\n50\n...\n888 NaN\nNONE\n50 40\n889 26.0\n11px\n890 32.0\n30\n15px\n20\nMark\nEncoding\n10\n(Represents a\n(Maps datum to visual\ndatum)\nposition)\n0\n33\nNC\n2024-09-24 12:01:21\n\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n3% docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1615\nNarges Norouzi\n...\n0\nI\nRug Plot: Different Marks\nabo\nT\nO\n3529573\n80\nO\nage\n0 22.0\n10px\n70\n1 38.0\n16px\n60\n2 26.0\n11px\n50\n...\n888 NaN\nNONE\n50 40\n889 26.0\n11px\n890 32.0\n30\n15px\n20\nMark\nEncoding\n10\n(Represents a\n(Maps datum to visual\ndatum)\nposition)\n0\n33\nNC\n2024-09-24 12:01:21\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1615\nNarges Norouzi\n...\n0\n7\nRug Plot: Different Marks\nT\nK\n*\nab\nO\n3529573\n80\nO\nage\nA\n0 22.0\n10px\n70\n1 38.0\n16px\n60\n2 26.0\n11px\n50\n...\n...\n888 NaN\nNONE\n2 40\nage\n889 26.0\n11px\n890 32.0\n30\n15px\n20\nMark\nEncoding\n10\n(Represents a\n(Maps datum to visual\ndatum)\nposition)\n0\n33\nHY\nNC\n2024-09-24 12:01:06\n",
        "Question": "is a 2D rug plot with circles just a scatterplot",
        "Timestamp": 3265,
        "TA Response": "Not quite\u2014 we use rug plots for single variable distributions, while scatterplots are for multiple variables.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The answer \"four\" refers to the four different information channels being utilized in the visualization discussed in the lecture. These channels are the X-axis, Y-axis, color, and the area of each circle in the plot. The lecture emphasizes the importance of strategically using these channels to effectively convey information and avoid misinterpretations, such as the incorrect use of the length channel in a bar chart.",
        "Recent Audio": "coding in this plot? Looks like 4 is a winner. Can someone explain what those four channels are? Yes. X and Y2, we have the colour tree, we have the the area of each one of those circles that we have. So we have 4 different information channels that we're utilizing. And this is how we are basically trying to strategize what information channel to use, how to use that information channel. And then what's the priority in terms of which channel is more appropriate for what kind of variable that we need to create? So the answer was four. And then the next thing that we will start talking about in the in the rest of the lecture is that if you're thinking about all of those channels and all of those encodings that we can have, how to effectively use it? I'm going to show you this plot basically showing you the worst example of when the X axis or the Y axis could be utilized. So in this example, what you're seeing is that the X axis or the length information on the X axis is being abused in a sense that when we're trying to plot the association of different cars to different nationalities or to different countries. What we're seeing here is that if USA is the first, that means that there is something less about a car because the length of the bar is smaller as opposed to basically having a scattered scatter points for all of these dimensions that.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1700\nNarges Norouzi\n...\nK\nAbusing Encodings: Length\nab\nT\nThere are many things that can go wrong in a visualization. For example, the visualization\n3529573\nbelow abuses the length channel:\nCar\nAccord\nAMC Pacer\nAudi 5000\nBMW 320i\nChamp\nChev Nova\nCivic\nDatsun 210\nDatsun 810\nFor the next huge chunk of today's\nDeville\nLe Car\nlecture, we'll dive into ways to properly\nLinc Cont\nHorizon\nMustang\nuse other aspects of a visualization:\nPeugeot\nSaab 900\nSubaru\nx/y\nVolvo 260\nColor\nVW Dasher\nUSA Japan Germany France\nSweden\nNation\nMarkings\nCar nationality for 1979\nConditioning\napt\nFig. 11. Incorrect use of a bar chart for the Nation relation. The\nContext\nlengths of the bars suggest an ordering on the vertical axis, as if the\nUSA cars were longer or better than the other cars, which is not true\nfor the Nation relation.\nLink\n?? This is a very famous paper, but I'm not sure why Mackinlay\nthinks the bar chart would suggest USA cars are longer ??\n38\n2024-09-24 12:04:33\n\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\n>\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1700\nNarges Norouzi\n...\nAbusing Encodings: Length\nH\nabo\nT\nThere are many things that can go wrong in a visualization. For example, the visualization\n3529573\nbelow abuses the length channel:\nCar\nAccord\nAMC Pacer\nAudi 5000\nBMW 320i\nChamp\nChev Nova\nCivic\nDatsun 210\nDatsun 810\nFor the next huge chunk of today's\nDeville\nLe Car\nlecture, we'll dive into ways to properly\nLinc Cont\nHorizon\nMustang\nuse other aspects of a visualization:\nPeugeot\nSaab 900\nSubaru\nx/y\nVolvo 260\nVW Dasher\nColor\nUSA Japan Germany France\nSweden\nNation\nCar nationality for 1979\nMarkings\nConditioning\napt\nFig. 11. Incorrect use of a bar chart for the Nation relation. The\nContext\nlengths of the bars suggest an ordering on the vertical axis, as if the\nUSA cars were longer or better than the other cars, which is not true\nfor the Nation relation.\nLink\n?? This is a very famous paper, but I'm not sure why Mackinlay\nthinks the bar chart would suggest USA cars are longer ??\n38\n...\n38\nS\n2024-09-24 12:05:06\n",
        "Retrieved Audio": "Looks like 4 is a winner. Can someone explain what those four channels are?\nLooks like 4 is a winner. Umm, can someone explain what those four channels are?\nCalculate why dimension to the power of four so that then.",
        "Retrieved Notes": "er><img src = \"images/vis_9.png\" width=300px></img></center>\n<br />\nLet's frame this problem through the lens of the data science lifecycle.\n<center><img src = \"images/data_life_cycle.PNG\" width=300px></img></center>\n### 1. Question/Problem Formulation\n::: {.callout-note}\n#### Driving Questions\n- What do we want to know?\n- What problems are we trying to solve?\n- What are the hypotheses we want to test?\n- What are our metrics for success? \n:::\nThe old system was unfair because it was systemically inaccurate;\nbelow is a classifier for animals. It starts by asking how many legs the animal has. If the answer is no, we immediately say snake. If the answer is four, the model is not quite sure, so we must ask an additional question: does it purr?  If the answer is yes, the animal is a cat; and if the answer is no, the animal is a dog. The other possibility is that the number of legs was two, in that case we have up to two additional questions we may need to ask before eventually deciding if it is a kangaroo, human, o\nc{4}{10}$ of the flips were heads, so we might guess $\\hat \\theta = 0.4$. In the next section, we will mathematically prove why this is the best possible estimate.\n### Likelihood of Data\nLet's call the result of the coin flip a random variable $Y$. This is a Bernoulli random variable with two outcomes. $Y$ has the following distribution: \n$$P(Y = y) = \\begin{cases}\n        p, \\text{if }  y=1\\\\\n        1 - p, \\text{if }  y=0\n    \\end{cases} $$\n$p$ is unknown to us. But we can find the $p$ that makes the data",
        "Retrieved Video": "\n2024-09-24 11:51:54\n\ne here?\n175\nData is \"smushed\" - hard to interpret,\n150\neven if we jittered.\n125\nDifficult to generalize a clear\n100\nrelationship between the variables.\nAdult literacy rate\n75\n.....\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n19\n2024-09-24 11:38:04\n\ne here?\n175\nData is \"smushed\" - hard to interpret,\n150\neven if we jittered.\n125\nDifficult to generalize a clear\n100\nrelationship between the variables.\nAdult literacy rate\n75\n.....\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n19\n2024-09-24 11:38:04\n",
        "Question": "Could i get some help understanding why the answer was four?",
        "Timestamp": 3285,
        "TA Response": "X, y, color, and size/area",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, the x-axis is being abused because the length information on the x-axis is being used improperly, which can mislead the interpretation of the data. The example discussed shows that the ordering of data points on the x-axis does not convey meaningful information, leading to a misrepresentation of the association between different categories, such as cars and nationalities. This misuse of the x-axis can create a misleading visual impression, suggesting relationships or differences that do not exist.",
        "Recent Audio": "if you're thinking about all of those channels and all of those encodings that we can have, how to effectively use it? I'm going to show you this plot basically showing you the worst example of when the X axis or the Y axis could be utilized. So in this example, what you're seeing is that the X axis or the length information on the X axis is being abused in a sense that when we're trying to plot the association of different cars to different nationalities or to different countries. What we're seeing here is that if USA is the first, that means that there is something less about a car because the length of the bar is smaller as opposed to basically having a scattered scatter points for all of these dimensions that. Or any other kind of plots that we could use because the lengths of these bars here does not have does not include any information. And the ordering of the all of the data points that we have here and also the data points that we have here does not include any information. So that's why this is an example that the lengths encoding is being abused in this example on the X axis. So we're going to take a 2 minute break and I'm going to come back and talk about other information channels on different encodings and the best practices of using. Each one of them and we're going to see some fun examples. 2 minutes break, get started.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\n->\n2. docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g3048da694a6_0_2\nNarges Norouzi\nK\n...\nK\nabo\nT\n\"Bar graph of how much door\nI've painted.\"\nSky\nSunny side of pyramid\nShady side of pyramid\nOUR RELATIONSHIP ENTERED\nITS DECLINE AT THIS POINT.\nTHAT'S WHEN YOU\nSTARTED GRAPHING\nEVERYTHING.\nCOINCIDENCE!\nInterlude\nBOX & WHISKER PLOT\nD\nBOX & BEARD PLOT\n>x\nG-RAPH\nNC\n2024-09-24 12:05:45\n\nLec 08 - DS100 Fa24 - Visualiza >\nlab? - JupyterLab\nx\n+\n->\ndata 100.datahub.berkeley.edu/user/norouzi/lab?\nNarges Norouzi\nFile\nEdit\nView\nRun Kernel Tabs Settings Help\nlec08.ipynb\nX +\n...\na + x 0 0 * . * * Download\nCode\nv 1 git\nOpen i\nO\nV\nO\nT\n\u00b7 Transforming data makes visualizations easier to interpret\nA\n\u00b7 Linear relationships are straightforward to understand - we have ideas of what slopes and intercepts mean\nLater on in the course, the ability to linearize data will help us make more effective models\n[ ]:\n# Some data cleaning to help with the next example\ndf = pd.DataFrame(index=wb.index)\ndf['lit'] = wb['Adult literacy rate: Female: % ages 15 and older: 2005-14'] \\\n+ wb[\"Adult literacy rate: Male: % ages 15 and older: 2005-14\"]\ndf['inc'] = wb['gni' ]\ndf.dropna(inplace=True)\nplt.scatter(df[\"inc\"], df[\"lit\"])\nplt.xlabel(\"Gross national income per capita\")\nplt.ylabel(\"Adult literacy rate\");\nWhat is making this plot non-linear?\n. There are a few extremely large values for gross national income that are distorting the horizontal scale of the plot. If we rescaled the x-values\nsuch that these large values become proportionally smaller, the plot would be more linear\n. There are too many large values of adult literacy rate all clumped together at the top of the plot. If we rescaled the y-axis such that large values of\ny are more spread out, the plot would be more linear\nFirst, we can transform the x-values such that very large values of x become smaller. This can be achieved by performing a log transformation of the\ngross national income data. When we take the logarithm of a large number, this number becomes proportionally much smaller relative to its original\nvalue. When we take the log of a small number, the number does not change very significantly relative to its starting value.\n[ ]:\n# np. Log compute the natural (base e) Logarithm\nplt.scatter(np. log(df[\"inc\"]), df[ \"lit\"])\nSimple\n0\n1 @\nPython 3 (ipykernel) | Connecting Mem: 178.11 / 16384.00 MB\nMode: Command\nLn 6, Col 22\nlec08.12024-09-24112:05:59\n",
        "Retrieved Audio": "So in this example, what you're seeing is that the X axis or the length information on the X axis is being abused in a sense that when we're trying to plot the association of different cars to different nationalities or to different countries. What we're seeing here is that if USA is the first, that means that there is something less about a car because the length of the bar is smaller as opposed to basically having a scattered scatter points for all of these dimensions that.\nSo in this example, what you're seeing is that the X axis or the length information on the X axis is being abused in a sense that when we're trying to plot the association of different cars to different nationalities or to different countries. What we're seeing here is that if USA is the first, that means that there is something less about a car because the length of the bar is smaller as opposed to basically having a scatter, a scatter points for all of these dimensions that.\nMention or in any area in the plot. So we need to fix both of these issues, but I'm going to basically try to spell out what's the process of fixing each one of those issues. How can I fix this issue of having all of these data points be kind of outlier in the distribution in the right hand side? How can I make sure that my X dimension is kind of scaled in a way that these data points are going to come closer to the rest of the?",
        "Retrieved Notes": "esent relationships in our data by manipulating the scale or axes. The visualization below improperly portrays two seemingly independent relationships on the same plot. The authors have clearly changed the scale of the y-axis to mislead their audience.\n<img src=\"images/wrong_scale_viz.png\" alt='wrong_scale_viz' width='350'> \nNotice how the downwards-facing line segment contains values in the millions, while the upwards-trending segment only contains values near three hundred thousand. These lines should not\nsents a possible shape of non-linear data. To use the diagram, find which of the four bulges resembles your dataset the most closely. Then, look at the axes of the quadrant for this bulge. The horizontal axis will list possible transformations that could be applied to your x data for linearization. Similarly, the vertical axis will list possible transformations that could be applied to your y data. Note that each axis lists two possible transformations. While *either* of these transformations has the *poten\ner><img src = \"images/vis_9.png\" width=300px></img></center>\n<br />\nLet's frame this problem through the lens of the data science lifecycle.\n<center><img src = \"images/data_life_cycle.PNG\" width=300px></img></center>\n### 1. Question/Problem Formulation\n::: {.callout-note}\n#### Driving Questions\n- What do we want to know?\n- What problems are we trying to solve?\n- What are the hypotheses we want to test?\n- What are our metrics for success? \n:::\nThe old system was unfair because it was systemically inaccurate;",
        "Retrieved Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_93\nNarges Norouzi\n...\nO\nApplying Transformations\nab\nT\nWhat makes this plot non-linear?\n3529573\n200\n175\n1. A few large outlying x values are distorting\n150\nthe horizontal axis.\n125\n100\nAdult literacy rate\n75\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n21\nNC\n2024-09-24 11:42:52\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_93\nNarges Norouzi\n...\nO\nApplying Transformations\nab\nT\nWhat makes this plot non-linear?\n3529573\n200\n175\n1. A few large outlying x values are distorting\n150\nthe horizontal axis.\n125\n100\nAdult literacy rate\n75\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n21\nNC\n2024-09-24 11:42:52\n\nLec 08 - DS100 Fa24 - Visualiza\nX\nlab? - JupyterLab\nx +\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_93\nNarges Norouzi\n:\nO\nApplying Transformations\nabo\nT\nWhat makes this plot non-linear?\n3529573\n200\n175\n1. A few large outlying x values are distorting\n150\nthe horizontal axis.\n125\n100\nAdult literacy rate\n75\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n21\nO\nHY\nNC\n2024-09-24 11:43:16\n",
        "Question": "Is the x axis being abused because it is in the wrong order?",
        "Timestamp": 3343,
        "TA Response": "no it's cause they put countries as the x axis in a bar plot, which makes the length of each bar misleading",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, there was a poll related to visualization theory and evaluating how many channels or variables are being encoded in a plot. Unfortunately, I don't have the capability to reopen polls, but you might want to check with your instructor or classmates to see if they can provide the poll details or if it can be reopened for you.",
        "Recent Audio": "if you're thinking about all of those channels and all of those encodings that we can have, how to effectively use it? I'm going to show you this plot basically showing you the worst example of when the X axis or the Y axis could be utilized. So in this example, what you're seeing is that the X axis or the length information on the X axis is being abused in a sense that when we're trying to plot the association of different cars to different nationalities or to different countries. What we're seeing here is that if USA is the first, that means that there is something less about a car because the length of the bar is smaller as opposed to basically having a scattered scatter points for all of these dimensions that. Or any other kind of plots that we could use because the lengths of these bars here does not have does not include any information. And the ordering of the all of the data points that we have here and also the data points that we have here does not include any information. So that's why this is an example that the lengths encoding is being abused in this example on the X axis. So we're going to take a 2 minute break and I'm going to come back and talk about other information channels on different encodings and the best practices of using. Each one of them and we're going to see some fun examples. 2 minutes break, get started.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\n->\n2. docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g3048da694a6_0_2\nNarges Norouzi\nK\n...\nK\nabo\nT\n\"Bar graph of how much door\nI've painted.\"\nSky\nSunny side of pyramid\nShady side of pyramid\nOUR RELATIONSHIP ENTERED\nITS DECLINE AT THIS POINT.\nTHAT'S WHEN YOU\nSTARTED GRAPHING\nEVERYTHING.\nCOINCIDENCE!\nInterlude\nBOX & WHISKER PLOT\nD\nBOX & BEARD PLOT\n>x\nG-RAPH\nNC\n2024-09-24 12:05:45\n\nLec 08 - DS100 Fa24 - Visualiza >\nlab? - JupyterLab\nx\n+\n->\ndata 100.datahub.berkeley.edu/user/norouzi/lab?\nNarges Norouzi\nFile\nEdit\nView\nRun Kernel Tabs Settings Help\nlec08.ipynb\nX +\n...\na + x 0 0 * . * * Download\nCode\nv 1 git\nOpen i\nO\nV\nO\nT\n\u00b7 Transforming data makes visualizations easier to interpret\nA\n\u00b7 Linear relationships are straightforward to understand - we have ideas of what slopes and intercepts mean\nLater on in the course, the ability to linearize data will help us make more effective models\n[ ]:\n# Some data cleaning to help with the next example\ndf = pd.DataFrame(index=wb.index)\ndf['lit'] = wb['Adult literacy rate: Female: % ages 15 and older: 2005-14'] \\\n+ wb[\"Adult literacy rate: Male: % ages 15 and older: 2005-14\"]\ndf['inc'] = wb['gni' ]\ndf.dropna(inplace=True)\nplt.scatter(df[\"inc\"], df[\"lit\"])\nplt.xlabel(\"Gross national income per capita\")\nplt.ylabel(\"Adult literacy rate\");\nWhat is making this plot non-linear?\n. There are a few extremely large values for gross national income that are distorting the horizontal scale of the plot. If we rescaled the x-values\nsuch that these large values become proportionally smaller, the plot would be more linear\n. There are too many large values of adult literacy rate all clumped together at the top of the plot. If we rescaled the y-axis such that large values of\ny are more spread out, the plot would be more linear\nFirst, we can transform the x-values such that very large values of x become smaller. This can be achieved by performing a log transformation of the\ngross national income data. When we take the logarithm of a large number, this number becomes proportionally much smaller relative to its original\nvalue. When we take the log of a small number, the number does not change very significantly relative to its starting value.\n[ ]:\n# np. Log compute the natural (base e) Logarithm\nplt.scatter(np. log(df[\"inc\"]), df[ \"lit\"])\nSimple\n0\n1 @\nPython 3 (ipykernel) | Connecting Mem: 178.11 / 16384.00 MB\nMode: Command\nLn 6, Col 22\nlec08.12024-09-24112:05:59\n",
        "Retrieved Audio": "All right, good morning, Data 100. Welcome to our Lecture 8. Please log into a slide. Oh, there's going to be a number of fun polls today because it's all going to be about visualization theory to make sense of like how can we evaluate how good or sensitive our visual system is and then how to adapt the visualizations that we're creating based on that perception?\nMake a guess, let's see if there's anything on the slider.\nSo the question was how many channels are being used or how many variables are being coding in this plot?",
        "Retrieved Notes": "y had a dismal response rate (about 24\\%); who knows how the other non-respondents would have polled? The *Digest* folded just 18 months after this disaster.\nAt the same time, George Gallup, a rising statistician, also made predictions about the 1936 elections. Despite having a smaller sample size of \u00e2\u0080\u009conly\u00e2\u0080\u009d 50,000 (this is still more than necessary; more when we cover the Central Limit Theorem), his estimate that 56\\% of voters would choose Roosevelt was much closer to the actual result (61\\%). Gallup\n**\"No\"**, then the surveyor knows the **individual didn't cheat**. We assume the individual is comfortable revealing this information. \nGenerally, we can assume that the coin lands heads 50% of the time, masking the remaining 50% of the \"No\" answers. We can therefore **double** the proportion of \"No\" answers to estimate the **true** fraction of \"No\" answers.\n**Election Polls**\nToday, the *Gallup Poll* is one of the leading polls for election results. The many sources of biases -- who responds to polls? Do v\noters tell the truth? How can we predict turnout? -- still remain, but the *Gallup Poll* uses several tactics to mitigate them. Within their sampling frame of \"civilian, non-institutionalized population\u00e2\u0080\u009d of adults in telephone households in continental U.S., they use random digit dialing to include both listed/unlisted phone numbers and to avoid selection bias. Additionally, they use a within-household selection process to randomly select households with one or more adults. If no one answers, re-call mul",
        "Retrieved Video": "are infinite possibilities!\n37\nCC\nC\nHY\nNC\nSA\n2024-09-24 12:04:28\n\n9-24 11:58:37\n\n\n2024-09-24 11:51:54\n",
        "Question": "did we have a poll yet? i think my zoom went out and i wasnt able to answer it, could we reopen it?",
        "Timestamp": 3369,
        "TA Response": "No polls yet",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The professor emphasizes that the encoding of data using the length of bars can be misleading if not used properly. In the example discussed, the length of the bars does not effectively convey meaningful information about the data, such as the association of different cars to different nationalities or countries. The bars for Saab 900 and Volvo 260 do not encode useful information because the lengths do not correspond to any meaningful metric or ordering, making the visualization ineffective. This highlights the importance of choosing appropriate encodings that accurately represent the data.",
        "Recent Audio": "about all of those channels and all of those encodings that we can have, how to effectively use it? I'm going to show you this plot basically showing you the worst example of when the X axis or the Y axis could be utilized. So in this example, what you're seeing is that the X axis or the length information on the X axis is being abused in a sense that when we're trying to plot the association of different cars to different nationalities or to different countries. What we're seeing here is that if USA is the first, that means that there is something less about a car because the length of the bar is smaller as opposed to basically having a scattered scatter points for all of these dimensions that. Or any other kind of plots that we could use because the lengths of these bars here does not have does not include any information. And the ordering of the all of the data points that we have here and also the data points that we have here does not include any information. So that's why this is an example that the lengths encoding is being abused in this example on the X axis. So we're going to take a 2 minute break and I'm going to come back and talk about other information channels on different encodings and the best practices of using. Each one of them and we're going to see some fun examples. 2 minutes break, get started. All right, restarting.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\n->\n2. docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g3048da694a6_0_2\nNarges Norouzi\nK\n...\nK\nabo\nT\n\"Bar graph of how much door\nI've painted.\"\nSky\nSunny side of pyramid\nShady side of pyramid\nOUR RELATIONSHIP ENTERED\nITS DECLINE AT THIS POINT.\nTHAT'S WHEN YOU\nSTARTED GRAPHING\nEVERYTHING.\nCOINCIDENCE!\nInterlude\nBOX & WHISKER PLOT\nD\nBOX & BEARD PLOT\n>x\nG-RAPH\nNC\n2024-09-24 12:05:45\n\nLec 08 - DS100 Fa24 - Visualiza >\nlab? - JupyterLab\nx\n+\n->\ndata 100.datahub.berkeley.edu/user/norouzi/lab?\nNarges Norouzi\nFile\nEdit\nView\nRun Kernel Tabs Settings Help\nlec08.ipynb\nX +\n...\na + x 0 0 * . * * Download\nCode\nv 1 git\nOpen i\nO\nV\nO\nT\n\u00b7 Transforming data makes visualizations easier to interpret\nA\n\u00b7 Linear relationships are straightforward to understand - we have ideas of what slopes and intercepts mean\nLater on in the course, the ability to linearize data will help us make more effective models\n[ ]:\n# Some data cleaning to help with the next example\ndf = pd.DataFrame(index=wb.index)\ndf['lit'] = wb['Adult literacy rate: Female: % ages 15 and older: 2005-14'] \\\n+ wb[\"Adult literacy rate: Male: % ages 15 and older: 2005-14\"]\ndf['inc'] = wb['gni' ]\ndf.dropna(inplace=True)\nplt.scatter(df[\"inc\"], df[\"lit\"])\nplt.xlabel(\"Gross national income per capita\")\nplt.ylabel(\"Adult literacy rate\");\nWhat is making this plot non-linear?\n. There are a few extremely large values for gross national income that are distorting the horizontal scale of the plot. If we rescaled the x-values\nsuch that these large values become proportionally smaller, the plot would be more linear\n. There are too many large values of adult literacy rate all clumped together at the top of the plot. If we rescaled the y-axis such that large values of\ny are more spread out, the plot would be more linear\nFirst, we can transform the x-values such that very large values of x become smaller. This can be achieved by performing a log transformation of the\ngross national income data. When we take the logarithm of a large number, this number becomes proportionally much smaller relative to its original\nvalue. When we take the log of a small number, the number does not change very significantly relative to its starting value.\n[ ]:\n# np. Log compute the natural (base e) Logarithm\nplt.scatter(np. log(df[\"inc\"]), df[ \"lit\"])\nSimple\n0\n1 @\nPython 3 (ipykernel) | Connecting Mem: 178.11 / 16384.00 MB\nMode: Command\nLn 6, Col 22\nlec08.12024-09-24112:05:59\n",
        "Retrieved Audio": "That we have. So for example in this example we encoded more than three variables. But what I want to ask you is that how many variables are we encoding here exactly?\nStart talking about and thinking about about how many information channels are being utilized and how to effectively use all the information channels that we have. So for example, in this example, we encoded more than three variables. But what I want to ask you is that how many variables are we encoding here exactly?\nSo the question was how many channels are being used or how many variables are being coding in this plot?",
        "Retrieved Notes": "e colors poorly encode numerical data. To understand why, let's analyze the following images.\n:::: {.columns}\n::: {.column width=\"30%\"}\n<img src=\"images/jet_four_by_four.png\" alt='four_by_four_colormap' width='300'>\n:::\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n::: {.column width=\"30%\"}\n<img src=\"images/jet_3_images.png\" alt='jet_3_colormap' width='250'>\n:::\n::::\nThe diagram on the left compares how a variety of colormaps represent pixel data that transitions from a high to low inten\ner><img src = \"images/vis_9.png\" width=300px></img></center>\n<br />\nLet's frame this problem through the lens of the data science lifecycle.\n<center><img src = \"images/data_life_cycle.PNG\" width=300px></img></center>\n### 1. Question/Problem Formulation\n::: {.callout-note}\n#### Driving Questions\n- What do we want to know?\n- What problems are we trying to solve?\n- What are the hypotheses we want to test?\n- What are our metrics for success? \n:::\nThe old system was unfair because it was systemically inaccurate;\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of ",
        "Retrieved Video": "'m not sure why Mackinlay\nthinks the bar chart would suggest USA cars are longer ??\n38\n...\n38\nS\n2024-09-24 12:05:06\n\n'm not sure why Mackinlay\nthinks the bar chart would suggest USA cars are longer ??\n38\n...\n38\nS\n2024-09-24 12:05:06\n\nnot sure why Mackinlay\nthinks the bar chart would suggest USA cars are longer ??\n38\n2024-09-24 12:04:33\n",
        "Question": "I did not understand well about this slide: abusing encoding: length. What does the professor emphasize. Why the bar for Saab 900 and Volvo 260 do not encode?",
        "Timestamp": 3411,
        "TA Response": "The plot put countries as the x axis in a bar plot, which makes the length of each bar misleading. The correct data is technically displayed in the plot, and we can find where different car models are from. But the plot is poorly encoded due to the lengths not corresponding to some numerical amount.",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The four channels used to represent multi-dimensional data in the discussed visualization are the X-axis, Y-axis, area (size of the markers), and color. These channels allow for encoding multiple dimensions of information within a two-dimensional plot.",
        "Recent Audio": "about all of those channels and all of those encodings that we can have, how to effectively use it? I'm going to show you this plot basically showing you the worst example of when the X axis or the Y axis could be utilized. So in this example, what you're seeing is that the X axis or the length information on the X axis is being abused in a sense that when we're trying to plot the association of different cars to different nationalities or to different countries. What we're seeing here is that if USA is the first, that means that there is something less about a car because the length of the bar is smaller as opposed to basically having a scattered scatter points for all of these dimensions that. Or any other kind of plots that we could use because the lengths of these bars here does not have does not include any information. And the ordering of the all of the data points that we have here and also the data points that we have here does not include any information. So that's why this is an example that the lengths encoding is being abused in this example on the X axis. So we're going to take a 2 minute break and I'm going to come back and talk about other information channels on different encodings and the best practices of using. Each one of them and we're going to see some fun examples. 2 minutes break, get started. All right, restarting.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\n->\n2. docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g3048da694a6_0_2\nNarges Norouzi\nK\n...\nK\nabo\nT\n\"Bar graph of how much door\nI've painted.\"\nSky\nSunny side of pyramid\nShady side of pyramid\nOUR RELATIONSHIP ENTERED\nITS DECLINE AT THIS POINT.\nTHAT'S WHEN YOU\nSTARTED GRAPHING\nEVERYTHING.\nCOINCIDENCE!\nInterlude\nBOX & WHISKER PLOT\nD\nBOX & BEARD PLOT\n>x\nG-RAPH\nNC\n2024-09-24 12:05:45\n\nLec 08 - DS100 Fa24 - Visualiza >\nlab? - JupyterLab\nx\n+\n->\ndata 100.datahub.berkeley.edu/user/norouzi/lab?\nNarges Norouzi\nFile\nEdit\nView\nRun Kernel Tabs Settings Help\nlec08.ipynb\nX +\n...\na + x 0 0 * . * * Download\nCode\nv 1 git\nOpen i\nO\nV\nO\nT\n\u00b7 Transforming data makes visualizations easier to interpret\nA\n\u00b7 Linear relationships are straightforward to understand - we have ideas of what slopes and intercepts mean\nLater on in the course, the ability to linearize data will help us make more effective models\n[ ]:\n# Some data cleaning to help with the next example\ndf = pd.DataFrame(index=wb.index)\ndf['lit'] = wb['Adult literacy rate: Female: % ages 15 and older: 2005-14'] \\\n+ wb[\"Adult literacy rate: Male: % ages 15 and older: 2005-14\"]\ndf['inc'] = wb['gni' ]\ndf.dropna(inplace=True)\nplt.scatter(df[\"inc\"], df[\"lit\"])\nplt.xlabel(\"Gross national income per capita\")\nplt.ylabel(\"Adult literacy rate\");\nWhat is making this plot non-linear?\n. There are a few extremely large values for gross national income that are distorting the horizontal scale of the plot. If we rescaled the x-values\nsuch that these large values become proportionally smaller, the plot would be more linear\n. There are too many large values of adult literacy rate all clumped together at the top of the plot. If we rescaled the y-axis such that large values of\ny are more spread out, the plot would be more linear\nFirst, we can transform the x-values such that very large values of x become smaller. This can be achieved by performing a log transformation of the\ngross national income data. When we take the logarithm of a large number, this number becomes proportionally much smaller relative to its original\nvalue. When we take the log of a small number, the number does not change very significantly relative to its starting value.\n[ ]:\n# np. Log compute the natural (base e) Logarithm\nplt.scatter(np. log(df[\"inc\"]), df[ \"lit\"])\nSimple\n0\n1 @\nPython 3 (ipykernel) | Connecting Mem: 178.11 / 16384.00 MB\nMode: Command\nLn 6, Col 22\nlec08.12024-09-24112:05:59\n",
        "Retrieved Audio": "Looks like 4 is a winner. Umm, can someone explain what those four channels are?\nLooks like 4 is a winner. Can someone explain what those four channels are?\nX and Y2, we have the colour tree, we have the the area of each one of those circles that we have. So we have 4 different information channels that we're utilizing. And this is how we are basically trying to strategize what information channel to use, how to use that information channel.",
        "Retrieved Notes": "ful for representing multi-dimensional data. Notice how the following visual highlights four distinct \"dimensions\" of data:\n- X-axis\n- Y-axis\n- Area\n- Color\n<img src=\"images/mutli_dim_encodings.png\" alt='multi_dim_encoding' width='400'> \nThe human visual perception system is only capable of visualizing data in a three-dimensional plane, but as you've seen, we can encode many more channels of information.\n### Harnessing the Axes\n#### Consider the Scale of the Data\nHowever, we should be careful to not misrepr\nely non-linear - this is precisely why it's considered a worse colormap.\n### Harnessing Markings\nIn our earlier discussion of multi-dimensional encodings, we analyzed a scatter plot with four pseudo-dimensions: the two axes, area, and color. Were these appropriate to use? The following diagram analyzes how well the human eye can distinguish between these \"markings\".\n<img src=\"images/markings_viz.png\" alt='markings_viz' width='400'>\nThere are a few key takeaways from this diagram\n- Lengths are easy to discer\nsents a possible shape of non-linear data. To use the diagram, find which of the four bulges resembles your dataset the most closely. Then, look at the axes of the quadrant for this bulge. The horizontal axis will list possible transformations that could be applied to your x data for linearization. Similarly, the vertical axis will list possible transformations that could be applied to your y data. Note that each axis lists two possible transformations. While *either* of these transformations has the *poten",
        "Retrieved Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nX\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1677\nNarges Norouzi\n...\n0\nGoing Beyond: Encoding 3+ Variables\nabo\nT\nHow many variables are we encoding here?\n3529573\n.\nIn other words, how many \"channels\" of information are there?\n1.0 -\n0.8\n0.6\n0.4-\n0.2\n0.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n35\nNC\nSA\n2024-09-24 12:02:37\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nX\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1677\nNarges Norouzi\n...\n0\nGoing Beyond: Encoding 3+ Variables\nabo\nT\nHow many variables are we encoding here?\n3529573\n.\nIn other words, how many \"channels\" of information are there?\n1.0 -\n0.8\n0.6\n0.4-\n0.2\n0.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n35\nNC\nSA\n2024-09-24 12:02:37\n\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nlab? - JupyterLab\nx +\n->\nC\ndocs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.SLIDES_API633393785_0\nNarges Norouzi\n..\n7\n:= Active poll\nSynchronous\nab\nT\n248 8\nHow many variables are we\nencoding here?\n4 0\nA\n1.0 -\n40%\n0.8\n3\n0.6\n18%\n0.4\nJoin at\n6+\nslido.com\n0.2\n17%\n#3529 573\n0.0\n5\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n15%\n2\n10%\n2024-09-24 12:03:25\n",
        "Question": "What was the four channels again? X,Y, shape and colors?",
        "Timestamp": 3412,
        "TA Response": "X,y,area,color",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Using the sine function to transform data for linearization is generally not practical because the sine function is periodic and oscillates between -1 and 1. This means it can distort the data significantly and does not help in spreading out values or addressing issues like outliers. Transformations like logarithmic or square root are more commonly used because they can effectively handle skewness and outliers by compressing large values and expanding smaller ones, making the data more linear.",
        "Recent Audio": "sense that when we're trying to plot the association of different cars to different nationalities or to different countries. What we're seeing here is that if USA is the first, that means that there is something less about a car because the length of the bar is smaller as opposed to basically having a scattered scatter points for all of these dimensions that. Or any other kind of plots that we could use because the lengths of these bars here does not have does not include any information. And the ordering of the all of the data points that we have here and also the data points that we have here does not include any information. So that's why this is an example that the lengths encoding is being abused in this example on the X axis. So we're going to take a 2 minute break and I'm going to come back and talk about other information channels on different encodings and the best practices of using. Each one of them and we're going to see some fun examples. 2 minutes break, get started. All right, restarting. In 54321 there's a question and a slide about why the length was abused. Here it's basically because the length encodes the type and the length does not measure any anything quantitative between different categories or different countries. So that's why you might think that a car from Sweden is much better than a car in USA if you're thinking about lengths or these bars measuring any quantitative value.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\n->\n2. docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g3048da694a6_0_2\nNarges Norouzi\nK\n...\nK\nabo\nT\n\"Bar graph of how much door\nI've painted.\"\nSky\nSunny side of pyramid\nShady side of pyramid\nOUR RELATIONSHIP ENTERED\nITS DECLINE AT THIS POINT.\nTHAT'S WHEN YOU\nSTARTED GRAPHING\nEVERYTHING.\nCOINCIDENCE!\nInterlude\nBOX & WHISKER PLOT\nD\nBOX & BEARD PLOT\n>x\nG-RAPH\nNC\n2024-09-24 12:05:45\n\nLec 08 - DS100 Fa24 - Visualiza >\nlab? - JupyterLab\nx\n+\n->\ndata 100.datahub.berkeley.edu/user/norouzi/lab?\nNarges Norouzi\nFile\nEdit\nView\nRun Kernel Tabs Settings Help\nlec08.ipynb\nX +\n...\na + x 0 0 * . * * Download\nCode\nv 1 git\nOpen i\nO\nV\nO\nT\n\u00b7 Transforming data makes visualizations easier to interpret\nA\n\u00b7 Linear relationships are straightforward to understand - we have ideas of what slopes and intercepts mean\nLater on in the course, the ability to linearize data will help us make more effective models\n[ ]:\n# Some data cleaning to help with the next example\ndf = pd.DataFrame(index=wb.index)\ndf['lit'] = wb['Adult literacy rate: Female: % ages 15 and older: 2005-14'] \\\n+ wb[\"Adult literacy rate: Male: % ages 15 and older: 2005-14\"]\ndf['inc'] = wb['gni' ]\ndf.dropna(inplace=True)\nplt.scatter(df[\"inc\"], df[\"lit\"])\nplt.xlabel(\"Gross national income per capita\")\nplt.ylabel(\"Adult literacy rate\");\nWhat is making this plot non-linear?\n. There are a few extremely large values for gross national income that are distorting the horizontal scale of the plot. If we rescaled the x-values\nsuch that these large values become proportionally smaller, the plot would be more linear\n. There are too many large values of adult literacy rate all clumped together at the top of the plot. If we rescaled the y-axis such that large values of\ny are more spread out, the plot would be more linear\nFirst, we can transform the x-values such that very large values of x become smaller. This can be achieved by performing a log transformation of the\ngross national income data. When we take the logarithm of a large number, this number becomes proportionally much smaller relative to its original\nvalue. When we take the log of a small number, the number does not change very significantly relative to its starting value.\n[ ]:\n# np. Log compute the natural (base e) Logarithm\nplt.scatter(np. log(df[\"inc\"]), df[ \"lit\"])\nSimple\n0\n1 @\nPython 3 (ipykernel) | Connecting Mem: 178.11 / 16384.00 MB\nMode: Command\nLn 6, Col 22\nlec08.12024-09-24112:05:59\n",
        "Retrieved Audio": "Kind of guessed what kind of transformation I need on the X axis, what kind of transformation I need on the Y axis and we went ahead and transformed the data.\nAgain, I did both of the transformation. You might not need both. You might only need one depending on the shape of your data and depending on how deep it bulges into One Direction. Remember that I was telling you that, OK, we do a lot of X transformation, we could have also transformed using a sqrt X. So that's kind of the reason that we could have done that. Or basically we did Y to the power of 4 transformation or any power transformation would have worked in this case.\nAny of such transformations such as log, square root, things like that would work and will help us to have a uniform kind of distribution in the X axis or in the Y axis. So one problem solved. Now our X dimension is log of the GNI right by the X axis is still the same and you can see that the X axis is still has this issue of all of these data points kind of clump together at the higher range that we.",
        "Retrieved Notes": "sents a possible shape of non-linear data. To use the diagram, find which of the four bulges resembles your dataset the most closely. Then, look at the axes of the quadrant for this bulge. The horizontal axis will list possible transformations that could be applied to your x data for linearization. Similarly, the vertical axis will list possible transformations that could be applied to your y data. Note that each axis lists two possible transformations. While *either* of these transformations has the *poten\nnsformation specifically? Why not some other function to linearize the data?\nPractically, many other mathematical operations that modify the relative scales of `\"Age\"` and `\"Length\"` could have worked here.\n## Multiple Linear Regression\nMultiple linear regression is an extension of simple linear regression that adds additional features to the model. The multiple linear regression model takes the form:\n$$\\hat{y} = \\theta_0\\:+\\:\\theta_1x_{1}\\:+\\:\\theta_2 x_{2}\\:+\\:...\\:+\\:\\theta_p x_{p}$$\nOur predicted value \n to share some positive relationship with gross national income, we are not able to describe the specifics of this trend in much detail.\nA transformation would allow us to visualize this data more clearly, which, in turn, would enable us to describe the underlying relationship between our variables of interest.\nWe will most commonly apply a transformation to **linearize a relationship** between variables. If we find a transformation to make a scatter plot of two variables linear, we can \"backtrack\" to find ",
        "Retrieved Video": "Lec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nX\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_117\nNarges Norouzi\n...\n0\nApplying Transformations\nab T\nWhat makes this plot non-linear?\n3529573\n2. Many large y values are all clumped together, compressing the vertical axis.\n200\n175\n150\n125\n100\nAdult literacy rate\n75\n50\n25\n6\n7\n8\n9\n10\n11\nLog(gross national income per capita)\n23\nCC\nBY\nNC\nS.A\n2024-09-24 11:48:57\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nX\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_117\nNarges Norouzi\n...\n0\nApplying Transformations\nab T\nWhat makes this plot non-linear?\n3529573\n2. Many large y values are all clumped together, compressing the vertical axis.\n200\n175\n150\n125\n100\nAdult literacy rate\n75\n50\n25\n6\n7\n8\n9\n10\n11\nLog(gross national income per capita)\n23\nCC\nBY\nNC\nS.A\n2024-09-24 11:48:57\n\nLec 08 - DS100 Fa24 - Visualiza X\nlab? - JupyterLab\nx +\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_93\nNarges Norouzi\n...\nO\nApplying Transformations\nab\nT\nWhat makes this plot non-linear?\n3529573\n200\n175\n1. A few large outlying x values are distorting\n150\nthe horizontal axis.\n125\n100\nAdult literacy rate\n75\n50\n25\n0\n10000 20000 30000 40000 50000 60000 70000\nGross national income per capita\n21\nNC\n2024-09-24 11:42:52\n",
        "Question": "Can I use the sine function to transform the data, as there will never be an outlier after transformation? (just joke",
        "Timestamp": 3449,
        "TA Response": "I would strongly encourage against this, haha",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The context provided does not mention any upcoming polls. It discusses historical polling methods and the Gallup Poll's techniques but does not specify future polling events.",
        "Recent Audio": "and what's the science behind choosing different colors. So this is an example that for people who are not red, green blind and they have full vision, they would see a number inside this figure. But if you go ahead and simulate it, let me see if I can get the simulation going. So if we go ahead and simulate it, you can also simulate it using different kind of softwares that you have. But what I'm using is Color Oracle. Which basically it will simulate what people with different kinds of color blindness would experience if they look at the plot that you created or they look at the slides that you create. A lot of time when we create a slice, we also stimulated to make sure that all the figures are visible by all the students as well. So for example, if I go ahead and simulate this view, you could kind of guess that there is some number in the center, but it's really hard to see. And again, you can simulate for. Different types of color deficiencies, and then you would basically be able to see if your plot is accessible or not. So Speaking of accessibility, what makes a plot accessible or useful or or choice of the color more effective than other kind of color choices is based on the color maps that we're choosing. So each kind of color map has a set of different colors, and different colors are going to encode different intensities or different values.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\nx\nJupyterLab\n\u00d7\n+\nC \u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1789\nNarges Norouzi\nColormaps\nO\n[ 7 :\n...\nO\nab T\n3529573\nJet\nViridis\n49\nNC\n2024-09-24 12:16:04\n\nLec 08 - DS100 Fa24 - Visualiza X\nJupyterLab\nx +\nC \u00ba% docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1789\nNarges Norouzi\nColormaps\nO\n...\nO\n| 7 :\nabo T\n3529573\nJet\nViridis\n49\nO\nHY\nNC\n2024-09-24 12:16:18\n",
        "Retrieved Audio": "Yes.\nYes.\nor not, right? So.",
        "Retrieved Notes": "y had a dismal response rate (about 24\\%); who knows how the other non-respondents would have polled? The *Digest* folded just 18 months after this disaster.\nAt the same time, George Gallup, a rising statistician, also made predictions about the 1936 elections. Despite having a smaller sample size of \u00e2\u0080\u009conly\u00e2\u0080\u009d 50,000 (this is still more than necessary; more when we cover the Central Limit Theorem), his estimate that 56\\% of voters would choose Roosevelt was much closer to the actual result (61\\%). Gallup\n**\"No\"**, then the surveyor knows the **individual didn't cheat**. We assume the individual is comfortable revealing this information. \nGenerally, we can assume that the coin lands heads 50% of the time, masking the remaining 50% of the \"No\" answers. We can therefore **double** the proportion of \"No\" answers to estimate the **true** fraction of \"No\" answers.\n**Election Polls**\nToday, the *Gallup Poll* is one of the leading polls for election results. The many sources of biases -- who responds to polls? Do v\noters tell the truth? How can we predict turnout? -- still remain, but the *Gallup Poll* uses several tactics to mitigate them. Within their sampling frame of \"civilian, non-institutionalized population\u00e2\u0080\u009d of adults in telephone households in continental U.S., they use random digit dialing to include both listed/unlisted phone numbers and to avoid selection bias. Additionally, they use a within-household selection process to randomly select households with one or more adults. If no one answers, re-call mul",
        "Retrieved Video": "\n2024-09-24 11:51:54\n\nll 2024\n7\n...\n7\nS\nV\n2024-09-24 11:16:57\n\nge is this plot trying to convey?\nP\n.\n289750\nIs anything suspicious?\n935573\n2006\n2007 2008 2009 2010 2011\n2012 2013\n41\nNC\n2024-09-24 12:08:23\n",
        "Question": "Will there be another poll?",
        "Timestamp": 3952,
        "TA Response": "Yes",
        "Lecture": 8
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The lecture notes and audio do not specifically mention the influence of linguistics or the number of colors recognized by a language on color mapping. The focus is more on the perceptual aspects of color mapping, accessibility for color vision impairments, and the effectiveness of different colormaps for visualizing data. While linguistic factors can influence color perception, the lecture content provided does not address this aspect directly.",
        "Recent Audio": "the KDE plots 4 different types of colors separately and use the color channel in order to distinguish between categories. So the most effective place for to use the color in the case that there is a kind of combination of qualitative variables and quantitative variable is to visualize categories using the color. In terms of visualizing qualitative variables, how can we use colour more efficiently is that we can have different kinds of colour schemes or colour maps. So for example on the left hand side I'm using a colour map that is. So it starts from some of these high numbers from some of the high numbers to be colored as as white. And then it goes all the way to this dark this, this dark color that we have. And that's, that's how we can we can visualize these numbers in a sequential way. There's also the type of colouring scheme that is called divergent colouring scheme. So if you want to communicate to. Opposite kind of views and two different specific categories that you don't want to make them sequential. To encode any order you could actually use divergent color map. So here's the color map I think encoding. Different national governor suings and different states. So that's why you see some of the states be blue and some of the states be red. And it really doesn't make sense to put them on a purple sequential color scale. They should be separate colors communicating different views, specifically using the plot, thankfully.",
        "Recent Video": "Lec 08 - DS100 Fa24 - Visualiza\nx\nJupyterLab\nX\n35 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2us0jCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1842\nNarges Norouzi\n...\nSequential vs. Diverging Colormaps for Quantitative Data\nabp\nT\n3529573\nNational Governor Swing By County: 1990-2018\nJanuary\n112\n115\n145\n171\n196\n204\n242\n284\n315\n340\n360\n417\n- 600\nFebruary\n118\n126\n150\n180\n196\n188\n233\n277\n301\n318\n342\n391\nMarch\n132\n141\n178\n193\n236\n235\n267\n317\n356\n362\n406\n419\n.500\nApril\n129\n135\n163\n181\n235\n227\n269\n313\n348\n848\n396\n461\nMay\n121\n125\n172\n183\n229\n234\n270\n318\n355\n363\n420\n472\nJune\n135\n149\n178\n218\n243\n264\n315\n374\n422\n435\n472\n535\n-400\nJuly\n148\n170 199\n230\n264\n302\n364\n413\n465\n491\n548\n622\nmonth\nAugus\n148\n170\n199\n242\n272\n293\n347\n405\n467\n505\n559\n606\n- 300\nSeptember\n136\n158\n184\n209\n237\n259\n312\n355\n404\n404\n463\n508\nOctobe\n119\n133\n162\n191\n211\n229\n274\n306\n347\n359\n407\n461\n- 200\nNovember\n104\n114\n146\n172\n180\n203\n237\n271\n305\n310\n362\n390\nDecember\n118\n140\n166\n194\n201\n29\n278\n306\n336\n337\n405\n432\n1949 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 1960\nyear\nIf the data progresses from low to high,\nIf low and high values deserve equal\nuse a sequential scheme where lighter\nemphasis, use a diverging scheme\ncolors are for more extreme values.\nwhere lighter colors represent middle\nvalues.\n54\nCC\nHY\nNC\nSA\n2024-09-24 12:21:01\n\nLec 08 - DS100 Fa24 - Visualiza\nX\nJupyterLab\nG\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g2686122ddf1_1_0\nNarges Norouzi\n[m] >\n...\nDefault matplotlib Colormaps\nab,\nT\nO\n3529573\nPerceptually Uniform Sequential colormaps\nSequential colormaps\nGreys\nviridis\nPurples\nBlues\nplasma\nGreens\nOranges\ninferno\nReds\nYOrBr\nmagma\nYIOrRd\nQualitative colormaps\nOrRd\nPuRd\nPastel1\nRdPu\nBuPu\nPastel2\nGnBu\nPuBu\nPaired\nYIGnBu\nPuBuGn\nAccent\nBuGn\nYIGn\nDark2\nSet1\nDiverging colormaps\nP\u0130YG\nSet2\nPRGn\nBrBG\nSet3\nPuOr\nRdGy\ntab10\nRdBu\ntab20\nRdYIBu\nRdYIGn\ntab20b\nSpectral\ncoolwarm\ntab20c\nbwT\nseismic\nTaken from matplotlib documentation.\n55\nNC\n2024-09-24 12:22:18\n",
        "Retrieved Audio": "The concept of a color mapping perceptual uniform.\nHere's the color map I think in coding.\nDifferent types of color deficiencies, and then you would basically be able to see if your plot is accessible or not. So Speaking of accessibility, what makes a plot accessible or useful or or choice of the color more effective than other kind of color choices is based on the color maps that we're choosing. So each kind of color map has a set of different colors, and different colors are going to encode different intensities or different values.",
        "Retrieved Notes": "ye. \nWe already explored using color to encode a categorical variable in our scatter plot. Let's now discuss the uses of color in novel visualizations like colormaps and heatmaps.\n5-8% of the world is red-green color blind, so we have to be very particular about our color scheme. We want to make these as accessible as possible. Choosing a set of colors that work together is evidently a challenging task!\n#### Colormaps\nColormaps are mappings from pixel data to color values, and they're often used to highligh\ne colors poorly encode numerical data. To understand why, let's analyze the following images.\n:::: {.columns}\n::: {.column width=\"30%\"}\n<img src=\"images/jet_four_by_four.png\" alt='four_by_four_colormap' width='300'>\n:::\n::: {.column width=\"20%\"}\n<!-- empty column to create gap -->\n:::\n::: {.column width=\"30%\"}\n<img src=\"images/jet_3_images.png\" alt='jet_3_colormap' width='250'>\n:::\n::::\nThe diagram on the left compares how a variety of colormaps represent pixel data that transitions from a high to low inten\nely non-linear - this is precisely why it's considered a worse colormap.\n### Harnessing Markings\nIn our earlier discussion of multi-dimensional encodings, we analyzed a scatter plot with four pseudo-dimensions: the two axes, area, and color. Were these appropriate to use? The following diagram analyzes how well the human eye can distinguish between these \"markings\".\n<img src=\"images/markings_viz.png\" alt='markings_viz' width='400'>\nThere are a few key takeaways from this diagram\n- Lengths are easy to discer",
        "Retrieved Video": "Lec 08 - DS100 Fa24 - Visualiza\nX\nJupyterLab\n\u00d7\n+\n->\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1775\nNarges Norouzi\n...\n0\nabo\nT\nI\nK\n3529573\nPerception of Color\nChoosing a set of colors which work\ntogether is a challenging task!\nDownload the Color Oracle App to\nsimulate common color vision\nimpairments.\n48\nNC\n2024-09-24 12:14:47\n\nLec 08 - DS100 Fa24 - Visualiza\nX\nJupyterLab\n\u00d7\n+\n->\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1775\nNarges Norouzi\n...\n0\nabo\nT\nI\nK\n3529573\nPerception of Color\nChoosing a set of colors which work\ntogether is a challenging task!\nDownload the Color Oracle App to\nsimulate common color vision\nimpairments.\n48\nNC\n2024-09-24 12:14:47\n\nv\nLec 08 - DS100 Fa24 - Visualiza\n\u00d7\nJupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/100QsR5pyozga0Zh5GNamUIDh2usOjCu8XLxS2rAAijk/edit#slide=id.g24571923393_0_1781\nNarges Norouzi\n...\nab\nT\n3529573\n13\nPerception of Color\nChoosing a set of colors which work\ntogether is a challenging task!\nDownload the Color Oracle App to\nsimulate common color vision\nimpairments.\n48\n2024-09-24 12:15:11\n",
        "Question": "Does color mapping depend on linguistics in the number of colors recognized by a language?",
        "Timestamp": 4303,
        "TA Response": "Really interesting question... I'm not sure if there's been any work regarding this specifically. As we saw, modern color mapping is mostly based on the illumination / brightness / relative hues of the colors rather than the colors themselves.",
        "Lecture": 8
    }
]