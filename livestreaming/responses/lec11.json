[
    {
        "Conversation History": "\t",
        "GPT Response": "It seems like the lecture has ended, and the Zoom meeting is now closed. If you have any questions about the lecture content or need clarification on any topics discussed, feel free to ask!",
        "Recent Audio": "doing any estimation. The last thing that we will do, and this is what you will see in discussions as we will visualize your residuals. So when we go ahead and visualize residuals, what we want to see is that if this is, if these are the four different data sets that I'm dealing with and and I'm passing the regression line through them, you can kind of see that the regression line matches one or two of them perfectly, but it's not a suitable choice for the rest of them. If you go ahead and visualize the residual meaning that. On the Y axis, you plot the residuals. On the X axis, you plot X. We don't want this residual plot to have any kind of pattern, which is what you're seeing in this first case. That residual plot does not show any pattern, but you're seeing that residual plot absolutely shows the pattern, meaning that linear regression was not a good choice or residual plot in this case also shows the pattern. Also in this case, shows the pattern, right? So what we want at the end is that once you. The residual plot, you really want to make sure that the errors are kind of randomly distributed and equally distributed across X and also across the residual kind of dimension. Your T's are going to more talk about this a little bit more in the discussion. Thank you for coming. We're going to talk about constant model and more loss functions on Thursday.",
        "Recent Video": "Lec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\n\u00d7\n+\n->\n5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g28310efdf74_0_5\nNorges Norouzi\nLec 10 - DS100 Fa24 - Modeling & SLR\n1\n1\n59 0\nSlideshow\nShare\nFile\nDue to heavy collaborator use, some tools for this presentation may become unavailable for larger audiences. Publishing can make files more accessible.\nLearn more\nDismiss\n:\n+\nabg\nT\n1\n3\n4\n5\n6\n7\n8\n9\nEvaluate the\nModel\n55\n1380664\n56\nLecture 10\n+\n57\nIntroduction to Modeling, SLR\n58\nContent credit: Acknowledgments\n59\nR\n60\n60\nIntroduction to Modeling, SLR\nA\nClick to add speaker notes\n2024-10-01 12:31:06\n\n",
        "Retrieved Audio": "",
        "Retrieved Notes": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
        "Retrieved Video": "+ +\nslido\nronous\nJoin at\nslido.com\n#1078 489\n-------\n",
        "Question": "zoom meeting closed",
        "Timestamp": 0,
        "TA Response": "Hi, we've just opened it back up",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The course website for Fall 2024 provides links to the slides and code in HTML format, but if you're looking for the code notebook, you might need to access it through the DataHub or the lecture demo notebook link provided in the course materials. You can view or download specific files, such as the Tuberculosis dataset, from the [lecture demo notebook](https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Ffa23-student&urlpath=lab%2Ftree%2Ffa23-student%2Flectu). If you need further assistance, consider reaching out to your instructor or TA for guidance on accessing the notebooks.",
        "Recent Audio": "really want to make sure that the errors are kind of randomly distributed and equally distributed across X and also across the residual kind of dimension. Your T's are going to more talk about this a little bit more in the discussion. Thank you for coming. We're going to talk about constant model and more loss functions on Thursday. All right, ready to go finally. So this is our lecture 11. I'm actually very excited about this lecture. This is a lecture that we're going to actually get some practice, but everything a bit seen in the previous lecture, we're going to practice a lot more. So we are going to see a new model, so it's called a constant model we're going to talk about. Some more loss functions. We're going to try keep optimizing models and then see how this calculus thing that we started using in fitting a model is going to actually be. Really helpful. And then we're going to also just briefly talk about transformation. You had seen transformation before individualization lecture, but you're going to again see it one more time and you're going to see how it can be really helpful in linear regression. So this is where we are. So think about this lecture as a kind of a debrief lecture plus a little bit of examples that we're going to go over so that it connects us from the previous lecture to also. Lecture tree. There is also going to be some homework for you to do.",
        "Recent Video": "+\nPlan for Next Few Lectures: Modeling\n1078489\na.\nQuestion &\nData\nProblem\nAcquisition\nFormulation\nPrediction and\nExploratory\nInference\nData Analysis\nReports, Decisions,\nand Solutions\n(today)\nModeling I:\nModeling II:\nModeling III:\nIntro to Modeling, Simple\nDifferent models, loss\nMultiple Linear\nLinear Regression\nfunctions, linearization\nRegression\n3\n\n+\n1078489\nModeling Process Reiteration\n.\nEvaluating Model the SLR Model\n.\nIteration 2: Constant Model + MSE\n.\nIteration 3: Constant Model + MAE\nTransformations to Fit Linear Models\nNotation for Multiple Linear Regression\nToday's Roadmap\nLecture 11, Data 100 Fall 2024\n",
        "Retrieved Audio": "Lecture tree. There is also going to be some homework for you to do.\nLike kind of a debrief lecture plus a little bit of examples that we're going to go over so that it connects us from the previous lecture to also lecture tree. There is also going to be some homework for you to do about getting prep for homework tree that I'm going to tell you a little bit in the middle of the lecture. So today I'm going to start by again, going through multiple examples and I'm going to first start off by just reviewing what we had seen before.\nAll right, ready to go finally. So this is our lecture 11. I'm actually very excited about this lecture. This is a lecture that we're going to actually get some practice, but everything a bit seen in the previous lecture, we're going to practice a lot more. So we are going to see a new model, so it's called a constant model we're going to talk about.",
        "Retrieved Notes": "ithin the first two days of class. Discussions will be on Tuesdays and Wednesdays from 9 am-4 pm. There will be no direct lab sessions. See our course policies for more information about labs.\n\n___\n\n## Where can I find links to the class schedule, optional textbook, and other additional relevant resources?\n\nThe course website for Fall 2024 is linked [here](https://ds100.org/fa24/).\n\n___\n\n## I can\u00e2\u0080\u0099t find any information on bCourses, why?\n\nWe will only be using bCourses for synchronizing access to Gradesco\nnd does not contain binary-encoded data) in many ways:\n1. Using a text editor like emacs, vim, VSCode, etc.\n2. Opening the CSV directly in DataHub (read-only), Excel, Google Sheets, etc.\n3. The `Python` file object\n4. `pandas`, using `pd.read_csv()`\nTo try out options 1 and 2, you can view or download the Tuberculosis from the [lecture demo notebook](https://data100.datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https%3A%2F%2Fgithub.com%2FDS-100%2Ffa23-student&urlpath=lab%2Ftree%2Ffa23-student%2Flectu\nop left corner of the page.\n3.  In the file explorer that appears, click `dsc10-2024-fa`, then navigate to the folder that contains the file you\u00e2\u0080\u0099re trying to download. If you\u00e2\u0080\u0099re working on Homework 3, for instance, you\u00e2\u0080\u0099ll click `homeworks` and then `hw03`.\n4.  Click the square checkbox to the left of your assignment notebook (e.g. `hw03.ipynb`). Click \u00e2\u0080\u009cShutdown\u00e2\u0080\u009d at the top. Then, click the checkbox again and click \u00e2\u0080\u009cDownload\u00e2\u0080\u009d. This should download your notebook as a `.ipynb` file.\n\n## []",
        "Retrieved Video": "+ +\nslido\nronous\nJoin at\nslido.com\n#1078 489\n-------\n\n+ +\nslido\nronous\nJoin at\nslido.com\n#1078 489\n-------\n\n+\n+\nPlan for Next Few Lectures: Modeling\n1078489\n?\nQuestion &\nData\nProblem\nAcquisition\nFormulation\nPrediction and\nExploratory\nInference\nData Analysis\nReports, Decisions,\nand Solutions\n(today)\nModeling I:\nModeling II:\nModeling III:\nIntro to Modeling, Simple\nDifferent models, loss\nMultiple Linear\nLinear Regression\nfunctions, linearization\nRegression\n3\nA\n",
        "Question": "In the ds100 website I only see the slides and the code in HTML but not the code notebook",
        "Timestamp": 82,
        "TA Response": "Datahub is currently down so we have had issues getting this ready for students! Hopefully UC Tech will be able to rectify the issue soon and the demo will then be posted.",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "In the context of simple linear regression, \\( \\hat{y} \\) represents the predicted value of the dependent variable, which is calculated using the model equation \\( \\hat{y} = \\theta_0 + \\theta_1 x \\). You use \\( \\hat{y} \\) when discussing or calculating predictions, while \\( \\theta_0 + \\theta_1 x \\) is used when you are explicitly showing the formula or process of making those predictions. Essentially, \\( \\hat{y} \\) is a shorthand for the expression \\( \\theta_0 + \\theta_1 x \\).",
        "Recent Audio": "talked about last time was that this function is parameterized by Theta. Why? What do I mean by parameterized by teta? I mean that the SLR function that we had at the top is just the family of functions, right? It's just the template for all of the lines. We can have because depending on the Tetra 0 and Theta one value, I can have different lines and therefore because this teta value can change and because this prediction right here can change, therefore the residual can change and therefore the value of my error or the my model risk can also change. So that's why this is the function of Theta and in this case Theta is a vector that is including. Both Tetra 0 and teta one as well. So you will see in the next lecture and also at the end of today's lecture that if I keep adding features, I'll keep adding parameters to this vector. That's why everything is shown by just tetas the annotation. Or the notation that we're using here, just that it includes all the parameters that the model has. In this case, in simple linear regression, it's just two. There's Tetris one. We also talked about at the end that we need to actually go ahead and estimate the optimum set of parameters. We did the calculus. We went ahead and calculate the partial derivative of the risk with respect to Theta zero and also with respect to Theta 1-2 equations, 2 unknowns. We solved it and.",
        "Recent Video": "+\n+\nReview of the The Modeling Process (Simple Linear Regression)\n1078489\n1. Choose a model\nSLR model\ny\n=00+01x\n2. Choose a loss\nL2 Loss\nD(y, y) = (y -1)2\nSquared loss\nfunction\nMean Squared Error\n(MSE)\nn\nMin Prize 0, 0]\nR() = = (U-(00+01.2)2\n3. Fit the model\naverage Toss\ni=1\nMSE for SLR\nwith calculus\ny=00+\u00d4\u00a1x\n4. Evaluate model\n01 = P\nperformance\n7\n\n+\nReview of the The Modeling Process (Simple Linear Regression)\n1078489\n1. Choose a model\nSLR model\n=00+01x\n2. Choose a loss\nL2 Loss\nL(y,g) = (y -\u00a7)2\nSquared loss\nfunction\nMean Squared Error\n(MSE)\nMin rizz 0, 0]\n3. Fit the model\naverage Toss\ni=1\nMSE for SLR\nwith calculus\n00=9-01\u20b8\n4. Evaluate model\nD:\n0 = Pay\nperformance\n",
        "Retrieved Audio": "What we talked about last time was that we don't care about individual single point in our data set, but means that care about the average error that we're in carrying over the entire data set. So that's why we would go ahead and plug in what you're seeing from Y hat. Here we're plugging in the equivalent for Y hack, which is the equation that's going to give you the Y hat 0 plus Theta 1 * X and then we would say that for all the ground tourist values.\nEquation that's going to give you the Y hat 0 plus Theta 1 * X. And then we would say that for all the ground truth values, we would go ahead and calculate the residual between the ground truth value Yi and then the residual value that we have that is calculated based on XI. There should be a subscript XI here and then we're calculating all of those residuals. We are squaring those residuals that you're seeing. There's a square here so.\nOr the notation that we're using here, just that it includes all the parameters that the model has. In this case, in simple linear regression, it's just two. There's Tetris one. We also talked about at the end that we need to actually go ahead and estimate the optimum set of parameters. We did the calculus. We went ahead and calculate the partial derivative of the risk with respect to Theta zero and also with respect to Theta 1-2 equations, 2 unknowns. We solved it and.",
        "Retrieved Notes": "0| = \\begin{cases} y_i - \\theta_0 \\quad \\text{ if } \\theta_0 \\le y_i \\\\ \\theta_0 - y_i \\quad \\text{if }\\theta_0 > y_i \\end{cases}$$\n- Taking derivatives:\n$$\\frac{d}{d\\theta_0} |y_i - \\theta_0| = \\begin{cases} \\frac{d}{d\\theta_0} (y_i - \\theta_0) = -1 \\quad \\text{if }\\theta_0 < y_i \\\\ \\frac{d}{d\\theta_0} (\\theta_0 - y_i) = 1 \\quad \\text{if }\\theta_0 > y_i \\end{cases}$$\n- This means that we obtain a different value for the derivative for data points where $\\theta_0 < y_i$ and where $\\theta_0 > y_i$. We can su\nta_0 + \\theta_1 x_i$$\nThe examples above are known as **parametric models**. They relate the collected data, $x_i$, to the prediction we make, $\\hat{y}_i$. A few parameters ($\\theta$, $\\theta_0$, $\\theta_1$) are used to describe the relationship between $x_i$ and $\\hat{y}_i$.\nNotice that we don't immediately know the values of these parameters. While the features, $x_i$, are taken from our observed data, we need to decide what values to give $\\theta$, $\\theta_0$, and $\\theta_1$ ourselves. This is the heart \n}{n} \\sum_{i=1}^n y_i - \\hat{\\theta}_0 - \\hat{\\theta}_1 x_i = 0 $$ \n$$\\left(\\frac{1}{n} \\sum_{i=1}^n y_i \\right) - \\hat{\\theta}_0 - \\hat{\\theta}_1\\left(\\frac{1}{n} \\sum_{i=1}^n x_i \\right) = 0$$\n$$ \\hat{\\theta}_0 = \\bar{y} - \\hat{\\theta}_1 \\bar{x}$$\nWith a bit more maneuvering, the second equation gives the estimate of $\\hat{\\theta}_1$. Start by multiplying the first estimating equation by $\\bar{x}$, then subtracting the result from the second estimating equation.\n$$\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i",
        "Retrieved Video": "+\n+\nReview of the The Modeling Process (Simple Linear Regression)\n1078489\n1. Choose a model\nSLR model\ny\n=00+01x\nL(y,g) = (y -\u00a7)2\n2. Choose a loss\nL2 Loss\nSquared loss\nfunction\nMean Squared Error\n(MSE)\nMin Prize 0, 0]\nR() = = (U-(00+01.2)2\nn\n3. Fit the model\naverage Toss\ni=1\nMSE for SLR\nwith calculus\ny=00+\u00d4\u00a1x\n00=9-01\u20ab\n4. Evaluate model\n01 = P\nperformance\n-------\n\n+\n+\nReview of the The Modeling Process (Simple Linear Regression)\n1078489\n1. Choose a model\nSLR model\ny\n=00+01x\n2. Choose a loss\nL2 Loss\nD(y, y) = (y -1)2\nSquared loss\nfunction\nMean Squared Error\n(MSE)\nn\nMin Prize 0, 0]\nR() = = (U-(00+01.2)2\n3. Fit the model\naverage Toss\ni=1\nMSE for SLR\nwith calculus\ny=00+\u00d4\u00a1x\n4. Evaluate model\n01 = P\nperformance\n7\n\nReview of the The Modeling Process (Simple Linear Regression)\ny = 00+ 01x\n1078489\n1. Choose a model\nSLR model\nL(y,y) = (y -4)2\n2. Choose a loss\nL2 Loss\nSquared loss\nfunction\nMean Squared Error\n(MSE)\nR(0) = = {(vi-(00+0,12)2\nn\n3. Fit the model\ni=1\nMSE for SLR\n4. Evaluate model\nperformance\n7\n-------\n",
        "Question": "when do we use y^ versus plugging in theta0 + theta1x?",
        "Timestamp": 436,
        "TA Response": "I think they are equivalent, so just when it is convenient vs when we want to be specific about values we are plugging in",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "No, points above the perfect prediction line in a residual plot indicate underestimation, not overestimation. In a residual plot, the residual is calculated as the actual value minus the predicted value. If the residual is positive (above the line), it means the actual value was higher than the predicted value, indicating underestimation. Conversely, points below the line indicate overestimation.",
        "Recent Audio": "zero and Theta one value for all of these four data sets, what we will get is actually all of these lines. So you see there is this line for data set one, this line for data set 2, for data set three and data set 4. So this is kind of the case that I went ahead and calculated the statistics, correlation was high enough, I went ahead and did the modeling, but I should have visualized the. That said beforehand, so that I could see visually that there is nonlinear trend or there is outlier that I have to remove before doing the modeling. And because I've not done that, then the residual plots are kind of looking funny. So for this case, the residual plot is fine. So this is the residual plot that we're plotting, which is equal to residual being equal to 0, which basically means that y -, y hat is equal to 0, which effectively means that Y is equal to Y hat. So that's. Of perfect prediction. So if the points are on that line, that's perfect prediction. If the points are above that line, that means that there was underestimation and then overestimation is under the line. So that's kind of how you can see because we are, we know from the previous lecture that with MSC the average errors or the average residuals is equal to equal to 0, we should have equal number of overestimations and underestimations and so on and so forth. That's why.",
        "Recent Video": "File Edit View Run Kemel Tabs Settings Help\nleci1.jpynb\nX\na + x 00 * ( * * Download Markdown v )\nab\nT\n0\nprint()\nprint ()\n>>> Dataset I:\nK meam : 9.00, y_mean : 7.50\nK_Stdev: 3.16, y_stdev: 1.94\n\" = Correlation(x, y): 0.816\n*\nheta_@: 3.00, heta_1: 0.50\nRMSE: 1.119\nLS fit\nResiduals\n11\n2\n1\ne=0\n7\ny-j =0\ny= y\n=2\n8\n10\n12\n14\n10 12 14\n>>> Dataset IE:\nK_mean : 9.00, y_mean : 7.50\nK_stdev: 3.36, y_stdev: 1.94\nr . Correlation(x, y): 0.816\nheta_@; 3.00, heta_1: 0.50\nRMSE: 1.119\nSimple\n2 1. 1 @\nPython 3 (pykernel) | Idle Mem: 335.85 / 4096.00 MB\nMode: Command\nLn 1, Coll 1 lec1|d-100MM\n\n+\nFile Edit View Run Kemel Tabs Settings Help\nleci1.jpynb\nX\n+\na + x 00 . . ( > & Download Markdown w\nab\nT\nO\n93) Dataset Il:\nK_mean 1 9.00, y_mean : 7.50\nK_stdev: 3.16, y_Stdev: 1.94\n\"= Correlation(x, y): 0.816\n*\nRMSE: 1.119\nheta_0: 3.00, heta_1: 0.50\nLS fit\nResiduals\n10\n1.0\n0.5\n-0.5\n-1.0\n-1.5\n3\n-2.0\n10\n12\n14\n6\n10\n12\n14\n>>> Dataset III:\nK_meam : 9.00, y_mean : 7.50\nK_Atdev: 3.36, y_atdev: 1.94\nr . Correlation(x, y): 0.816\nheta @: 3.00, heta_1: 0.50\nRMSE: 1.318\nSimple\n2 1. 1\nPython 3 (pykernel) | Idle Mem: 335.86 / 4096.00 MB\nMode: Command\nLn 1, Col 1 lec1 1 30ditt\n",
        "Retrieved Audio": "Of perfect prediction. So if the points are on that line, that's perfect prediction. If the points are above that line, that means that there was underestimation and then overestimation is under the line. So that's kind of how you can see because we are, we know from the previous lecture that with MSC the average errors or the average residuals is equal to equal to 0, we should have equal number of overestimations and underestimations and so on and so forth. That's why.\nOf perfect prediction. So if the points are on that line, that's perfect prediction. If the points are above that line, that means that there was underestimation and then overestimation is under the line. So that's kind of how you can see because we are, we know from the previous lecture that with MSC the average errors or the average residuals is equal to equal to 0, we should have equal number of overestimations and underestimations and so on and so forth. That's why.\nAnd then the last piece that I again mentioned was that we always want to visualize residuals. Remember, residual was calculated using this equation that we said that it's equal to the ground truth value minus the predicted value. So there is sometimes that residual is positive, sometimes that the residual is negative. And what we learn from data 8 is that if we plot the residual plot, we do not want to see any pattern.",
        "Retrieved Notes": "ttps://inferentialthinking.com/chapters/15/5/Visual_Diagnostics.html?highlight=heteroscedasticity#detecting-heteroscedasticity): a good regression fit should display no clear pattern in its plot of residuals. The residual plots for Anscombe's quartet are displayed below. Note how only the first plot shows no clear pattern to the magnitude of residuals. This is an indication that SLR is not the best choice of model for the remaining three sets of points.\n<!-- <img src=\"images/residual.png\" alt='residual' wid\nhe plateau to the right is slightly tilted. Because gradient descent follows the tilted loss surface downwards, it never converges.\nThe diverging weights cause the model to be **overconfident**. Say we add a new point $(x, y) = (-0.5, 1)$. Following the behavior above, our model will incorrectly predict $p=0$, and thus, $\\hat y = 0$.\n<center><img src=\"images/toy_linear_separable_dataset_2.png\" alt='toy_linear_separability' width='450'></center>\n<br>\nThe loss incurred by this misclassified point is infinite.\nctions above 1 and below 0). This means that the output can't always be interpreted (what does it mean to predict a class of -2.3?).\nOur usual linear regression framework won't work here. Instead, we'll need to get more creative.\n### Graph of Averages\nBack in [Data 8](https://inferentialthinking.com/chapters/08/1/Applying_a_Function_to_a_Column.html#example-prediction), you gradually built up to the concept of linear regression by using the **graph of averages**. Before you knew the mathematical underpinnin",
        "Retrieved Video": "curate\" predictions (lower \"average loss\" across\ndata)\n3. Visualization:\nLook at a residual plot of e; = y; - y; to visualize the difference between actual and\npredicted values.\n11\n------\n\ncurate\" predictions (lower \"average loss\" across\ndata)\n3. Visualization:\nLook at a residual plot of e; = y; - g; to visualize the difference between actual and\npredicted values.\n11\n------\n\nw()\nChecking the residuals will also shed light on the differences among these four datasets.\nIf you remember from Data 8, \"the residual plot of a good regression shows no pattern\".\n",
        "Question": "Are points above the perfect prediction line for residuals overestimation?",
        "Timestamp": 1209,
        "TA Response": "The points represent the actual values, and our line is the prediction line. If the points are above the line, then it shows that our prediction is lower than the actual value, so it would be an underestimation.",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Minimizing RMSE instead of MSE would not change the optimal parameters of the model because RMSE is simply the square root of MSE. Both metrics are minimized simultaneously since RMSE is a monotonic transformation of MSE. The choice between them typically depends on the preference for reporting error in the same units as the target variable, which RMSE provides. Thus, the underlying optimization process remains the same.",
        "Recent Audio": "overall of them. And again, you know that I've repeated it multiple times. That is parameterized by teta. Depending on teta 0 value that you pick, you'll get a different loss value. And our goal is to find out what is the optimum Theta zero that gives us the minimum. Cost function for the cost value for this objective function. What I've done here is that I replaced why I had with teta 0 'cause that makes it simpler. So my function is now a very simple function that the Y is are always given from the data set and the only other argument that I have or the parameter that I have is set to 0 and I just need to optimize this function that is parameterized by Theta zero with respect to Theta 0, which means that I can just use. Right, derivatives. So writing it again, there are different approaches that I can do it. There is one way that I can just go through a calculus, just take the derivative, find the optimum point and the minimum point and then report that minimum point as the optimum. Teta 0 has value, but the data set of Bubba T because it was a small enough, you could just plug in the values and just find the optimum value, right? It's just. The normal calculation that you would do, but usually we don't have the luxury of having just a small data set and we would usually have millions and millions of data points.",
        "Recent Video": "Fit the Model: Three Approaches\n\u00c2(00) = - E (vi-0)2\n1078489\ni=1\nApproach 1\nIf you want to prove the general case for any data, you could\ndirectly minimize the objective. We can show that average loss is minimized by\n00 = mean(y) = }\nApproach 2\nIf you know your data D = {20, 21, 22, 29, 33}, you could modify the\nobjective by plugging in values first:\nR(0) = = (20 - 00)2 + (21 - 00)2 + (22 - 0)2 + (29 - 00)2 + (33 - 00)2)\n25\n------\n\nFit the Model: Three Approaches\n\u00c2(00) = - E (Vi-0)2\n1078489\ni=1\nApproach 1\nIf you want to prove the general case for any data, you could\ndirectly minimize the objective. We can show that average loss is minimized by\n\u00d4o = mean(y) = }\nApproach 2\nIf you know your data D = {20, 21, 22, 29, 33}, you could modify the\nobjective by plugging in values first:\nR(0) = = ((20-00)2+(21-0)2+(22-00)2+(29-00)2+(33-00)2)\n25\n------\n",
        "Retrieved Audio": "Was high enough then we would go ahead and do the modeling. We talked about root mirror squared error. So root minimum squared error means that I'm calculating the same MSC function that I was trying to optimize. But this time I'm going to tell you the square root of this MSD function because what I want to do is I want to make sure that I'm reporting the error in the same unit as Y. So why is that the case? For example, say that I'm.\nBecause we're just estimating some parameters for some model using the least squares loss function. So when you say least squares, when you hear about least the squares regression, that means that it's a regression line that is optimized using at least a squares function or using the MSD function. So this is kind of the terminology and where they're coming from. So once I have the optimal parameters or irrespective of whether I have optimum parameters or not, if every time.\nTell you the square root of this MSC function, because what I want to do is I want to make sure that I'm reporting the error in the same unit as Y. So why is that the case? For example, say that I'm predicting some length value. Length is the target that I'm predicting. If I tell you the error in, for example, meter score, it's hard for you to make sense of what that meter score is and how do you compare different models as opposed to me going back to the major kind of unit?",
        "Retrieved Notes": "t Mean Square Error (RMSE). However, is RMSE a good proxy for fairness in this context?\nBroad metrics of error like RMSE can be limiting when evaluating the \"fairness\" of a property appraisal system. RMSE does not tell us anything about the distribution of errors, whether the errors are positive or negative, and the relative size of the errors. It does not tell us anything about the regressivity of the model, instead just giving a rough measure of our model's overall error. \nEven with a low RMSE, we can't g\niginal observed $y_i$'s, we might be inclined to say that our model has done well.\n   - (If we're fitting a linear model) Compute the correlation $r$. A large magnitude for the correlation coefficient between the feature and response variables could also indicate that our model has done well.    \n2. Performance metrics:\n   - We can take the **Root Mean Squared Error (RMSE)**.\n     - It's the square root of the mean squared error (MSE), which is the average loss that we've been minimizing to determine optima\nood model?\nTo illustrate this process, we derived the optimal model parameters under simple linear regression (SLR) with mean squared error (MSE) as the cost function. A summary of the SLR modeling process is shown below:\n<div align=\"middle\">\n<img src=\"images/slr_modeling.png\" alt='modeling' width='600'>\n</div>\nIn this lecture, we'll dive deeper into step 4 - evaluating model performance - using SLR as an example. Additionally, we'll also explore the modeling process with new models, continue familiarizing ",
        "Retrieved Video": "+\nMinimizing MSE is Minimizing Squared Residuals\nR(0) = = C(y: - ))2\nn\n1078489\nLower residuals = better regression fit!\nResidual ('error\") in prediction\nLeast Squares Fit\nSuboptimal Fit\n50000\n50000\n40000\n40000\n30000\n30000\n20000\n20000\nCharacters\nCharacters\n10000\n200\n400\n10000\n200\n400\nPeriods\nPeriods\n------\n\n+\nMinimizing MSE is Minimizing Squared Residuals\nR(0) = 1 E(Vi-gi)2\nn\n1078489\nLower residuals = better regression fit!\nResidual ('error\") in prediction\nLeast Squares Fit\nSuboptimal Fit\n50000\n50000\n40000\n40000\n30000\n30000\n20000\n20000\nCharacters\nCharacters\n10000\n10000\n200\n400\n200\n400\nPeriods\nPeriods\n------\n\nMinimizing MSE is Minimizing Squared Residuals\nR(0) = 1 E(Vi - gi)2\n1078489\nLower residuals = better regression fit!\nResidual ('error\") in prediction\nLeast Squares Fit\nSuboptimal Fit\n50000\n50000\n40000\n40000\n30000\n30000\n20000\n20000\nCharacters\nCharacters\n10000\n10000\n200\n400\n200\n400\nPeriods\nPeriods\n------\n",
        "Question": "Would anything change if we minimized RMSE instead of MSE?",
        "Timestamp": 1732,
        "TA Response": "If we used RMSE instead of MSE, we would get the same optimal parameters. This is because by minimizing MSE, we are essentially minimizing RMSE (since it's just taking the square root of MSE).",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The optimal value for a variable, often denoted with a hat (e.g., \\(\\hat{\\theta}\\)), is determined during the estimation process when the parameters are calculated to minimize a loss function, such as the Mean Squared Error (MSE). This is achieved by setting the derivative of the loss function with respect to the parameter to zero and solving for the parameter, as shown in the calculus steps provided. In contrast, prediction involves using these estimated parameters to predict outputs for unseen data. Thus, \\(\\hat{\\theta}\\) is optimal in the context of estimation, not prediction.",
        "Recent Audio": "keep doing it, I'll keep taking derivatives and you'll get comfortable with it if I can do it in front of the class. You can just do it on the home breaks and you will actually do it on your homework. 5 It's a very fun homework and you will get a lot of practice with it, but you'll have to start early. Setting it equal to 0. When I set it equal to 0, what happens? So I'm finding the optimum when optimum happens when the derivative is equal to 0. Which means that instead of Ted to 0, now I have Ted to zero half. So I'm going to write this as -2 over north of the summation of I from 1 up to north of Yi minus Theta 0, hat this time equal to 0. OK, and now I can just go ahead and solve it for Tetta 0 hat, which again is going to be that -2 / n times the summation of I from 1 up to north of Yi minus. It's a zero hat is equal to 0. I can just multiply 2 sides by negative N over 2 to get rid of all of those coefficients over there, which effectively means that I can cross this out and I'll write this as summation of I from 1 up to north of Yi minus that to 0 hat equal to 0. I'll separate out those two terms. I'll turn this into I from 1 up to north of Yi minus.",
        "Recent Video": "+\nFit the Model: Calculus for the General Case\n1. Differentiate with respect to 0g.\n3. Solve for 60-\n1078489\n( R(O)= (\u2211(-0.0)\n\uc989 \ub3c4 (yi_d)=0\nDerivative of sun is\nsum of derivatives\n= To(31-80)2\n== > 2(yi-80)(-1\nChairrule\n== 2\u2211(yi-80)\nSimplify const ants\n2. Set equal to 0.\n0=2(yi-60)\n29\n------\n\n+\n2\nFit the Model: Calculus for the General Case\nT\n1. Differentiate with respect to 0g.\n3. Solve for 60-\n1078489\n= { (y ; - 0 ) = 0\nDerivative of sun is\nsum of derivatives\n=== 2(yi-80)(-1)\nChair rule\n== 2\u2211(yi-80)\nSimplify const ants\n2. Set equal to 0.\n0=2(yi-60)\n29\n------\n",
        "Retrieved Audio": "Optimize using at least a squares function or using the MSD function. So this is kind of the terminology and where they're coming from. So once I have the optimal parameters or irrespective of whether I have optimum parameters or not, if every time that I plug in the X value and these parameters are known, I can just go ahead and get my prediction. So that's the difference between estimation and prediction. And I just want to make sure that we're all on the same page.\nKind of reminding you of the terminology that I've been carefully using in the past lecture and also in today's lecture. There is a notion of estimation and there is a notion of prediction.\nt as the predicted value.",
        "Retrieved Notes": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of \nariables, as well as how to transform variables to help **linearize** a dataset and fit these models to maximize their performance.\nAll of this was done with one major caveat: the regression models we've worked with so far are all **linear in the input variables**. We've assumed that our predictions should be some combination of linear variables. While this works well in some cases, the real world isn't always so straightforward. We'll learn an important method to address this issue \u00e2\u0080\u0093 feature engineering\na tradeoff between two competing factors: model variance and training error. \nSo far, our analysis has been mostly qualitative. We've acknowledged that our choice of model complexity needs to strike a balance between model variance and training error, but we haven't yet discussed *why* exactly this tradeoff exists.\nTo better understand the origin of this tradeoff, we will need to dive into **random variables**. The next two course notes on probability will be a brief digression from our work on modeling so ",
        "Retrieved Video": "Welloye. Jpytel.sh\n+\n+\nTerminology: Prediction vs. Estimation\nThese terms are often used somewhat interchangeably, but there is a subtle difference\n1078489\nbetween them.\nEstimation is the task of using data to calculate model parameters.\nPrediction is the task of using a model to predict outputs for unseen data.\n\n+\nTerminology: Prediction vs. Estimation\nThese terms are often used somewhat interchangeably, but there is a subtle difference\n1078489\nbetween them.\nEstimation is the task of using data to calculate model parameters.\nPrediction is the task of using a model to predict outputs for unseen data.\nWe estimate parameters by\nminimizing average loss ...\ny=00+\u00d4\u00a1x\nLeast Squares Estimation\nis when we choose the\nparameters that minimize MSE.\n------\n\n+\nTerminology: Prediction vs. Estimation\nThese terms are often used somewhat interchangeably, but there is a subtle difference\n1078489\nbetween them.\nEstimation is the task of using data to calculate model parameters.\nPrediction is the task of using a model to predict outputs for unseen data.\nWe estimate parameters by\nminimizing average loss ...\ny=00+01x\nLeast Squares Estimation\nis when we choose the\nparameters that minimize MSE.\n",
        "Question": "When is variable^ the optimal for that variable as compared to a prediction",
        "Timestamp": 2051,
        "TA Response": "?",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, you will need to understand the mathematical process for exams, as the course aims to train data scientists who are familiar with the mechanics and core algorithms rather than just using black-box solutions. However, whether specific formulas will be provided during exams typically depends on the course's exam policy, which you should verify with your instructor or course syllabus. It's a good idea to be comfortable with deriving and applying these formulas independently.",
        "Recent Audio": "minus that to 0 hat equal to 0. I'll separate out those two terms. I'll turn this into I from 1 up to north of Yi minus. The summation of I from 1 up to north of the zero hat equal to 0I. I can take this step to 0 hat summation to the other side so that I can get rid of. The negative sign and then I will have one term related to teta 0 hat with respect to something else. That means that I have solved it. So this effectively means that I have the summation of I from 1 up to north of Yi which is equal to summation of I from 1 up to north of Theta 0 hat, which the summation of I from 1 up to north of T to 0 hat, which is a constant. Is going to be equal to north times Theta 0 hat. Right. So then everything is going to be simple. I have one side that I have the zero hat and a coefficient and I have this other side that I have a summation of the things that are that are known to me. I have all of those inputs and I have all of this why I values. So I can directly go ahead and say that teta 0 hat is equal to the summation of I from 1 up to north of Y I / n, which we know is equal to average of Y or Y bar. So that's why.",
        "Recent Video": "+\nFit the Model: Calculus for the General Case\n1. Differentiate with respect to 0g.\n3. Solve for 60-\n1078489\n( R(6)= == ()-00)\n= = (y ; - 0 2) = 0\nDerivative of sun is\nsum of derivatives\n= = > 2(yi-0%)(-1)\nChairrule\n56\n== 2\u2211(3-80)\nSimplify const ants\n2. Set equal to 0.\n11\n=no.\n=)\n0=2(vi-60)\n<6\n1\n50\nn\n29\n------\n\n+\nFit the Model: Calculus for the General Case\n1. Differentiate with respect to 0g:\n3. Solve for 60-\n1078489\n(6)= (\u516d) (vi-00)\n= = (y ; - 0 ) = 0\nDerivative of sun is\nsum of derivatives\n(1g ,- 60)= = >\nChair rule\n56\n=2x(Ji-80)\nSimplify constants\nA\n50\n2. Set equal to 0.\n11\nno.\n=)\n0=2(vi-60)\n156\n1\nQ =\nn\n=\n29\n",
        "Retrieved Audio": "That are going to be a little bit not so interesting, but we'll get comfortable with it because you have to get comfortable with these things as data scientists. What we want to train at Berkeley is data scientists who actually know all the mechanics and also all the cores of all the algorithms as opposed to just using the black boxes from all of the these machine learning frameworks, right? And you're going to gain that by going through all of these math heavy lectures.\nIf I go ahead and calculate.\nIf I go ahead and calculate.",
        "Retrieved Notes": "d we strongly recommend spending time covering the prerequisite material yourself. We will assume that you have prior knowledge of these requirements and that these resources are simply to refresh your memory of concepts that you have previously learned. Please reach out to staff if you have any questions or concerns about this.\n\n**Calculus**: In terms of calculus, you will need to know a few things, most of which are covered within the space of the first homework and lab. Specifically, you will need to kno\nw univariate calculus rules like: Taking derivatives of a univariate function (i.e. f(x), where x is the only variable); Derivative power rule; Knowing derivatives of mathematical functions like: sinx,cosx,logx,ex; Chain rule; Product rule (rarely); Derivatives of sums. We will expect some multivariate fluency like: Taking partial derivatives of a multivariate function (i.e. f(x,y,z), where x,y,z are all variables); Gradients (the concept).\n\n-   Khan Academy: [Derivatives, Definitions, and Basic Rules](http\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of ",
        "Retrieved Video": "+\n+\nThe Modeling Process\nT\n1. Choose a model\nHow should we\n1078489\nrepresent the world?\n2. Choose a loss\nHow do we quantify\nfunction\nprediction error?\nHow do we choose the\n3. Fit the model\nbest parameters of our\nmodel given our data?\n4. Evaluate model\nHow do we evaluate\nperformance\nwhether this process gave\nrise to a good model?\n\n+\nThe Modeling Process\n0\nHow should we\n1078489\n1. Choose a model\nrepresent the world?\n2. Choose a loss\nHow do we quantify\nfunction\nprediction error?\n\n+\nThe Modeling Process\nHow should we\n1078489\n1. Choose a model\nrepresent the world?\n2. Choose a loss\nHow do we quantify\nfunction\nprediction error?\n",
        "Question": "do we have to know this exact process of math for exams? if so, will the formula be given?",
        "Timestamp": 2130,
        "TA Response": "Hi, you should be familiar with the math shown in the lecture, eg minimizing loss to find model parameters. There will be more practice during discussion and homework. You can reference past exam reference sheets to see the formulas we provide.",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Based on the lecture context, it seems that understanding the derivations is important. The course aims to train data scientists who know the mechanics and cores of algorithms, not just the outputs. Therefore, you should be prepared to understand and possibly perform these derivations on exams.",
        "Recent Audio": "of Y I / n, which we know is equal to average of Y or Y bar. So that's why. Pick 25 because that was your initial intuition that you have to take average. And if you want to give a constant prediction, it's better that that constant prediction be the average of all the values that you already know. And you're going to see another way that it can also make sense in another way and get a different value. How are we doing good sideways? Bad. No idea what's going on. OK, it will make sense to tell you the truth. These three lectures, you have to watch them at least twice. Maybe the next lecture you'll have to watch it three times. So that's kind of the trend. But once you build these foundations, you will really be proud of yourself because you will make use of all of these calculus that you have learned. And actually next lecture, you will actually use some linear algebra in order to build ML models. Which is really interesting. OK, moving on. So this is what we found out and kind of summarizing everything that we've seen so far, we had seen that. Y bar is my optimum parameter or my Theta 0 hat. If I go ahead and plug in Y bar into my objective function, this is going to give me this equation which effectively is the sample variance or Sigma to the power. Sigma scored the standard division of Y to the power.",
        "Recent Video": "+\n+\nInterpreting 00 = 9\nThis is the optimal parameter for constant model + MSE.\n1078489\nIt holds true regardless of what data sample you have.\n.\nIt provides some formal reasoning as to why the mean is such a common summary\nstatistic.\nFun fact:\nR(\u00d4o) = R(3) = = >(vi -7)2 = 0}\nn\nThe minimum MSE is the sample variance.\ni=1\n32\n------\n\nInterpreting 00 = y\nThis is the optimal parameter for constant model + MSE.\n1078489\n.\nIt holds true regardless of what data sample you have.\nIt provides some formal reasoning as to why the mean is such a common summary\nstatistic.\nFun fact:\nR(\u00d40) = R(y) = = >(vi - 7)2 = o3\nn\nThe minimum MSE is the sample variance.\ni=1\n32\n",
        "Retrieved Audio": "But they're not going to be as fun because I'm going to still do all of those derivations that are going to be a little bit not so interesting. But we'll get comfortable with it because you have to get comfortable with these things as they as data scientists, what we want to train at Berkeley is data scientists who actually know all the mechanics and also all the cores of all the algorithms as opposed to just using the black boxes from all of the these machine.\nSo we'll keep thinking about how I can take the derivative of that single term that is hidden between multiple summations and also it is a scored as well.\nThat are going to be a little bit not so interesting, but we'll get comfortable with it because you have to get comfortable with these things as data scientists. What we want to train at Berkeley is data scientists who actually know all the mechanics and also all the cores of all the algorithms as opposed to just using the black boxes from all of the these machine learning frameworks, right? And you're going to gain that by going through all of these math heavy lectures.",
        "Retrieved Notes": "d we strongly recommend spending time covering the prerequisite material yourself. We will assume that you have prior knowledge of these requirements and that these resources are simply to refresh your memory of concepts that you have previously learned. Please reach out to staff if you have any questions or concerns about this.\n\n**Calculus**: In terms of calculus, you will need to know a few things, most of which are covered within the space of the first homework and lab. Specifically, you will need to kno\nw univariate calculus rules like: Taking derivatives of a univariate function (i.e. f(x), where x is the only variable); Derivative power rule; Knowing derivatives of mathematical functions like: sinx,cosx,logx,ex; Chain rule; Product rule (rarely); Derivatives of sums. We will expect some multivariate fluency like: Taking partial derivatives of a multivariate function (i.e. f(x,y,z), where x,y,z are all variables); Gradients (the concept).\n\n-   Khan Academy: [Derivatives, Definitions, and Basic Rules](http\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of ",
        "Retrieved Video": "+\n+\nPlan for Next Few Lectures: Modeling\n1078489\n?\nQuestion &\nData\nProblem\nAcquisition\nFormulation\nPrediction and\nExploratory\nInference\nData Analysis\nReports, Decisions,\nand Solutions\n(today)\nModeling I:\nModeling II:\nModeling III:\nIntro to Modeling, Simple\nDifferent models, loss\nMultiple Linear\nLinear Regression\nfunctions, linearization\nRegression\n3\nA\n\n+\nEvaluating Models\nWhat are some ways to determine if our model was a good fit to our data?\n1078489\n1. Visualize data, compute statistics:\nPlot original data.\nCompute column means, standard deviation.\nIf we want to fit a linear model, compute correlation r.\n11\n\n+\nEvaluating Models\nWhat are some ways to determine if our model was a good fit to our data?\n1078489\n1. Visualize data, compute statistics:\nPlot original data.\nCompute column means, standard deviation.\nIf we want to fit a linear model, compute correlation r.\n11\n------\n",
        "Question": "will we need to know how to make these derivations on exams or will we be fine just knowing what they spit out?",
        "Timestamp": 2216,
        "TA Response": "You'll need to be familiar with the steps shown in lecture today (eg calculating loss, minimizing loss). There will be more practice in discussion and homework.",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, when we say \"argmin,\" we are referring to finding the argument (or parameter value) that minimizes the loss function. In the context of the lecture, this means identifying the value of $\\theta$ that results in the lowest average loss, which is crucial for determining the best model parameters. The focus is on the parameter value that achieves this minimum, rather than the minimum value of the loss function itself.",
        "Recent Audio": "I plug it into this objective function that I had what? Minimum value that I'll get at the end, the minimum value that you'll get at the end is probably, or you can calculate it. That is going to be sample variance. But again, we care less about the minimum value of a function, but we care more about the argument. Every time that you heard that or we're calculating argument or we're calculating the argument that minimizes this and that you know that it is the estimation process and that's very important so. Going back to this example again, you saw that because we in our mind we were thinking that the loss function is MSC. If you go ahead and calculated that value is going to be 25 and we're good to go in that case. So now I want to go ahead and just evaluate what had happened and what is the impact of having this constant model versus a model that can be more complex and what's the shape of this loss function? Overall that we're dealing with. So the example that we're going to see in the demo is an example of a data set of dugong. So what we're trying to do is just to predict the age of dugong. In one case I have only the ages. I'm using the constant model in order to do the prediction of the age. In another example, I'm going to go ahead and use the lengths of dugongs in order to predict.",
        "Recent Video": "+\n[Data] Comparing Two Different Models, I\nSuppose we wanted to predict dugong ages.\n1078489\nA Dugong [image source)\nNot a Dugong, a Dewgong [image source]\nConstant Model\nSimple Linear Regression\ny = 00\ny =00+ 01x\nDemo\nData: Sample of ages.\nData: Sample of (length, age)s.\nD= {v/1, y2, . . . , Un}\nD={(\u00a31,1/1), (x2, 92),\n..., (In, yn) }\n35\n-----\n\n[Data] Comparing Two Different Models, I\nSuppose we wanted to predict dugong ages.\n1078489\nA Dugong [image source]\nNot a Dugong, a Dewgong [image source]\nConstant Model\nSimple Linear Regression\ny = 00\ny=00+ 01x\nDemo\nData: Sample of ages.\nData: Sample of (length, age)s.\nD = {31, 32 ,... , Un}\nD={(*1,9/1), (x2, 92),\n..., (In, yn) }\n35\n",
        "Retrieved Audio": "Find the argument that actually minimizes what minimizes objective function and the argument or the value that minimizes the objective function in our case is exactly Y bar, and that's what we care about. You could also argue that, OK, if I want to go ahead and just get this Tata zero that you had or this teta or teta 0 hat that you had and I plug it into this objective function that I had. What?\nOf the cost function that that parameter is giving us is that once you have the parameter, you have the model, you can draw it, you can do prediction and whatnot. You care less about what was the what was the, its performance on the original training data that you had. So that's why we care less about the value of the cost, but we care about the argument that gave us the minimum value for the cost function and that actually has a term. So we always say that.\nYou care less about what was the what was the its performance on the original training data that you had. So that's why we care less about the value of the cost, but we care about the argument that gave us the minimum value for the cost function and that actually has a term. So we always say that what we are trying to do in order to find out the Tetra 0 hat is that across all of the possible Tetra 0 values that we had. Remember I was saying that.",
        "Retrieved Notes": " purposes, we care less about the minimum value of cost, $R(\\hat{\\theta_0})$, and more about the *value of $\\theta$* that results in this lowest average loss. In other words, we concern ourselves with finding the best parameter value such that:\n$$\\hat{\\theta} = \\underset{\\theta}{\\operatorname{\\arg\\min}}\\:R(\\theta)$$\nThat is, we want to find the **arg**ument $\\theta$ that **min**imizes the cost function.\n### Comparing Two Different Models, Both Fit with MSE\nNow that we've explored the constant model with an \nient Descent \nAt this point, we've grown quite familiar with the process of choosing a model and a corresponding loss function and optimizing parameters by choosing the values of $\\theta$ that minimize the loss function. So far, we've optimized $\\theta$ by\n1. Using calculus to take the derivative of the loss function with respect to $\\theta$, setting it equal to 0, and solving for $\\theta$.\n2. Using the geometric argument of orthogonality to derive the OLS solution $\\hat{\\theta} = (\\mathbb{X}^T \\mathbb{X})^\nasoning as to why the mean is such a common summary statistic.\nOur optimal model parameter is the value of the parameter that minimizes the cost function. This minimum value of the cost function can be expressed:\n$$R(\\hat{\\theta_0}) = \\min_{\\theta_0} R(\\theta_0)$$\nTo restate the above in plain English: we are looking at the value of the cost function when it takes the best parameter as input. This optimal model parameter, $\\hat{\\theta_0}$, is the value of $\\theta_0$ that minimizes the cost $R$.\nFor modeling",
        "Retrieved Video": "Fit the Model: Three Approaches\n\u00c2(00) = - E (vi-0)2\n1078489\ni=1\nApproach 1\nIf you want to prove the general case for any data, you could\ndirectly minimize the objective. We can show that average loss is minimized by\n00 = mean(y) = }\nApproach 2\nIf you know your data D = {20, 21, 22, 29, 33}, you could modify the\nobjective by plugging in values first:\nR(0) = = (20 - 00)2 + (21 - 00)2 + (22 - 00)2 + (29 - 00)2 + (33 - 00)2)\n25\n------\n\nFit the Model: Three Approaches\n\u00c2(00) = - E (Vi-0)2\n1078489\ni=1\nApproach 1\nIf you want to prove the general case for any data, you could\ndirectly minimize the objective. We can show that average loss is minimized by\n\u00d4o = mean(y) = }\nApproach 2\nIf you know your data D = {20, 21, 22, 29, 33}, you could modify the\nobjective by plugging in values first:\nR(0) = = ((20-00)2+(21-0)2+(22-00)2+(29-00)2+(33-00)2)\n25\n------\n\nFit the Model: Three Approaches\n\u00c2(00) = - E (vi-0)2\n1078489\ni=1\nApproach 1\nIf you want to prove the general case for any data, you could\ndirectly minimize the objective. We can show that average loss is minimized by\n00 = mean(y) = }\nApproach 2\nIf you know your data D = {20, 21, 22, 29, 33}, you could modify the\nobjective by plugging in values first:\nR(0) = = (20 - 00)2 + (21 - 00)2 + (22 - 0)2 + (29 - 00)2 + (33 - 00)2)\n25\n------\n",
        "Question": "When we say argmin, are we finding arguments that minimize the loss function?",
        "Timestamp": 2397,
        "TA Response": "Yes, it's the model parameters(arguments) that minimize a function.",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "To determine if the second derivative of a cost function is positive, you need to evaluate the second derivative itself. If the second derivative is positive for all values of the parameter, it indicates that the function is convex, meaning it has a single minimum point. In the context of the lecture, the cost function is likely a quadratic function (as suggested by the use of mean squared error), which is inherently convex, and thus its second derivative is positive. This ensures that the function has a unique minimum, which is crucial for optimization methods like gradient descent.",
        "Recent Audio": "age. And then we're basically using the input, which is the length in order to predict the age of each one of those dugongs. So we're going to take a look at OK, as we simplified our assumptions and we went from simple linear regression to the constant model, did we sacrifice anything? And what's the shape of this cost function overall? So let's take a look. So here what I'm doing is that. I have this data set that I'm going to look, so the data set for the constant model, data set for the linear model. The data is also posted on Data hub, so the data is loaded. So what I'm doing next is that I'm going to plot this cost function for the case that I have a constant model on this specific data set. So what you're seeing here is that the cost function or this objective function is a function of PETA, right? You see that PETA lira is on the X axis. For different teta 0 values, I have different costs on the Y axis. This is a 2 dimensional lost landscape. So this is the first time you're hearing that term. You will keep hearing that term on and on and every time that we're trying to optimize a model. So how did I arrive at that? Basically I said that I'm going to just enumerate over all possible values of Petas, and I'm assuming that I'm just going to only look at the range that I think.",
        "Recent Video": "File Edit View Run Kernel Tabs Settings Help\nX\n+\nabo\nT\nO\nplt.style.use(\"default\") # Revert style to defoult mpl\nadjust_fontsize(size=16)\nWmatplotlib inline\ndef mse_constant (theta, data):\n*\nreturn mp.mean(np.array[[(y_obs - theta) \"* 2 for y_obs in data)), axis-0)\nthetas . np.limspace(-20, 42, 1000% I\n12_loss_thetas = mse_constant(thetas, data_constant)\nplt.plot (thetas, 12_loss_thetas)\nplt.ylabel(e\"MSE')\n# Optimal point\nthetahat . mp.mean(data_constant]\nplt.scatter([thetahat), [mse constant(thetahat, data_constant ) ) , se50, label = \"\"Sihat(\\theta)_05\")\nplt.legend()\nW plt.savefig('mse_constant_Loss.png\", bbox_inches = \"tight\");\nplt.show()\n1000\n800\nw 600\nSimple\nPython 3 (pykernel) | Idle Mem: 340.63 / 4096.00 MB\nMode: Edit\nLn B, Col 36\n\nFile Edit View Run Kemel Tabs Settings Help\nX\n+\na + x 00 . . ( > &Download\n0\nabo\nT\n[16]]\nplt.style.use[\"default\") # Revert style to default mpl\nadjust_fontsize(size=16)\nAmatplotlib inline\ndef mse_constant (theta, data):\n*\nreturn mp.mean(np.array[[(y_obs - theta) \". 2 for y_obs in data]), axis-0)\nthetas . mp.limspace(-20, 42, 1000)\n12_loss_thetas - Ese_constant(thetas, data_constant)\nplt.plot(thetas, 12_loss_thetas)\nplt.xlabel(r'$\\theta_05\"}\nplt.ylabel(e'MSE')\n# Getimol point\nthetahat . mp.mean(data_constant]\nplt.scatter([thetahat], [mse_constant (thetahat, data_constant]], $=50, label = r\"$\\hat (\\theta)_65\"}\n# plt.savefig('mse_constant_Loss.png\", bbox_inches = \"right\");\nplt.legend[)\nplt.show()\n1000\n800\nw 600\nSimple\n211 @\nPython 3 (pykernel) | Idle Mem: 340.63 / 4096.00 MB\nMode: Edit\nLn 9, Col 36\n",
        "Retrieved Audio": "From I, I from 1 up to north and then I have the derivative of Yi minus Theta 0 is square with respect to Theta zero. OK, so I'm going to show you one thing that I also showed you last time that if you think about why I minus Theta 0 as function FI would call it. What I'm doing here is that I want to now take the derivative of this function F to the power of 2.\nRemember from last time that the first step always is going to be that I can turn the derivative of a summation into a sum of derivatives, right? Because you will always be doing the derivative of a summation because all these are objective function is some summation of the error over the entire data set. So that's always going to be your first step. So I'm going to do that. I'm going to say that I'm taking out the coefficient 1 / n, I'm taking out the summation.\nTalk about that in the next few slides. So kind of showing you what I meant by our function or the cost function being a parameter of Theta. So what you're seeing here in two plots is that one plot is the optimum line with optimum set of parameters set to 0 head and Theta one hat and then the other one is a sub optimal example. In the sub optimal example, what you're seeing is that I have very large residuals if you take a look at the.",
        "Retrieved Notes": "ient Descent\nLooking at the function across this domain, it is clear that the function's minimum value occurs around $\\theta = 5.3$. Let's pretend for a moment that we *couldn't* see the full view of the cost function. How would we guess the value of $\\theta$ that minimizes the function? \nIt turns out that the first derivative of the function can give us a clue. In the plots below, the line indicates the value of the derivative of each value of $\\theta$. The derivative is negative where it is red and positi\nve where it is green.\nSay we make a guess for the minimizing value of $\\theta$. Remember that we read plots from left to right, and assume that our starting $\\theta$ value is to the left of the optimal $\\hat{\\theta}$. If the guess \"undershoots\" the true minimizing value \u00e2\u0080\u0093 our guess for $\\theta$ is lower than the value of the $\\hat{\\theta}$ that minimizes the function \u00e2\u0080\u0093 the derivative will be **negative**. This means that if we increase $\\theta$ (move further to the right), then we **can decrease** our\n0| = \\begin{cases} y_i - \\theta_0 \\quad \\text{ if } \\theta_0 \\le y_i \\\\ \\theta_0 - y_i \\quad \\text{if }\\theta_0 > y_i \\end{cases}$$\n- Taking derivatives:\n$$\\frac{d}{d\\theta_0} |y_i - \\theta_0| = \\begin{cases} \\frac{d}{d\\theta_0} (y_i - \\theta_0) = -1 \\quad \\text{if }\\theta_0 < y_i \\\\ \\frac{d}{d\\theta_0} (\\theta_0 - y_i) = 1 \\quad \\text{if }\\theta_0 > y_i \\end{cases}$$\n- This means that we obtain a different value for the derivative for data points where $\\theta_0 < y_i$ and where $\\theta_0 > y_i$. We can su",
        "Retrieved Video": "Fit the Model: Calculus for the General Case\n1. Differentiate with respect to 0g.\nd (=) (1-0)2)\n3. Solve for 60-\n1078489\ndoo\nd R(0) = 200 7\n{{ (y; - \u00e2?) = 0\ndo (yi -00)2\nDerivative of sun is\nsum of derivatives\n-\n= = > 2(y :- 80)(-1)\nChair rule\n36\n= = 2 E(ui -00)\nSimplify constaants\n2. Set equal to 0.\n2 = { is =n\n11\nend.\n0=25(vi-60)\n:= 1\n29\n-----\n\nFit the Model: Calculus for the General Case\n1. Differentiate with respect to 0g.\n3. Solve for 60-\n1078489\n=\n1\n=.= (3)-80)2\nDerivative of sun is\nsum of derivatives\n=\"\u2265 2(i-%)(-1)\nChair jule\n=2\u2103(Ji-80)\nSimplify const ants\n2. Set equal to 0.\n0=22\u2103(3-60)\n=2\n29\n\n+\n01\nFit the Model: Calculus for the General Case\nf = 2f df\n1. Differentiate with respect to 0g:\n3. Solve for 60-\n1078489\n-= - 15\nDerivative of sum is\nsum of derivatives\n2\ni= do.\n= [ 2 (y ;- 00) (-1)\nChain rule\n\ub958 \ud2b8 (g; - 80)\nSimplify constants\n2. Set equal to 0.\n- [ (y :- )=0\n26\n",
        "Question": "For the cost function how do u know that the second derivative is positive? ",
        "Timestamp": 2503,
        "TA Response": "Hi, we know if a function has a positive second derivative if it is convex/concave up (U-shpae).",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The derivative is considered undefined when \\( y_i = \\theta_0 \\) because the absolute value function creates a cusp at this point, leading to a discontinuity in the derivative. In this class, we disregard this case to simplify the analysis and focus on the cases where \\( y_i \\neq \\theta_0 \\), which are more straightforward to handle. If we didn't ignore this case, it would complicate the derivation process without significantly affecting the overall result, as the probability of \\( y_i \\) being exactly equal to \\( \\theta_0 \\) is typically negligible in practice.",
        "Recent Audio": "just conditioning over different Yi values? I would say that. What if I write this as 1 / n times the summation of I from 1 up to N? Of the cases that Yi is greater than Theta 0, the case that Yi is less than Theta zero and the case that Yi is equal to Theta 0. Right? In this case I can get rid of the the absolute value function. So I would say that this is undefined. The derivative is undefined. And we're not going to consider the case that Yi is exactly equal to tet to 0 for the purpose of data 100. For the other two cases when Yi is greater than tet to zero, I can just directly say that, oh, OK, my function now is going to be the derivative of Yi minus Theta 0, right, with respect to Theta 0, exactly because Yi is greater than Theta 0, so that whatever is inside the absolute value function is positive. So it will be just. Value in the other case that Yi is less than Theta 0. Whatever is inside the absolute value function is going to be negative. So the when I take the absolute value, I should multiply it with a -1. So what I'll do is I'll say that this in this case is equal to taking the derivative of. Theta 0 -, y I right. So this way it's kind of simple, it's kind of separate and I can move forward with taking the derivatives.",
        "Recent Video": "Fit the Model: Calculus\n1. Differentiate with respect to 00 -\nThe following derivation is beyond what\n1078489\nd\n20 12 5135-001\nR(00) = 100 n\nwe expect you to generate on your own.\ndeo\nBut you should understand it.\ni=1\n82\n7, diy1-80)\n\" WI \"W\"\n1\ndo.\n- Is\nI (8 .- y:) y; < 0.\nunder\nyisa\n-15\n47\n------\n\nFit the Model: Calculus\n1. Differentiate with respect to 00-\nThe following derivation is beyond what\n1078489\nd\nCon Elai -Bol\nR(00) = doon\nwe expect you to generate on your own.\ndeo\nBut you should understand it.\n= = = a00\nayi -Pol\nn\ni=1\n\" WI \"W\"\n1\ndo.\n-15\nI (8 .- y:) y; < 0.\n7\nundet\ny=0.\nyiso.\n-15\n47\n------\n",
        "Retrieved Audio": "Of the cases that Yi is greater than Theta 0, the case that Yi is less than Theta zero and the case that Yi is equal to Theta 0. Right? In this case I can get rid of the the absolute value function. So I would say that this is undefined. The derivative is undefined.\nOf the cases that Yi is greater than Theta 0, the case that Yi is less than Theta zero and the case that Yi is equal to Theta 0. Right? In this case I can get rid of the the absolute value function. So I would say that this is undefined. The derivative is undefined.\nAnd we're not going to consider the case that Yi is exactly equal to tet to 0 for the purpose of data 100. For the other two cases when Yi is greater than tet to zero, I can just directly say that, oh, OK, my function now is going to be the derivative of Yi minus Theta 0, right, with respect to Theta 0, exactly because Yi is greater than Theta 0, so that whatever is inside the absolute value function is positive. So it will be just.",
        "Retrieved Notes": "0| = \\begin{cases} y_i - \\theta_0 \\quad \\text{ if } \\theta_0 \\le y_i \\\\ \\theta_0 - y_i \\quad \\text{if }\\theta_0 > y_i \\end{cases}$$\n- Taking derivatives:\n$$\\frac{d}{d\\theta_0} |y_i - \\theta_0| = \\begin{cases} \\frac{d}{d\\theta_0} (y_i - \\theta_0) = -1 \\quad \\text{if }\\theta_0 < y_i \\\\ \\frac{d}{d\\theta_0} (\\theta_0 - y_i) = 1 \\quad \\text{if }\\theta_0 > y_i \\end{cases}$$\n- This means that we obtain a different value for the derivative for data points where $\\theta_0 < y_i$ and where $\\theta_0 > y_i$. We can su\ngnore this issue. It turns out that disregarding this case doesn't influence our final result.\n- To perform the derivative, consider two cases. When $\\theta_0$ is *less than or equal to* $y_i$, the term $y_i - \\theta_0$ will be positive and the absolute value has no impact. When $\\theta_0$ is *greater than* $y_i$, the term $y_i - \\theta_0$ will be negative. Applying the absolute value will convert this to a positive value, which we can express by saying $-(y_i - \\theta_0) = \\theta_0 - y_i$.\n$$|y_i - \\theta_\n) (-1) \\quad \\quad \\text{chain rule}\n\\\\ &= {\\frac{-2}{n}}\\sum^{n}_{i=1} (y_i - \\theta_0) \\quad \\quad \\text{simply constants}\n\\end{align}\n$$\n2. Set the derivative equation equal to 0:\n   $$\n   0 = {\\frac{-2}{n}}\\sum^{n}_{i=1} (y_i - \\hat{\\theta_0})\n   $$\n3. Solve for $\\hat{\\theta_0}$\n$$\n\\begin{align}\n0 &= {\\frac{-2}{n}}\\sum^{n}_{i=1} (y_i - \\hat{\\theta_0})\n\\\\ &= \\sum^{n}_{i=1} (y_i - \\hat{\\theta_0}) \\quad \\quad \\text{divide both sides by} \\frac{-2}{n}\n\\\\ &= \\left(\\sum^{n}_{i=1} y_i\\right) - \\left(\\sum^{n}_{i",
        "Retrieved Video": "Fit the Model: Calculus\n1. Differentiate with respect to 00 -\nThe following derivation is beyond what\n1078489\nd\n20 12 5135-001\nR(00) = 100 n\nwe expect you to generate on your own.\ndeo\nBut you should understand it.\ni=1\n82\n7, diy1-80)\n\" WI \"W\"\n1\ndo.\n- Is\nI (8 .- y:) y; < 0.\nunder\nyisa\n-15\n47\n------\n\nFit the Model: Calculus\n1. Differentiate with respect to 00-\nThe following derivation is beyond what\n1078489\nd\nCon Elai -Bol\nR(00) = doon\nwe expect you to generate on your own.\ndeo\nBut you should understand it.\n= = = a00\nayi -Pol\nn\ni=1\n\" WI \"W\"\n1\ndo.\n-15\nI (8 .- y:) y; < 0.\n7\nundet\ny=0.\nyiso.\n-15\n47\n------\n\n+\n2\nFit the Model: Calculus for the General Case\nT\n1. Differentiate with respect to 0g.\n3. Solve for 60-\n1078489\n= { (y ; - 0 ) = 0\nDerivative of sun is\nsum of derivatives\n=== 2(yi-80)(-1)\nChair rule\n== 2\u2211(yi-80)\nSimplify const ants\n2. Set equal to 0.\n0=2(yi-60)\n29\n------\n",
        "Question": "Why do we see the derivative of yi = theta zero as undefined (for the purpose of this class)? What would happen if we don't?",
        "Timestamp": 3497,
        "TA Response": "Hi, I believe you are referencing when we fit the model for a constant model. Using L1 loss, we end up taking the derivative of the absolute value of y1 - theta0. When the argument is zero, this is technically undefined, so we wouldn't be able to cacluate the derivative.",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, \\(\\theta_0\\) would be the median. The lecture notes indicate that the optimal value \\(\\hat{\\theta_0}\\) is the median of the observations \\(y\\), as it ensures an equal number of points on either side, which is a characteristic of the median. This is derived from setting the derivative of the objective function with respect to \\(\\theta_0\\) equal to zero and solving for \\(\\theta_0\\).",
        "Recent Audio": "the case that Tata 0 is less than Y I-1 case is that Tata 0 is greater than Yi. But what do I need to know do this time? I'm done with the derivative. I calculated derivative. What was the second step? Doing the actual estimation, setting it equal to 0. So that's what I'm going to do. So setting it equal to 0, I'm going to actually go ahead and say I'm going to just get rid of this coefficient right here because I don't care about that. I'll set the entire thing equal to 0. When I set it equal to 0, Tetra 0 will become equal to Theta 0 hat. So you'll see that now my summation is equal to 10 to 0 hat being less than Yi. And is contributing. That's not a nice -1. Is contributing -1 And then for the case that Theta 0 hat is greater than Y, is contributing +1 setting it equal to 0. OK, So what I'm going to do is I'm going to actually move the this negative term to the other side, and then I'll try to take a look at it one more time when it's out there. So summation of Tetris, 0 hat greater than Yi. This is this term that I move. This is equal to. Moving this other term to the other side is going to be equal to the summation of the zero hat less than Yi, and this is the +1 positive one. What does this tell you?",
        "Recent Video": "Fit the Model: Calculus\nT\n1. Differentiate with respect to 00 -\n2. Set equal to 0\n1078489\nd\nd 1\n100= 135-001\ndoo\nR(%) =\ndoo n\n[ (-1+] (+) =0\n=12\nvi -00 if 00 \u2264 vi\n[yi - 001 = 100 -vi if 00 > Vi\nif100 < yi\n100 > yi\n-\nSTE\n50\n------\n\nFit the Model: Calculus\n1. Differentiate with respect to 00-\n2. Set equal to 0\n1078489\nd\nR(00) = d00 n Dlui - Pol\nd 1\ndeo\n{ (-1)+> (+) =0\nao li- Pol\n= = = a00\nn\ni=1\n5\nJyi -00 if 00 \u2264 yi\n3: - Bol = 100 - yi\nif 00 > yi\n50\nif 00 < yi\nif 00 > Vi\n-\n={>(-1)+ >(+)\n50\n------\n",
        "Retrieved Audio": "This function right here with respect to Theta 0.\nThis function right here with respect to Theta 0.\nAnd what is the derivative of F with respect to Theta 0?",
        "Retrieved Notes": "theta_0$ must be equal to the number of observations less than $\\theta_0$; there must be an equal number of points on the left and right sides of the equation. This is the definition of median, so our optimal value is\n$$ \\hat{\\theta_0} = median(y) $$\n## Summary: Loss Optimization, Calculus, and Critical Points\nFirst, define the **objective function** as average loss.\n- Plug in L1 or L2 loss.\n- Plug in the model so that the resulting expression is a function of $\\theta$.\nThen, find the minimum of the objecti\nc{4}{10}$ of the flips were heads, so we might guess $\\hat \\theta = 0.4$. In the next section, we will mathematically prove why this is the best possible estimate.\n### Likelihood of Data\nLet's call the result of the coin flip a random variable $Y$. This is a Bernoulli random variable with two outcomes. $Y$ has the following distribution: \n$$P(Y = y) = \\begin{cases}\n        p, \\text{if }  y=1\\\\\n        1 - p, \\text{if }  y=0\n    \\end{cases} $$\n$p$ is unknown to us. But we can find the $p$ that makes the data\ne effect on the response. In the extreme case, if the true parameter $\\theta_i$ is 0, then the feature $x_i$ has **no effect** on $Y(x)$. \nIf the true parameter $\\theta_i$ for a particular feature is 0, this tells us something pretty significant about the world: there is no underlying relationship between $x_i$ and $Y(x)$! But how can we test if a parameter is actually 0? As a baseline, we go through our usual process of drawing a sample, using this data to fit a model, and computing an estimate $\\hat{\\thet",
        "Retrieved Video": "aset III: theta_@: 3.00, theta_1: 0.50\nDataset IV: theta_0: 3.00, theta_1: 0.50\nLooks like all four datasets have exactly the same fit.\nIn fact, all four of them have the same 2, \u00a7. ( ,, o ,, correlation r, and RMSE! If we only look at these statistics, we will probably be inclined to say that\nthese datasets are similar.\n\n: theta_0: 3.00, theta_1: 0.50\nDataset IV: theta 0: 3.00, theta_1: 0.50\nLooks like all four datasets have exactly the same fit.\nIn fact, all four of them have the same 2, 9. o2, @ ,, correlation r, and RMSE! If we only look at these statistics, we will probably be inclined to say that\nthese datasets are similar.\n\nInterpreting 00 = y\nThis is the optimal parameter for constant model + MSE.\n1078489\nIt holds true regardless of what data sample you have.\n. .\nIt provides some formal reasoning as to why the mean is such a common summary\nstatistic.\nR(0) = R(3) = == (vi-1) {0}\nn\nFun fact:\nThe minimum MSE is the sample variance.\ni=1\n32\n------\n",
        "Question": "would theta 0 just be the median?",
        "Timestamp": 3761,
        "TA Response": "Yes, if we used L1 loss, ^theta0 would just be the median.",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "It seems like there were active polls during the lecture, as indicated by the video context showing poll results. If you missed the synchronous participation polls, you might still have the option to complete asynchronous participation by answering all participation poll questions provided after the lecture. Check if there is a link or other instructions provided by your instructor for asynchronous participation.",
        "Recent Audio": "defining them in a way that the function should always be convex and differentiable. So that we can actually take derivative and do the optimization and so on and so forth, and then find the optimum minimum point of those functions. So in this case, what I'm going to do is just kind of visually. Compare what happened with MSD, what happened with Mae with the constant model. Because we use the single model, the only thing that was different just was just the loss function that we pick. Remember the last landscape for the MSE that was this nice curve, but the optimum point being 25. And then you saw that with the Mae we had this other function that was piecewise linear and the optimum was at 22. So that was kind of the difference between when we were doing the MSE versus Mae and we arrived at different head to 0 hat values. One time the teta 0 hat was mean, one time the teta 0 hat was equal to median. The other considerations was that one function was as smooth, it was differentiable, this other function was not as smooth, and then it was piecewise linear. And we're going to use some of these techniques later on in order to see how we can deal with all of these piecewise linear non differentiable functions that we're going to have in different machine learning models. One more consideration, remember that we had a two data sets in Anscombe Quartet data set that had.",
        "Recent Video": "+\nMSE and MAE: Comparing Sensitivity to (\nMSE (Mean Squared Loss)\nMAE (Mean Absolute Loss) 1078489\nMinimized with sample mean:\nMinimized with sample median:\n\u00d4o = mean(y) = \u00a7 \u00a7o = median(y)\ndata = {20, 21, 22, 29, 33, 1033}\nDemo\n58\n\n+ +\nMSE and MAE: Comparing Sensitivity to (\nMSE (Mean Squared Loss)\nMAE (Mean Absolute Loss) 1078489\nMinimized with sample mean:\nMinimized with sample median:\n\u00d4o = mean(y) = \u00a7 \u00a7o = median(y)\ndata = {20, 21, 22, 29, 33, 1033)\n180000\n350\n170000\n300\n160000\n90 = 193.0\n\u20b9 250 00 = 25.5\nMSE\n150000\n200\n140000\nDemo\n100\n200\n300\n0\n100\n200\n300\n------\n",
        "Retrieved Audio": "All four of these data sets, but what we'll see is that it's not accurate and we could have told, we could have tell, we could have kind of identified that it's problematic if we have visualized the data ahead of time. So I'm going to go to this demo actually and.\nSo remember when I was?\nI have.",
        "Retrieved Notes": "oters tell the truth? How can we predict turnout? -- still remain, but the *Gallup Poll* uses several tactics to mitigate them. Within their sampling frame of \"civilian, non-institutionalized population\u00e2\u0080\u009d of adults in telephone households in continental U.S., they use random digit dialing to include both listed/unlisted phone numbers and to avoid selection bias. Additionally, they use a within-household selection process to randomly select households with one or more adults. If no one answers, re-call mul\ny had a dismal response rate (about 24\\%); who knows how the other non-respondents would have polled? The *Digest* folded just 18 months after this disaster.\nAt the same time, George Gallup, a rising statistician, also made predictions about the 1936 elections. Despite having a smaller sample size of \u00e2\u0080\u009conly\u00e2\u0080\u009d 50,000 (this is still more than necessary; more when we cover the Central Limit Theorem), his estimate that 56\\% of voters would choose Roosevelt was much closer to the actual result (61\\%). Gallup\nuestions are launched; or (2) asynchronously using a link provided after lecture.\n\n-   Lecture participation is graded using poll responses on a 0/1 basis.\n    -   Synchronous Participation: complete at least one participation poll question during the live lecture timeslot (11:00am-12:30pm, Tuesdays and Thursdays). As long as you submit a response to at least one poll question in this timeframe, you will receive synchronous attendance credit.\n    -   Asynchronous Participation: complete all participation po",
        "Retrieved Video": "2\nActive poll\n. Synchronous\n365 &\nHere're the boba sales from 5 randomly selected previous days :(20, 21, 22, 29,\n33}. How many drinks will you sell tomorrow?\n25\n86%\nSomething else\n7%\nJoin at\n22\nslido.com\n6%\n#1078 489\n100\n1%\n0\n0%\n------\n\nActive poll\n\u00b7 Synchronous\nT\nHere're the boba sales from 5 randomly selected previous days :{20, 21, 22, 29,\n33}. How many drinks will you sell tomorrow?\n. 0\n\u00b7 25 \u20a3\n. 22\nJoin at\nslido.com\n\u00b7 100\n#1078 489\n\u00b7 Something else\n\n+\n2\nActive poll\n. Synchronous\nT\n169 8\nHere're the boba sales from 5 randomly selected previous days :(20, 21, 22, 29,\n33}. How many drinks will you sell tomorrow?\n25\n80%\n22\n11%\nJoin at\nSomething else\nslido.com\n6%\n#1078 489\n0\n29\n100\n1%\n",
        "Question": "All polls done already? I couldn\u2019t answer the first two due to not being able to log in my berkeley email.",
        "Timestamp": 4020,
        "TA Response": "We won't have any more polls, but you can complete them all asynchronously for attendance credit.",
        "Lecture": 11
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, there are statistical tests that can help quantify patterns in residual plots. One common method is the Breusch-Pagan test, which tests for heteroscedasticity by examining whether the variance of the residuals is dependent on the independent variables. Another approach is the Durbin-Watson test, which checks for autocorrelation in the residuals. These tests provide a more objective measure of whether patterns exist in the residuals beyond visual inspection.",
        "Recent Audio": "and taking a look at how it looks like this is the original data, original line, original residual plot. If I go ahead and linearize it, you would see that now I have log of Z, log of H or log of Y or I'm calling it AZ which is which is showing a linear pattern with respect to the input. So now the line. Through the data points a little bit more accurately. And if I go ahead and take a look at the residual plot, not the residual plot doesn't show any pattern, which technically means that log of Y is a better transformation to use with respect to the input X that I had and now linear regression model is going to be more appropriate. Otherwise, if your residual plot shows the pattern, that means that there is some pattern in the data that you haven't captured, so you have not done. Good enough of a job. So what does this mean? This means that OK, that was my data set, the data set of lengths with respect to age. I was not happy with just using this because I was seeing that the data set kind of curves upward. I did the transformation. I took the log of the age. So now I have log of the age, which I'm calling it as being equal to Z. And now the data is linear. I fit a linear regression line, so I have. Lengths with respect to log of H that I'm predicting.",
        "Recent Video": "+\n+\n0 D\nFit a Curve using Least Squares Regression\n1078489\nz = log (y) = 0\nAge by Length\nLog(Age) by Length\n30\n3\naby\n,20\n-\n. .\n10\n...\n0\n1.8\n20\n22 24 2.6\n2.0\n2.6\nLength\n1.8\n22\n2.4\nLength\n66\n------\n\n+\n+\nFit a Curve using Least Squares Regression\n1078489\nz = log (y)\nAge by Length\nLog(Age) by Length\nD\n30\n3\n,20\n10\n(Log(Age)\n0\n...\n0\n1.8\n2.0\n22 24 2.6\n1.8\n2.0\n2.2 2\n2.6\nLength\nLength\n66\n",
        "Retrieved Audio": "So that the remaining residual is basically noise in measurement kind of. And that's why we want to make sure that the residual plot is kind of uniformly distributed across X and also across Y and there is no kind of pattern that we can see in the output. So I'm going to start by talking about a specific data set. I'm going to show you the demo. This is specific data set is called anscom Quartet.\nAnd that's why kind of not the X axis, but it's basically also showing that same pattern that there is vertical line, vertical line pattern for data set for that we want. So that's exactly the reason that we go through all of these processes. We calculate the statistics, we visualize beforehand, we calculate rudeness squared errors. And then after we're done, we will actually go ahead and take a look at the residual plus just to make sure that is there any additional pattern that you should.\nThrough the data points a little bit more accurately. And if I go ahead and take a look at the residual plot, not the residual plot doesn't show any pattern, which technically means that log of Y is a better transformation to use with respect to the input X that I had and now linear regression model is going to be more appropriate. Otherwise, if your residual plot shows the pattern, that means that there is some pattern in the data that you haven't captured, so you have not done.",
        "Retrieved Notes": "ttps://inferentialthinking.com/chapters/15/5/Visual_Diagnostics.html?highlight=heteroscedasticity#detecting-heteroscedasticity): a good regression fit should display no clear pattern in its plot of residuals. The residual plots for Anscombe's quartet are displayed below. Note how only the first plot shows no clear pattern to the magnitude of residuals. This is an indication that SLR is not the best choice of model for the remaining three sets of points.\n<!-- <img src=\"images/residual.png\" alt='residual' wid\nture to understand the behavior of residuals. When working with several features in multiple linear regression, it no longer makes sense to consider a single feature in our residual plots. Instead, multiple linear regression is evaluated by making plots of the residuals against the predicted values. As was the case with SLR, a multiple linear model performs well if its residual plot shows no patterns.\n<div align=\"middle\">\n  <table style=\"width:100%\">\n    <tr align=\"center\">\n      <td>\n        <img src=\"imag\nalizations difficult to interpret. \nWe have a few methods to help reduce overplotting:\n* Decreasing the size of the scatter point markers can improve readability. We do this by setting a new value to the size parameter, `s`, of `plt.scatter` or `sns.scatterplot`.\n* **Jittering** is the process of adding a small amount of random noise to all x and y values to slightly shift the position of each datapoint. By randomly shifting all the data by some small distance, we can discern individual points more clearly ",
        "Retrieved Video": "w()\nChecking the residuals will also shed light on the differences among these four datasets.\nIf you remember from Data 8, \"the residual plot of a good regression shows no pattern\".\n\nWell . Japyterlab\nAnscombe's quartet: Residuals\nIdeal model evaluation steps, in order:\n1078489\n1. Visualize original data, Compute Statistics\n2. Performance Metrics\nFor our simple linear least square model, use RMSE (we'll see\nmore metrics later)\n3. Residual Visualization\nFrom Data 8:\n-15\nDemo\nThe residual\nplot of a good\nregression\nshows no\npattern.\n135\n------\n\nWell . Japyterlab\nAnscombe's quartet: Residuals\nIdeal model evaluation steps, in order:\n1078489\n1. Visualize original data, Compute Statistics\n2. Performance Metrics\nFor our simple linear least square model, use RMSE (we'll see\nmore metrics later)\n3. Residual Visualization\nFrom Data 8:\n-15\nDemo\nThe residual\n4\nplot of a good\nregression\nshows no\n41\npattern.\n-1.6\nT\n------\n",
        "Question": "is there any way to quantify patterns in the residual plot instead of relying on manual inspection? ",
        "Timestamp": 4435,
        "TA Response": "Hi, I believe there are some automated ways, but those are out of scope for this class.",
        "Lecture": 11
    }
]