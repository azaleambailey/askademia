[
    {
        "Conversation History": "\t",
        "GPT Response": "Adding a column of 1s to the design matrix allows us to incorporate the intercept term, \\(\\theta_0\\), into the linear regression model. This column of 1s, when multiplied by \\(\\theta_0\\), effectively adds the intercept to the model equation, \\(\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\theta_3x_3\\). By doing this, we can treat the intercept as just another parameter in the parameter vector, simplifying the matrix operations needed for fitting the model. This approach also helps maintain consistency in the matrix dimensions when performing linear algebra operations.",
        "Recent Audio": "choose our loss. This is measuring how good our predictions are relative to our data. What's the loss? We all love the squared loss. Why do we like it so much? Measuring difference. Square difference is nice. It's nice. And what does the square look like? A parabola. We'll talk more about why we like parabolas later, but this is a loss. We use simple linear regression. Regression use squared loss. We also sometimes call this least squares. Regression because we're trying to minimize the square of our air. OK, so this was the error on all my data I averaged. It's just the mean squared error, the mean of the square of the errors. And then we did linear algebra again, we wrote this. Why does that work? OK, let's break it down. Y -, Y hat that's just a difference between these two vectors. This one here. I wish I didn't erase her. That's difference between this vector here. And this vector here my predictions difference. What's the difference between 2 vectors? A vector? How big is that vector? What's the dimensions? What's the dimension of this resulting vector? We'll call it R. End by 1. Same to mention this. This is one by N 1 by N OK this here says take the sum of the squares of the value. This is the. Actually this is the says take the square root of the sum of the value. So the like R 1 ^2 + r two squared. This is the Euclidean norm.",
        "Recent Video": "Today's Goal: Ordinary Least Squares\nV\nFor each of our n data points:\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\ny = 01x1 + 02x2 + . . . + Opp\nRegression\nThe Data\n2. Choose a loss\nY = X0\nfunction\np\np\nX\n10\nn\n3. Fit the model\nCovariate Matrix\nResponse\nn x\n(features)\n(label)\nDepends on 0\n4. Evaluate model\nn\n1\nperformance\nR(0) = >(yi -Ii)2\n= - (||Y - \u00dd|2)2\ni=1\nn\n7\nCc\n2024-10-10 11:18:17\n\nToday's Goal: Ordinary Least Squares\nV\nFor each of our n data points:\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\ny = 01x1 + +2x2 + . . . + Opp\nRegression\nThe Data\nY = X0\n2. Choose a loss\np\nfunction\np\nA\nn\nX\n3. Fit the model\nCovariate Matrix\nResponse\nn\nX\n(features)\n(label)\nnitri\nDepends on 0\n4. Evaluate model\nn\nperformance\nR(0) = C(yi -yi)2\n1\n=\n-( Y - \u0176|2)2\ni=1\nn\n7\nCc\n2024-10-10 11:18:32\n",
        "Retrieved Audio": "I'm going to say that this data 0 is hard to keep track of, so I'm going to start with Theta 1. So I go from 1:00 to P so we have just P columns. And if I wanted a Theta zero, I'll just add an extra ones column at the beginning here. See what that does. Production ones column Theta one becomes my new Theta 0, right?\nAnd this is what we said. There were P columns. We'll talk more about our PS in a moment. And then our model was a constant Theta 0 plus all these extra Theta coefficients for each of our features.\nAnd I can add my columns of ones to encode the intercept if I want it. And this is desired. If I don't have to have an intercept, I can say, you know what, I don't want this. I'm just going to start with my actual data. I can add an extra comma once modeling choice. I make all right questions with the setup questions about the setup for this model.",
        "Retrieved Notes": "tor. When the dot product is computed, this 1 will be multiplied with $\\theta_0$ to give the intercept of the regression model. We call this 1 entry the **intercept** or **bias** term.\nGiven that we have three features here, we can express this model as:\n$$\\hat{y} = \\theta_0\\:+\\:\\theta_1x_{1}\\:+\\:\\theta_2 x_{2}\\:+\\:\\theta_3 x_{3}$$\nOur features are represented by $x_1$ (`FG`), $x_2$ (`AST`), and $x_3$ (`3PA`) with each having correpsonding parameters, $\\theta_1$, $\\theta_2$, and  $\\theta_3$.\nIn statistics, \nhot encoding, keep in mind that any set of one-hot encoded columns will always sum to a column of all ones, representing the bias column. More formally, the bias column is a linear combination of the OHE columns.\n<center><img src=\"images/bias.png\" alt='bias' width='600'></center>\nWe must be careful not to include this bias column in our design matrix. Otherwise, there will be linear dependence in the model, meaning $\\mathbb{X}^{\\top}\\mathbb{X}$ would no longer be invertible, and our OLS estimate $\\hat{\\thet\ne we are now dealing with many parameter values, we've collected them all into a **parameter vector** with dimensions $(p+1) \\times 1$ to keep things tidy. Remember that $p$ represents the number of features we have (in this case, 3).\n$$\\theta = \\begin{bmatrix}\n           \\theta_{0} \\\\\n           \\theta_{1} \\\\\n           \\vdots \\\\\n           \\theta_{p}\n         \\end{bmatrix}$$\nWe are working with two vectors here: a row vector representing the observed data, and a column vector containing the model paramete",
        "Retrieved Video": "ix\nand add a column of 1s to encode the intercept (if desired).\n5\n2024-10-10 11:14:56\n\nToday's Goal: Ordinary Least Squares\nFor each of our n2 data points:\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\ny = 00 + 01x1 + +2x2 + ... + Opp\nRegression\nThe Data\n2. Choose a loss\np+1\nY\n= X0\nfunction\np+1\nn\nX\nY obserck\nA\nCovariate Matrix\nResponse\nn\n3. Fit the model\nX\n(features)\n(label)\n4. Evaluate model\nperformance\n4\nCc\n2024-10-10 11:14:51\n\nToday's Goal: Ordinary Least Squares\nFor each of our n data points:\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\ny = 00 + 01x1 + 02x2 + .. . + Opp\nRegression\nThe Data\nY = X0\n2. Choose a loss\np+1 1\nfunction\np+1\nX\n10\nn\nY\nX\n3. Fit the model\nCovariate Matrix\nResponse\nn\n(features)\n(label)\n4. Evaluate model\nperformance\n4\nCc\n2024-10-10 11:11:24\n",
        "Question": "why are we able to add a column of 1 instead of theta 0 ",
        "Timestamp": 497,
        "TA Response": "Prof. Gonzalez just wants to have our X design matrix be n x p so that the dimensions are simpler. The first way we looked at this approach was to say we have p features and then this additional column of ones to add an intercept term, here Prof. Gonzalez is just reframing this as saying we have p - 1 features and a bias column of all ones leaving us with an n x p matrix",
        "Lecture": 13
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The resulting vector, referred to as \\( R \\), is an \\( N \\)-dimensional vector, where \\( N \\) represents the number of observations or data points. This vector is described as being \"tall and skinny,\" indicating it has dimensions \\( N \\times 1 \\).",
        "Recent Audio": "vector? What's the dimensions? What's the dimension of this resulting vector? We'll call it R. End by 1. Same to mention this. This is one by N 1 by N OK this here says take the sum of the squares of the value. This is the. Actually this is the says take the square root of the sum of the value. So the like R 1 ^2 + r two squared. This is the Euclidean norm. This right here is our standard. Pythagorean theorem right? Or generalized to many dimensions? How many dimensions is this? And interesting, right? So this vector here is an N dimensional vector. This difference vector R is tall and skinny like that. I'm going to take the sum of the squares of these things and I'll take the square root, and then guess what I'll do? I'll immediately square it again. So what is that? The square of the square root is. The thing. So this right here is the sum of the squares, also known as that right? Everyone with me so far, we got the model, we got the loss. There's questions on Zoom and I'm not good at Zoom. So if there are questions on Zoom and I need to know it or people let me know, I apologize you maybe ask the questions on on slide. Oh. Slider is a good place to check ask questions OK? So now we have our model, we have our loss, and then in last lecture we derive this amazing equation.",
        "Recent Video": "Recall: Ordinary Least Squares\nV\nY = X0\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\nRegression\nV\nL2 Loss\n2. Choose a loss\nR(0) =- (|Y- Y|2)2\n1\nfunction\nMean Squared Error\n(MSE)\nn\nMinimize\n3. Fit the model\nThe Normal Equation:\naverage loss\nwith calculus, geometry,\nXTX6 = XTY\n4. Evaluate model\nVisualize,\nperformance\nRoot MSE\nMultiple R2\nSee last lecture for the derivation of the\nNormal Equation using geometry.\n8\nCc\n2024-10-10 11:19:39\n\nRecall: Ordinary Least Squares\nV\nY = X0\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\nRegression\nV\n2. Choose a loss\nL2 Loss\nMean Squared Error\nR(0)\n=\n-(\\-}||2)2\n1\nfunction\n(MSE)\nn\nMinimize\n3. Fit the model\nThe Normal Equation:\naverage loss\nwith calculus, geometry,\nXTXO = XTY\n4. Evaluate model\nVisualize,\nperformance\nRoot MSE\nMultiple R2\nSee last lecture for the derivation of the\nNormal Equation using geometry.\n8\nCc\n2024-10-10 11:19:44\n",
        "Retrieved Audio": "And this vector here my predictions difference. What's the difference between 2 vectors? A vector? How big is that vector? What's the dimensions? What's the dimension of this resulting vector? We'll call it R.\nAnd interesting, right? So this vector here is an N dimensional vector. This difference vector R is tall and skinny like that.\nOur matrix P + 1 columns and we multiply that by this vector. It's a skinny vector. This length is 1, and this length is what?",
        "Retrieved Notes": "orm\nWhen working with vectors, this idea of \"distance\" or the vector's **size/length** is represented by the **norm**. More precisely, the distance between two vectors $\\vec{a}$ and $\\vec{b}$ can be expressed as:\n$$||\\vec{a} - \\vec{b}||_2 = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \\ldots + (a_n - b_n)^2} = \\sqrt{\\sum_{i=1}^n (a_i - b_i)^2}$$\nThe double bars are mathematical notation for the norm. The subscript 2 indicates that we are computing the L2, or squared norm.\nThe two norms we need to know for Data 100\ne we are now dealing with many parameter values, we've collected them all into a **parameter vector** with dimensions $(p+1) \\times 1$ to keep things tidy. Remember that $p$ represents the number of features we have (in this case, 3).\n$$\\theta = \\begin{bmatrix}\n           \\theta_{0} \\\\\n           \\theta_{1} \\\\\n           \\vdots \\\\\n           \\theta_{p}\n         \\end{bmatrix}$$\nWe are working with two vectors here: a row vector representing the observed data, and a column vector containing the model paramete\nmages/dataset_dims.png\" width=\"600vw\"></center>\n<br>\nWhat about Dataset 4 below? \n<center><img src = \"images/dataset4.png\" width=\"400vw\"></center>\nIt may be tempting to say that it has 4 dimensions, but the `Weight (lbs)` column is actually just a linear transformation of the `Weight (kg)` column. Thus, no new information is captured, and the matrix of our dataset has a (column) rank of 3! Therefore, despite having 4 columns, we still say that this data is 3-dimensional. \nPlotting the weight columns togethe",
        "Retrieved Video": "ix\nand add a column of 1s to encode the intercept (if desired).\n5\n2024-10-10 11:14:56\n\nRecall: Ordinary Least Squares\nV\nY = X0\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\nRegression\nV\nL2 Loss\n2. Choose a loss\nR(0) =- (|Y- Y|2)2\n1\nfunction\nMean Squared Error\n(MSE)\nn\nMinimize\n3. Fit the model\nThe Normal Equation:\naverage loss\nwith calculus, geometry,\nXTX6 = XTY\n4. Evaluate model\nVisualize,\nperformance\nRoot MSE\nMultiple R2\nSee last lecture for the derivation of the\nNormal Equation using geometry.\n8\nCc\n2024-10-10 11:19:39\n\nRecall: Ordinary Least Squares\nV\nY = X0\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\nRegression\nV\n2. Choose a loss\nL2 Loss\nMean Squared Error\nR(0)\n=\n-(\\-}||2)2\n1\nfunction\n(MSE)\nn\nMinimize\n3. Fit the model\nThe Normal Equation:\naverage loss\nwith calculus, geometry,\nXTXO = XTY\n4. Evaluate model\nVisualize,\nperformance\nRoot MSE\nMultiple R2\nSee last lecture for the derivation of the\nNormal Equation using geometry.\n8\nCc\n2024-10-10 11:19:44\n",
        "Question": "what is the resulting vector dimensions?",
        "Timestamp": 588,
        "TA Response": "Our matrix X is n x p dimensions and multiplied by our theta vector p x 1, our resulting vector (y hat) is n x 1. Intuitively we have n data points, so we need n-number of y hat predictions to correspond with it.",
        "Lecture": 13
    },
    {
        "Conversation History": "\t",
        "GPT Response": "No, you do not lose a feature by changing the first column into intercepts. Instead, you add an additional column of ones to the feature matrix to account for the intercept term. This column does not replace any existing feature; it simply augments the matrix to allow for the intercept in the regression model. The original features remain intact, and the intercept is handled separately in the model.",
        "Recent Audio": "this for a minute. We have millions of records and maybe 3 or 4 dimensions. These things are really tall and skinny, right? So I'm going to have this like 3 by 3 little matrix. That captures everything I need to know about my exes. It's kind of cool, right? OK. So look at this side X transpose Y. Here's our data again, flipped on its side. Where the ones go, I had ones here I had the ones. Where would they be? Right here, right? There's no ones right. And multiplying. I have other features here. I'm going to multiply these features times Y That's kind of weird. That's what it says to do. I get this. P by one thing, so P by 1. So you get this other little thing here. Bring it up a little bit. This is pretty cool, right? What is that? Have a matrix. It's called M because matrix M there we go, and maybe another matrix A and I have this thing Theta. There's a vector, but it's a random variable. So I have sort of like, you know, M times Theta equals a. If this was just basic object, what would you do? You divide by M, divide by M and you go, well, beta equals a / m That's what we did right here. We did the linear algebra version of that, which is. We take the inverse of this matrix right here. That we solved for Theta. So if you bought that, this was true.",
        "Recent Video": "Understanding The Solution Matrices\nThe Normal Equation:\nXTX\u00d4 = X\u2122\nJoey Gonzalez (he, h ...\nIn most settings,\nn\n1\n1\n# observations\n# features\np\nXT\n=\nn\nP\nX\nY\nStatisticians often use p (parameters) and\nComputer Scientists often use d (dimensions).\nThe Data\np\n1\n1\np\n1\np\nXTX\nA\n=\nXTY\nn\nX\nY\nSolve for ) :\nCovariate Matrix\nResponse\n1\np\n7\n(features)\n(label)\n= p\n(X\u2122X)-1\nXTY\n9\n2024-10-10 11:21:53\n\nUnderstanding The Solution Matrices\nThe Normal Equation:\nXTX\u00d4 - XT\nJoey Gonzalez (he, h ...\nIn most settings,\nn\n1\n1\n# observations\n# features\np\nX\u2122\n=\nXT\nn\np\nX\nY\nStatisticians often use p (parameters) and\nComputer Scientists often use d (dimensions).\nThe Data\np\n1\n1\np\n1\np\nX\u2122X\nA\n=\nXTY\nM\na = a\nn\nX\nY\nSolve for @ :\nCovariate Matrix\n1\np\n1\nResponse\n(features)\n(label)\n= p\n(X\u2122X)-1\nXTY\n9\n2024-10-10 11:22:46\n",
        "Retrieved Audio": "And I can add my columns of ones to encode the intercept if I want it. And this is desired. If I don't have to have an intercept, I can say, you know what, I don't want this. I'm just going to start with my actual data. I can add an extra comma once modeling choice. I make all right questions with the setup questions about the setup for this model.\nWe have this intercept and there we go 1-2 up to P coefficients.\nWe call this our covariant matrix. This is our features we're trying to predict. Why? So in our label and our data, we have this extra column Y. Those are things we're trying to predict. We're trying to predict star ratings, income, something.",
        "Retrieved Notes": "tor. When the dot product is computed, this 1 will be multiplied with $\\theta_0$ to give the intercept of the regression model. We call this 1 entry the **intercept** or **bias** term.\nGiven that we have three features here, we can express this model as:\n$$\\hat{y} = \\theta_0\\:+\\:\\theta_1x_{1}\\:+\\:\\theta_2 x_{2}\\:+\\:\\theta_3 x_{3}$$\nOur features are represented by $x_1$ (`FG`), $x_2$ (`AST`), and $x_3$ (`3PA`) with each having correpsonding parameters, $\\theta_1$, $\\theta_2$, and  $\\theta_3$.\nIn statistics, \n\\hat{\\mathbb{Y}} = \\theta_0 + \\theta_1 \\phi_1 + \\theta_2 \\phi_2 + \\ldots + \\theta_p \\phi_p$$\nRecall that we represent our features with $\\phi_i$ to reflect the fact that we have performed feature engineering. \nPreviously, we restricted model complexity by limiting the total number of features present in the model. We only included a limited number of polynomial features at a time; all other polynomials were excluded from the model.\nWhat if, instead of fully removing particular features, we kept all features\n\u0093 ideally, more \"important\" features will receive greater weighting. \nNotice that the intercept term, $\\theta_0$, is excluded from this constraint. **We typically do not regularize the intercept term**.\nNow, let's think back to gradient descent and visualize the loss surface as a contour map. As a refresher, a loss surface means that each point represents the model's loss for a particular combination of $\\theta_1$, $\\theta_2$. Let's say our goal is to find the combination of parameters that gives us the lo",
        "Retrieved Video": "ix\nand add a column of 1s to encode the intercept (if desired).\n5\n2024-10-10 11:14:56\n\nToday's Goal: Ordinary Least Squares\nFor each of our n data points:\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\ny = 00 + 01x1 + 02x2 + .. . + OpXp\nRegression\nThe Data\n2. Choose a loss\nfunction\np+1\n12\n- 8\np+1\nn\nX\nY\nCovariate Matrix\nResponse\nn\nLX\n3. Fit the model\n(features)\n(label)\n4. Evaluate model\nperformance\n4\nCc\n2024-10-10 11:13:39\n\nToday's Goal: Ordinary Least Squares\nFor each of our 22 data points:\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\ny = 00 + 01x1 + +2x2 + .. . + Opp\nRegression\nThe Data\nY = X0\n2. Choose a loss\np+1 1\nfunction\np+1\nX\nYo\nA\nn\nn\nX\n3. Fit the model\nCovariate Matrix\nResponse\n(features)\n(label)\n4. Evaluate model\nperformance\n4\nCc\n2024-10-10 11:14:46\n",
        "Question": "Do we lose one feature by changing the first column into our intercepts?",
        "Timestamp": 765,
        "TA Response": "It is the exact same structure as before just think of before we had a theta_0 and now we are saying there is a theta_1 multiplied by a vector of ones which is again just a constant. Just think of Prof. Gonzalez framing meaning that we have p-1 features and then the column of ones for the intercept term, leaving us with an n x p design matrix x. This reframing is just because Prof. Gonzalez does not find linear algebra with an n x (p + 1) size matrix as intuitive as linear algebra with an n x p matrix",
        "Lecture": 13
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Using a linear algebra perspective to explain concepts like the normal equation in multiple linear regression helps to generalize the solution to higher dimensions and provides a more systematic approach to solving for the coefficients. Linear algebra allows us to efficiently handle large datasets and multiple features by leveraging matrix operations, which are computationally efficient and scalable. Additionally, understanding the linear algebra behind these models can provide insights into the properties of the solution, such as its uniqueness and stability. While it may seem confusing at first, this approach is foundational for more advanced topics in data science and machine learning.",
        "Recent Audio": "if you bought that, this was true. This was true. This is a hat. I mean, this is our best estimate for the mean squared error. Then that would imply that this other crazy thing is true. The Theta hat is just. This. So we know how to solve. The normal equation to get a model. This is our model. Amazing. Questions. This is fast. This is big stuff. This is a basic building block of a lot of things, so you can use this same piece of math here to win the Netflix challenge for $1,000,000. Yeah. Great question. Is this always true? Maybe there are conditions I need on this X and that is the next set of slides. So don't steal it from me. Before I do that, I want to talk about P and DI will confess I am a computer scientist. I do not know why people use P. My statistician friends refer to things as parameters, so they use PP dimensions. I will say D dimensions occasionally. Forgive me. OK, when you look at textbooks. They will alternate between P or D depending on what field they come from. So P dimensions, D dimensions, P parameters, D parameters, bit of notation. And to the earlier question, we're generally going to have situations where N is much bigger than P. That's an important property which we'll come back to when we talk about does this actually work? Under what conditions can this be done? All right. In fact, it's right here.",
        "Recent Video": "Understanding The Solution Matrices\nThe Normal Equation:\nJoey Gonzalez (he, h ...\nXTX\u00d4 = XTY\np\n1\n1\n=\nD\np\nXTX\nXTY\nNote that at least one solution always exists:\nIntuitively, we can always draw a line of best fit for a given set of data, but there may be\nmultiple lines that are \"equally good\". (Formal proof is beyond this course.)\n10\n2024-10-10 11:24:41\n\nUnderstanding The Solution Matrices\nThe Normal Equation:\nJoey Gonzalez (he, h ...\nXTX\u00d4 = XTY\np\n1\n1\nD\np xx\n=\nXTY\nNote that at least one solution always exists:\nIntuitively, we can always draw a line of best fit for a given set of data, but there may be\nmultiple lines that are \"equally good\". (Formal proof is beyond this course.)\n10\n2024-10-10 11:24:46\n",
        "Retrieved Audio": "By the way, I'm bad at linear algebra. I will make mistakes during lecture. To make my life easier, I always draw rectangles. So if you're having trouble in your algebra, draw some rectangles. So you got this rectangle. It's got N rows and records of data P + 1 columns. Why does it have P + 1 columns?\nSo today I'm going to try to do a little bit of recap from Lecture 12, because we did a lot of linear algebra and that can be confusing. And we're going to build on the linear algebra. So it's important to understand what's going on. And then today we will start gradient descent, one of the coolest, hottest algorithms of 2024. Billions of dollars are being spent on gradient descent.\nWe did the linear algebra version of that, which is. We take the inverse of this matrix right here.",
        "Retrieved Notes": "you have taken or are taking a linear algebra course, so we won't explain SVD in its entirety. In particular, we will go over:\n- Why SVD is a valid decomposition of rectangular matrices\n- Why PCA is an application of SVD\nWe will not dive deep into the theory and details of SVD. Instead, we will only cover what is needed for a data science interpretation. If you'd like more information, check out [EECS 16B Note 14](https://inst.eecs.berkeley.edu/~ee16b/sp23/notes/sp23/note14.pdf) or [EECS 16B Note 15](https:\nw approach is useful because it allows us to take advantage of the properties of linear combinations.\nBecause the prediction vector, $\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$, is a **linear combination** of the columns of $\\mathbb{X}$, we know that the **predictions are contained in the span of $\\mathbb{X}$**. That is, we know that $\\mathbb{\\hat{Y}} \\in \\text{Span}(\\mathbb{X})$.\nThe diagram below is a simplified view of $\\text{Span}(\\mathbb{X})$, assuming that each column of $\\mathbb{X}$ has length $n$. Notice\ncifically, we'll see how linear algebra offers us a powerful set of tools for understanding model performance.\n## OLS Problem Formulation\n### Multiple Linear Regression\nMultiple linear regression is an extension of simple linear regression that adds additional features to the model. The multiple linear regression model takes the form:\n$$\\hat{y} = \\theta_0\\:+\\:\\theta_1x_{1}\\:+\\:\\theta_2 x_{2}\\:+\\:...\\:+\\:\\theta_p x_{p}$$\nOur predicted value of $y$, $\\hat{y}$, is a linear combination of the single **observati",
        "Retrieved Video": "Understanding The Solution Matrices\nThe Normal Equation:\nXTX\u00d4 = X\u2122\nJoey Gonzalez (he, h ...\nIn most settings,\nn\n1\n1\n# observations\n# features\np\nXT\n=\nn\nP\nX\nY\nStatisticians often use p (parameters) and\nComputer Scientists often use d (dimensions).\nThe Data\np\n1\n1\np\n1\np\nXTX\nA\n=\nXTY\nn\nX\nY\nSolve for ) :\nCovariate Matrix\nResponse\n1\np\n7\n(features)\n(label)\n= p\n(X\u2122X)-1\nXTY\n9\n2024-10-10 11:21:53\n\nUnderstanding The Solution Matrices\nThe Normal Equation:\nJoey Gonzalez (he, h ...\nXTX\u00d4 = XTY\np\n1\n1\nD\np xx\n=\nXTY\nNote that at least one solution always exists:\nIntuitively, we can always draw a line of best fit for a given set of data, but there may be\nmultiple lines that are \"equally good\". (Formal proof is beyond this course.)\n10\n2024-10-10 11:24:46\n\nUnderstanding The Solution Matrices\nThe Normal Equation:\nJoey Gonzalez (he, h ...\nXTX\u00d4 = XTY\np\n1\n1\n=\nD\np\nXTX\nXTY\nNote that at least one solution always exists:\nIntuitively, we can always draw a line of best fit for a given set of data, but there may be\nmultiple lines that are \"equally good\". (Formal proof is beyond this course.)\n10\n2024-10-10 11:24:41\n",
        "Question": "whats the purpose of using the linear algebra pov to explain this? It just seems more confusing",
        "Timestamp": 870,
        "TA Response": "Our X matrix has a column vector for each of our n data points. Essentially, this is a large system of equations problem. Linear algebra allows us to do lots of calculations simultaneously.",
        "Lecture": 13
    },
    {
        "Conversation History": "\t",
        "GPT Response": "In the context of linear regression, the solution to finding the best fit line can be represented using the normal equation, \\( \\hat{\\beta} = (X^TX)^{-1}X^TY \\), which is a matrix algebra representation. Alternatively, you can approach this problem using calculus by minimizing the mean squared error (MSE) loss function, which involves taking derivatives with respect to the parameters and setting them to zero to find the optimal values. This calculus-based approach leads to the same solution as the normal equation. Additionally, optimization techniques like gradient descent can also be used to iteratively find the best fit line.",
        "Recent Audio": "talk about Theta one. Theta one is R times something here, and this maybe clues us into a problem. If there's no variability in X, what is this? This is 0. What's something divided by zero? It's not a number I don't know undefined, so we need a non constant X. We need variability next for this to be unique. Right for this to be defined. Does that mean I can't compute a line? What does this look like if I have no variability? Next we're going to that data look like. Let's draw some data. My little axis. Here's my Y, here's my X No very million X could look like. Fit that with the line. Well, OK, the line that goes up and down. We can't do that line slope Infinity not define, sorry. But you know what? This line is pretty good. There are lines. Just if I try to write it like this, I cannot. So there's a a solution. I can pick many solutions. Remember I'm finding the best solution. The best solution for mean squared error is the line that goes like this that I can't can't define using this notation. Right. OK. So now this box. What do you think? Unique. Any guess as to what conditions might make this all work out? Who thinks they know the answer to this? All right, who's taking linear algebra classes before correlated? OK, good, good. Let's talk about it. You guys see this? I wrote over the -1 you see this.",
        "Recent Video": "Does a Unique Solution Always Exist?\nModel\nEstimate\nUnique?\nJoey Gonzalez (he, h ...\nConstant Model\ny = 00 0 = mean(y) = y\nYes. Any set of values\n+ MSE Loss\nhas a unique mean.\nYes, if odd.\nConstant Model\ny = 00\n\u00d40 = median(y)\nNo, if even. Return\n+ MAE Loss\naverage of middle 2\nvalues.\nSimple Linear\n\u00d40 = y - 01x\nYes. Any set of\nRegression +\ny = 00 + 01x\nnon-constant in x values\nMSE Loss\n\u00a11 = ry\nhas a unique mean, SD, and\nOx\ncorrelation coefficient.\nOrdinary Least\n<\nSquares\nY = X0\nA =\nXX) -1XTY\n(Linear Model +\nMSE LOSS)\n11\n2024-10-10 11:28:32\n\nDoes a Unique Solution Always Exist?\nModel\nEstimate\nUnique?\nJoey Gonzalez (he, h ...\nConstant Model\ny= 00 0 = mean(y) = y\nYes. Any set of values\n+ MSE Loss\nhas a unique mean.\nYes, if odd.\nConstant Model\ny = 00 00 = median(y)\nNo, if even. Return\n+ MAE Loss\naverage of middle 2\nvalues.\nSimple Linear\n\u00d40 = y - \u00c21x\nYes. Any set of\nRegression +\ny = 00 + 01x\nMSE Loss\n\u00c01 = ry\nnon-constant in x values\nhas a unique mean, SD, and\ncorrelation coefficient.\nOrdinary Least\nY = X0 0 = (XTX)-1XTY\nYes, if X is full col rank\nSquares\n(all cols lin independent,\n(Linear Model +\n#datapoints>>\nMSE LOSs)\n#features)\n12\n2024-10-10 11:28:36\n",
        "Retrieved Audio": "Pythagorean theorem right? Or generalized to many dimensions? How many dimensions is this?\nDoes that mean I can't compute a line?\nWhat's the shape of this matrix?",
        "Retrieved Notes": " variable can also be represented using a histogram. If a variable is **continuous**, meaning it can take on infinitely many values, we can illustrate its distribution using a density curve. \n<p align=\"center\">\n<img src=\"images/discrete_continuous.png\" alt='discrete_continuous' width='700'>\n</p>\nWe often don\u00e2\u0080\u0099t know the (true) distribution and instead compute an empirical distribution. If you flip a coin 3 times and get {H, H, T}, you may ask \u00e2\u0080\u0094\u00e2\u0080\u0094 what is the probability that the coin will land heads?\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of \nw univariate calculus rules like: Taking derivatives of a univariate function (i.e. f(x), where x is the only variable); Derivative power rule; Knowing derivatives of mathematical functions like: sinx,cosx,logx,ex; Chain rule; Product rule (rarely); Derivatives of sums. We will expect some multivariate fluency like: Taking partial derivatives of a multivariate function (i.e. f(x,y,z), where x,y,z are all variables); Gradients (the concept).\n\n-   Khan Academy: [Derivatives, Definitions, and Basic Rules](http",
        "Retrieved Video": "Understanding The Solution Matrices\nThe Normal Equation:\nJoey Gonzalez (he, h ...\nXTX\u00d4 = XTY\nP\n1\n1\nD\np\nXTX\n=\nXTY\nNote that at least one solution always exists:\nIntuitively, we can always draw a line of best fit for a given set of data, but there may be\nmultiple lines that are \"equally good\". (Formal proof is beyond this course.)\n10\n2024-10-10 11:24:51\n\nUnderstanding The Solution Matrices\nThe Normal Equation:\nJoey Gonzalez (he, h ...\nXTX\u00d4 = XTY\np\n1\n1\n=\nD\np\nXTX\nXTY\nNote that at least one solution always exists:\nIntuitively, we can always draw a line of best fit for a given set of data, but there may be\nmultiple lines that are \"equally good\". (Formal proof is beyond this course.)\n10\n2024-10-10 11:24:41\n\nUnderstanding The Solution Matrices\nThe Normal Equation:\nJoey Gonzalez (he, h ...\nXTX\u00d4 = XTY\np\n1\n1\nD\np xx\n=\nXTY\nNote that at least one solution always exists:\nIntuitively, we can always draw a line of best fit for a given set of data, but there may be\nmultiple lines that are \"equally good\". (Formal proof is beyond this course.)\n10\n2024-10-10 11:24:46\n",
        "Question": "out of curiousity is there any other way to represent this? (like a calculus representation or other math, etc)",
        "Timestamp": 1108,
        "TA Response": "If you head to the course notes (https://ds100.org/course-notes/ols/ols.html#geometric-derivation), there's a graphic that might help make the derivation of the normal equation more intuitive. Under the orthogonality (green box), there's a mathematical derivation that may help make more sense of the normal equation. Basically, we know that X is orthogonal to the residual vector (Y - X  * theta_hat). If they're orthogonal, then taking the dot product of X transpose and the residual vector should result in the zero vector. If we distribute X transpose, we can isolate and solve for theta hat.",
        "Lecture": 13
    },
    {
        "Conversation History": "\t",
        "GPT Response": "A constant parameter is useful when we want a simple model that provides a baseline prediction, ignoring any relationships between variables. This can be helpful when we need a quick estimate or when the data does not exhibit clear patterns. However, we do not want a constant parameter when we aim to capture the underlying relationships and variability in the data, as it would oversimplify the model and potentially lead to poor predictions. In such cases, incorporating variability and other features into the model is crucial for capturing the complexity of the data.",
        "Recent Audio": "I guess Theta 2 if you count from zero. The ones doesn't matter where I put this, I could put over here. One of my terms is going to be multiplied by one, so I'm going to have this extra Theta that's just a constant. Build up it's my wise another column matrix or series in this case and look how easy it is to do this math in Python. So I say this is matrix inverse, this is matrix multiply, this is X transpose. So I do X transpose times X inverse. Right there. Times times X transpose times Y. Boom. Solve the system linear equations. Figure out the best model. Easy. If you just copy the equation very good, you shouldn't do this. OK so I just did a matrix inverse and then I multiply, which is jargon for saying solve a system linear equations. I could just ask the linear algebra library to solve the system of equations, but this is my matrix M and here's the thing. I'm trying to to solve equal to the column vector. A. Why would I want to do this? Computer science has thought long and hard about how to make this more efficient than this. Because they end up solving one of these and then doing a multiply up here when they could just solve directly. So you want something a little more stable, but doesn't look exactly like the equations here. This does the same thing, computes the same answer. Which one should you use?",
        "Recent Video": "File\nEdit\nView\nRun\nKernel\nTabs\nSettings\nHelp\nlec13.ipynb\n+\nMarkdown v\nNotebook [7 \u20ba\nPython 3\nJoey Gonzalez (he, h ...\nO\nRecall the solution to the normal equation if X is full column rank:\n\u00f4=(xx) 1X\u2122Y\n[6]:\ntheta_using_normal_equation = np. linalg. inv(X.T @ X) @ X.T @ y\ntheta_using_normal_equation\n[6] :\n0\n0.009828\n1\n0.001477\n2\n11.002995\ndtype: float64\nNote: The code above is inefficient. We won't go into this in our class, but it's better to use np. linalg. solve\nrather than computing the explicit matrix inverse.\n[7]:\nnp. linalg. solve(X.T @ X, X.T @ y)\n[7]: array([9.82848689e-03, 1.47749591e-03, 1.10029953e+01])\nMake Predictions 1l\n.\nWe'll save the predictions in a column so we can compare them against using sklearn.\nSimple\n0\n$_\n4\nPython 3 (ipykernel) | Idle\nMode: Command\nLn 1, Col 1\nlec2024-10-10 11:36:56\n\nFile\nEdit\nView\nRun\nKernel\nTabs\nSettings\nHelp\nlec13.ipynb\n+\n8 + x 0\nMarkdown v\nNotebook [7 \u20ba\nPython 3\nJoey Gonzalez (he, h ...\nO\n6=(x\"x) 1X\u2122Y\n[6]:\ntheta_using_normal_equation = np. linalg.inv(X.T @ X) @ X.T @ y\ntheta_using_normal_equation\n[6]:\n0\n0.009828\n1\n0.001477\n2\n11.002995\ndtype: float64\nNote: The code above is inefficient. We won't go into this in our class, but it's better to use np. linalg. solve\nrather than computing the explicit matrix inverse.\n[7]:\nnp. linalg. solve(X.T @ X, X.T @ y)\n[7]: array( [9.82848689e-03, 1.47749591e-03, 1.10029953e+01])\nMake Predictions\nWe'll save the predictions in a column so we can compare them against using sklearn.\nAf[linand hill donth mmil] - Vto numnul) a thata urina normal anuation\nSimple\n0\n4\nPython 3 (ipykernel) | Idle\nMode: Command\nLn 1, Col 1\nle 2024-10-10 11:37:00\n",
        "Retrieved Audio": "That's an important property which we'll come back to when we talk about does this actually work? Under what conditions can this be done?\nIt's not a number I don't know undefined, so we need a non constant X. We need variability next for this to be unique.\nWhich one should you use?",
        "Retrieved Notes": "the constant model always *predicts the same constant number*. This ignores any relationships between variables. For example, let's say we want to predict the number of drinks a boba shop sells in a day. Boba tea sales likely depend on the time of year, the weather, how the customers feel, whether school is in session, etc., but the constant model ignores these factors in favor of a simpler model. In other words, the constant model employs a **simplifying assumption**.\nIt is also a parametric, statistical m\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of \n and used each one only a \"little bit\"? If we put a limit on how *much* each feature can contribute to the predictions, we can still control the model's complexity without the need to manually determine how many features should be removed. \nWhat do we mean by a \"little bit\"? Consider the case where some parameter $\\theta_i$ is close to or equal to 0. Then, feature $\\phi_i$ barely impacts the prediction \u00e2\u0080\u0093 the feature is weighted by such a small value that its presence doesn't significantly change the valu",
        "Retrieved Video": "Understanding The Solution Matrices\nThe Normal Equation:\nJoey Gonzalez (he, h ...\nXTX\u00d4 = XTY\nP\n1\n1\nD\np\nXTX\n=\nXTY\nNote that at least one solution always exists:\nIntuitively, we can always draw a line of best fit for a given set of data, but there may be\nmultiple lines that are \"equally good\". (Formal proof is beyond this course.)\n10\n2024-10-10 11:24:51\n\nUnderstanding The Solution Matrices\nThe Normal Equation:\nXTX\u00d4 = X\u2122\nJoey Gonzalez (he, h ...\nIn most settings,\nn\n1\n1\n# observations\n# features\np\nXT\n=\nn\nP\nX\nY\nStatisticians often use p (parameters) and\nComputer Scientists often use d (dimensions).\nThe Data\np\n1\n1\np\n1\np\nXTX\nA\n=\nXTY\nn\nX\nY\nSolve for ) :\nCovariate Matrix\nResponse\n1\np\n7\n(features)\n(label)\n= p\n(X\u2122X)-1\nXTY\n9\n2024-10-10 11:21:53\n\nUnderstanding The Solution Matrices\nThe Normal Equation:\nJoey Gonzalez (he, h ...\nXTX\u00d4 = XTY\np\n1\n1\n=\nD\np\nXTX\nXTY\nNote that at least one solution always exists:\nIntuitively, we can always draw a line of best fit for a given set of data, but there may be\nmultiple lines that are \"equally good\". (Formal proof is beyond this course.)\n10\n2024-10-10 11:24:41\n",
        "Question": "When do we want a constant parameter and when do we not want one?",
        "Timestamp": 1605,
        "TA Response": "This will be a model design choice. Excluding it assumes the regression line passes through the origin, which can lead to bias if the true relationship does not pass through zero. In most cases, it\u2019s better to include the intercept unless you have strong theoretical reasons to believe the line should pass through the origin.",
        "Lecture": 13
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The convention is to include an intercept term in the model unless there is a specific reason not to. The intercept term is typically not regularized, which helps avoid the situation where the model always predicts zero. Including an intercept allows the model to better fit the data by accounting for the baseline level of the response variable when all features are zero.",
        "Recent Audio": "origin? 11 We know that the plane at the origin is going to be 11, not 0, because we say when everything else is 0, the intercept term is 11. All right, Hopefully understanding this analyze performance, Oh, I took it away from you. OK, so let's compute the mean squared error. So I'm going to import metrics because it's too hard to write 20 characters of code. But anyways, there's a mean squared error metrics built into psychic learn. Take the mean squared error of my build depth and my secular prediction predictions and I get this number. What unit is this in? Mean squared error, Well, this is in millimeters. I think build depth is in millimeters. Secular predictions are in millimeters. Mean squared error is therefore in millimeters squared. I have no clue what that means. What should I do? What units is that in millimeters? So on average I'm about. 1mm off in my prediction. Pretty good model. I'm pretty happy with that bonus. I should never do bonuses. Let's do it anyways. OK, what if I want to use a crazy model? We haven't taught this yet. Let's use a decision tree regressor. Who knows what that does. We can create an instance of it and we can fit it, right? Boom, fit a model, make some predictions. I'm going to just say predict. Just change my model from linear to tree model that I just fit right here. So crazy new model mean squared error. I don't like those units.",
        "Recent Video": "File\nEdit\nView\nRun\nKernel\nTabs\nSettings\nHelp\nlec13.ipynb\n+\n@ + x 0\nC\nCode\nNotebook [7 \u20ba\nPython 3\nJoey Gonzalez (he, h ...\nO\ntree_model = DecisionTreeRegressor()\ntree_model.fit(\nX=df [ [\"flipper_length_mm\", \"body_mass_g\"]],\ny=df [\"bill_depth_mm\"] )\n[22]:\nDecisionTreeRegressor\nDecisionTreeRegressor ( )\n.......\n[23]:\ndf [\"sklearn_dt_preds\"] = tree_model. predict (df [ [\"flipper_length_mm\", \"body_mass_g\"] ] )\n-\n[ ]:\nmean_squared_error(df [\"bill_depth_mm\"], df [\"sklearn_dt_preds\"] )\nLower error! A better model? Let's visualize it.\n[ ]:\nfig = px. scatter_3d(df, x=\"flipper_length_mm\", y=\"body_mass_g\", z=\"bill_depth_mm\")\n# Create a grid of points to evaluate plane\ngrid_resolution = 20\n(u,v) = np.meshgrid(\nSimple\n0\n$_\n4\nPython 3 (ipykernel) | Idle\nSaving completed\nMode: Command\nLn 1, Col 1\nle 2024-10-10 11:49:39\n\nFile\nEdit\nView\nRun\nKernel\nTabs\nSettings\nHelp\nlec13.ipynb\n+\n0 + x 0\nC\nCode\nNotebook [\nPython 3\nJoey Gonzalez (he, h ...\nO\ntree_model = DecisionTreeRegressor()\ntree_model.fit(\nX=df [ [\"flipper_length_mm\", \"body_mass_g\"]],\ny=df [\"bill_depth_mm\"] )\n[22]:\nDecisionTreeRegressor\nDecisionTreeRegressor ( )\n[23]:\ndf [\"sklearn_dt_preds\"] = tree_model.predict (df [ [\"flipper_length_mm\", \"body_mass_g\"] ] )\n\u00b7 [24] :\nnp. s|mean_squared_error(df[\"bill_depth_mm\"], df [\"sklearn_dt_preds\"] )\n[24]: np. float64(0.051107305936073065)\nLower error! A better model? Let's visualize it.\n[ ]:\nfig = px. scatter_3d(df, x=\"flipper_length_mm\", y=\"body_mass_g\", z=\"bill_depth_mm\")\n# Create a grid of points to evaluate plane\nSimple\n0\n$\n4\nPython 3 (ipykernel) | Idle\nMode: Edit\nLn 1, Col 5 le 2024-10-10 11:49:44\n",
        "Retrieved Audio": "That would have been a much faster way to get the last two lectures, but it's important you understand the math behind us. So what's the intercept going to be?\nNot bad, right? I had the If I did not have an intercept term, what would be true about this plane?\nAnd I can add my columns of ones to encode the intercept if I want it. And this is desired. If I don't have to have an intercept, I can say, you know what, I don't want this. I'm just going to start with my actual data. I can add an extra comma once modeling choice. I make all right questions with the setup questions about the setup for this model.",
        "Retrieved Notes": "\u0093 ideally, more \"important\" features will receive greater weighting. \nNotice that the intercept term, $\\theta_0$, is excluded from this constraint. **We typically do not regularize the intercept term**.\nNow, let's think back to gradient descent and visualize the loss surface as a contour map. As a refresher, a loss surface means that each point represents the model's loss for a particular combination of $\\theta_1$, $\\theta_2$. Let's say our goal is to find the combination of parameters that gives us the lo\ntor. When the dot product is computed, this 1 will be multiplied with $\\theta_0$ to give the intercept of the regression model. We call this 1 entry the **intercept** or **bias** term.\nGiven that we have three features here, we can express this model as:\n$$\\hat{y} = \\theta_0\\:+\\:\\theta_1x_{1}\\:+\\:\\theta_2 x_{2}\\:+\\:\\theta_3 x_{3}$$\nOur features are represented by $x_1$ (`FG`), $x_2$ (`AST`), and $x_3$ (`3PA`) with each having correpsonding parameters, $\\theta_1$, $\\theta_2$, and  $\\theta_3$.\nIn statistics, \nthe intercept term is excluded from the constraint - this is so we avoid the situation where we always predict 0.\nLet's summarize what we have seen. \n<center><img src=\"images/summary.png\" alt='summary' width='700'></center>\n### L1 (LASSO) Regularization\nHow do we actually apply our constraint $\\sum_{i=1}^p |\\theta_i| \\leq Q$? We will do so by modifying the *objective function* that we seek to minimize when fitting a model.\nRecall our ordinary least squares objective function: our goal was to find parameters",
        "Retrieved Video": "ix\nand add a column of 1s to encode the intercept (if desired).\n5\n2024-10-10 11:14:56\n\nToday's Goal: Ordinary Least Squares\nFor each of our n data points:\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\ny = 00 + 01x1 + +2x2 + . . . + Opp\nRegression\nThe Data\nY = X0\n2. Choose a loss\np+1 1\nfunction\np+1\n10\nn\nX\nY\nn\n3. Fit the model\nCovariate Matrix\nResponse\nX\n(features)\n(label)\n4. Evaluate model\nperformance\n4\nCc\n2024-10-10 11:12:12\n\nToday's Goal: Ordinary Least Squares\nFor each of our n data points:\nJoey Gonzalez (he, h ...\n1. Choose a model\nMultiple Linear\ny = 00 + 01x1 + 02x2 + .. . + Opp\nRegression\nThe Data\nY = X0\n2. Choose a loss\np+1 1\nfunction\np+1\nX\n10\nn\nY\nX\n3. Fit the model\nCovariate Matrix\nResponse\nn\n(features)\n(label)\n4. Evaluate model\nperformance\n4\nCc\n2024-10-10 11:11:24\n",
        "Question": "what\u2019s the convention for whether or not to include an intercept? ",
        "Timestamp": 2367,
        "TA Response": "This will be a model design choice. Excluding it assumes the regression line passes through the origin, which can lead to bias if the true relationship does not pass through zero. In most cases, it\u2019s better to include the intercept unless you have strong theoretical reasons to believe the line should pass through the origin.",
        "Lecture": 13
    },
    {
        "Conversation History": "\t",
        "GPT Response": "When the slope is zero, it indicates that we've reached a point where the gradient is flat, suggesting a local minimum, maximum, or saddle point. To determine if it's the global minimum rather than a local one, additional techniques such as evaluating the second derivative (to check for concavity) or using domain knowledge about the function's behavior are needed. In practice, multiple starting points or advanced optimization techniques might be used to ensure convergence to the global minimum.",
        "Recent Audio": "the X minus grad F direction and I put the arrow at the position of F of X. So at this point on the line. So my derivative, which way do I go that direction? So we can now write this algorithm really simply take one step. Algorithm has a guess, takes a derivative and it says my new guess is equal to my old guess minus the derivative. Turn my new guess. I can run this 10 steps starting at a guess of four, and these are my guesses, the sequence of guesses for X. They are the sequence of guesses for X that's trying to minimize that blue line. But let's draw a picture. See what that looks like? Lot steps. There we go. Here's my algorithm. Zoom in. So I guessed here, I took a step, guessed that's my next guest. So I took another step, a little bit steeper, bigger step. Now it'll get interesting. OK, so I was like, oh, jump over here. And he goes, Nope. Jump back over here. Jump back over here. See a problem? Keep taking steps and get to the bottom of the hill. I go jump over the other side and hit the the valley. I should go back this way. Take another step. I keep stepping over the bottom. How would we fix this? People in the front don't answer my question. How do we fix this? It takes smaller steps that would help, right? So maybe I could scale my step? My problem.",
        "Recent Video": "File\nEdit\nView\nRun\nKernel\nTabs\nSettings Help\nlec13.ipynb\n+\n8 + x 0\nC\nMarkdown v\nNotebook [7\nPython 3\nJoey Gonzalez (he, h ...\nO\n5\n5.1\n5.2\n5.3\n5.4\n5.5\nLooking pretty good! We do have a problem though - once we arrive close to the minimum value of the function, our\nguesses \"bounce\" back and forth past the minimum without ever reaching it.\nIn other words, each step we take when updating our guess moves us too far. We can address this by decreasing the\nsize of each step.\nLet's update our algorithm to use a learning rate (also sometimes called the step size), which controls how far we\nmove with each update. We represent the learning rate with a.\na(+1) = x(*) - a dar f(a)(f)\nA small & means that we will take small update steps; a large @ means we will take large steps.\nLet's update our function to use a = 0.3.\nWas taken and atan Tals alpha danisatival.\nSimple\n0\n$_\n4 \u20ba\nPython 3 (ipykernel) | Idle\nMode: Command\nLn 1, Col 1\nle 2024-10-10 12:13:15\n\nFile\nEdit\nView\nRun\nKernel\nTabs\nSettings Help\nlec13.ipynb\n+\n8 + x 0\nC\nMarkdown v\nNotebook [7 # Python 3\nJoey Gonzalez (he, h ...\nO\n2.0\nLooking pretty good! We do have a problem though - once we arrive close to the minimum value of the function, our\nguesses \"bounce\" back and forth past the minimum without ever reaching it.\nIn other words, each step we take when updating our guess moves us too far. We can address this by decreasing the\nsize of each step.\nLet's update our algorithm to use a learning rate (also sometimes called the step size), which controls how far we\nmove with each update. We represent the learning rate with a.\nx(+1) = x(*) -\n\u20ac.\n2\ndx\nA small @ means that we will take small update steps; a large a means we will take large steps.\nLet's update our function to use a = 0.3.\n[ ]:\ndef take_one_step_lr(x, alpha, derivative):\n# Find our new quess using the recurrence relation\nSimple\n0\n$_\n4\nPython 3 (ipykernel) | Idle\nMode: Command\nLn 1, Col 1\nlec2024-10-10 12:13:20\n",
        "Retrieved Audio": "What's the slope of that line 0? So which way should I go? Zero. I don't know which way to go. I'm at the bottom. So now we have a stopping condition. If we get to the zero, we're done, right?\nSlope is 0. Why is it slope 0?\nWe're doing simple stuff. Newton's method is actually a very sophisticated optimization technique that the second order does more than we'll do here. This is going to be a really simple algorithm, but it turns out this is what we actually use when we have billion dollar problems to solve. OK, let's find the minimum. So we're starting here and in in the real world, you don't get to see the blue line. All you get to do is see the little slope around you. How is the shape of the land that you're standing on?",
        "Retrieved Notes": "reater in value than our previous guess \u00e2\u0080\u0093 that is, we want to shift our guess to the right. You can think of this as following the slope \"downhill\" to the function's minimum value.\n<div align=\"middle\">\n  <table style=\"width:100%\">\n    <tr align=\"center\">\n      <td><img src=\"images/neg_step.png\" alt='neg_step' width='600'>\n      </td>\n    </tr>\n  </table>\n</div>\nIf we've overshot $\\hat{\\theta}$ by guessing too high of a value, we'll want our next guess to be lower in value \u00e2\u0080\u0093 we want to shift our guess \nring, let's quickly review gradient descent, which we covered in the last lecture. Recall that gradient descent is a powerful technique for choosing the model parameters that minimize the loss function.  \n### Gradient Descent Review\nAs we learned earlier, we set the derivative of the loss function to zero and solve to determine the optimal parameters $\\theta$ that minimize loss. For a loss surface in 2D (or higher), the best way to minimize loss is to \"walk\" down the loss surface until we reach our optimal \nm and a (barely perceptible) local minimum in the loss surface above. This means that there is the risk of gradient descent converging on the local minimum of the loss surface, missing the true optimum parameter $\\theta_1$.\n<center><img src=\"images/global_local_min.png\" alt='reg' width='400'></center>\n2. Squared loss is *bounded* for a classification task. Recall that each true $y$ has a value of either 0 or 1. This means that even if our model makes the worst possible prediction (e.g. predicting $p=0$ for ",
        "Retrieved Video": "Finding the Minimum\nWhere do we go next? We \"step\" downhill.\nJoey Gonzalez (he, h ...\n3.5\n3\n2.5\n2\n1.5\n1\n-Slope\n0.5\n0\n-0.5\n1\n2\n3\n4\n5\n6\nX\nFollow the downward slope of the line down to the minimum.\n34\n2024-10-10 12:00:08\n\nFinding the Minimum\nWhere do we go next? We \"step\" downhill.\nJoey Gonzalez (he, h ...\n3.5\n3\n2.5\n2\n>\n1.5\n1\n-Slope\n0.5\n0\n-0.5\n1\n2\n3\n4\n5\n6\nX\nFollow the downward slope of the line down to the minimum.\n34\n2024-10-10 12:00:17\n\nSlopes Tell Us Where to Go\nNegative slope - step to the right\nPositive slope - step to the left\nJoey Gonzalez (he, h ...\nMove in the positive direction\nMove in the negative direction\n4\n42\ncc\n2024-10-10 12:02:27\n",
        "Question": "When taking steps and we hit a slope of 0, how do we know we are at the minima and not the local minima? ",
        "Timestamp": 3789,
        "TA Response": "When we reach slope of 0, we stop, and don't know whether we're at a local min/max. We'll discuss more about how we deal with this possibility towards later in lecture (or in the next).",
        "Lecture": 13
    },
    {
        "Conversation History": "\t",
        "GPT Response": "If alpha is fixed, the step size remains constant throughout the updates. However, the steps can effectively become smaller as the algorithm approaches the minimum because the derivative of the function decreases near the minimum. This means that even with a fixed learning rate, the product of the learning rate and the derivative (which determines the step size) becomes smaller as the slope of the function flattens out near the minimum. Additionally, some algorithms use a technique called learning rate decay, where the learning rate itself is gradually reduced over time to ensure smaller steps as the algorithm converges.",
        "Recent Audio": "the slides, is I'm going to add this extra alpha term. This is called a learning rate for those in the machine learning world. It's going to allow me to pick a number less than one, and maybe I'll even shrink it as I go so I take smaller and smaller steps so I don't overstep the bottom of the hill, the minimum. All right, take one step now with alpha. My previous guess minus alpha times the derivative. I should pick a small alpha. Let's pick .3. Run it again. Zoom in. Zoom in again. Oh, zoomed in too far. Getting there so close, it just keeps getting smaller and smaller when it gets there. Why is my blue line not where my red dots are? Plotting error. There's no data here. This blue line isn't precise enough to draw the blue line. I just sampled points. These red dots are exactly where the data is for the purpose, so this is really good guess for the minimum. All right. Questions at this point? Yeah. Is your alpha always between zero and one? Interesting question. Generally it's not greater than one because you don't want to take too large of a step. It's usually a smaller number, and in some algorithms we'll use something called learning rate decay, which is what it sounds like. You make the learning rate get smaller as you take steps. You take 1000 steps, make learning a little bit smaller. Take a few more 1000 steps, learning rate gets smaller.",
        "Recent Video": "File\nEdit\nView\nRun\nKernel\nTabs\nSettings Help\nlec13.ipynb\n+\na + x 0\nC\nMarkdown v\nNotebook [7\nPython 3\nJoey Gonzalez (he, h ...\nO\n-f\n-0.6905\nPath\n+\n-0.691\n-0.6915\n-\n-0.692\n-0.6925\nSimple\n0\n$_\n4\nPython 3 (ipykernel) | Idle\nMode: Command\nLn 1, Col 1 lec2024-10-10 12:14:27\n\nFile\nEdit\nView\nRun\nKernel\nTabs\nSettings Help\nlec13.ipynb\n+\na + x 0\nC\nMarkdown v\nNotebook [7\nPython 3\nJoey Gonzalez (he, h ...\nO\n-f\n-0.6905\nPath\n+\n-0.691\n-0.6915\n-\n-0.692\n-0.6925\nSimple\n0\n4\nPython 3 (ipykernel) | Idle\nMode: Command\nLn 1, Col 1 le 2024-10-10 12:15:05\n",
        "Retrieved Audio": "Is your alpha always between zero and one? Interesting question. Generally it's not greater than one because you don't want to take too large of a step.\nAll right, take one step now with alpha. My previous guess minus alpha times the derivative. I should pick a small alpha. Let's pick .3. Run it again.\nIt's usually a smaller number, and in some algorithms we'll use something called learning rate decay, which is what it sounds like. You make the learning rate get smaller as you take steps. You take 1000 steps, make learning a little bit smaller. Take a few more 1000 steps, learning rate gets smaller.",
        "Retrieved Notes": " 2\nLet's update our algorithm to use a **learning rate** (also sometimes called the step size), which controls how far we move with each update. We represent the learning rate with $\\alpha$. \n$$x^{(t+1)} = x^{(t)} - \\alpha \\frac{d}{dx} f(x^{(t)})$$\nA small $\\alpha$ means that we will take small steps; a large $\\alpha$ means we will take large steps. When do we stop updating? We stop updating either after a fixed number of updates or after a subsequent update doesn't change much.\nUpdating our function to use\n a problem though \u00e2\u0080\u0093 once we arrive close to the minimum value of the function, our guesses \"bounce\" back and forth past the minimum without ever reaching it.\n<div align=\"middle\">\n  <table style=\"width:100%\">\n    <tr align=\"center\">\n      <td><img src=\"images/grad_descent_2.png\" alt='grad_descent_2' width='500'>\n      </td>\n    </tr>\n  </table>\n</div>\nIn other words, each step we take when updating our guess moves us too far. We can address this by decreasing the size of each step. \n#### Algorithm Attempt\nreater in value than our previous guess \u00e2\u0080\u0093 that is, we want to shift our guess to the right. You can think of this as following the slope \"downhill\" to the function's minimum value.\n<div align=\"middle\">\n  <table style=\"width:100%\">\n    <tr align=\"center\">\n      <td><img src=\"images/neg_step.png\" alt='neg_step' width='600'>\n      </td>\n    </tr>\n  </table>\n</div>\nIf we've overshot $\\hat{\\theta}$ by guessing too high of a value, we'll want our next guess to be lower in value \u00e2\u0080\u0093 we want to shift our guess ",
        "Retrieved Video": "sometimes called the step size), which controls how far we\nmove with each update. We represent the learning rate with a.\nx(+1) = x(*) -\n\u20ac.\n2\ndx\nA small @ means that we will take small update steps; a large a means we will take large steps.\nLet's update our function to use a = 0.3.\n[ ]:\ndef take_one_step_lr(x, alpha, derivative):\n# Find our new quess using the recurrence relation\nSimple\n0\n$_\n4\nPython 3 (ipykernel) | Idle\nMode: Command\nLn 1, Col 1\nlec2024-10-10 12:13:20\n\nse a smaller step size\nFelix\nyintercept\ntaking derivative\nx/2 (smaller steps)\nusing regression\n2024-10-10 12:04:32\n\nning rate (also sometimes called the step size), which controls how far we\nmove with each update. We represent the learning rate with a.\na(+1) = x(*) - a dar f(a)(f)\nA small & means that we will take small update steps; a large @ means we will take large steps.\nLet's update our function to use a = 0.3.\nWas taken and atan Tals alpha danisatival.\nSimple\n0\n$_\n4 \u20ba\nPython 3 (ipykernel) | Idle\nMode: Command\nLn 1, Col 1\nle 2024-10-10 12:13:15\n",
        "Question": "if alpha is fixed how does it take smaller steps when it gets to minima?",
        "Timestamp": 3892,
        "TA Response": "Prof Gonzalez stated that alpha can also be a function based on the number of steps we take, so the more steps we take, the smaller the alpha each time.",
        "Lecture": 13
    }
]