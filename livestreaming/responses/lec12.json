[
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, the granularity of this dataset is indeed scores for individual players. Each row in the dataset represents the performance metrics of a single player, including their average points per game and other statistics like field goals, assists, and three-point attempts.",
        "Recent Audio": "different player can be learned parameters so that we can calculate a weighted combination of those features together in order to predict the average number of points that they would score in a given in a given game. So for example in this case if I look at one specific Rd. this is the second player. What I can, what I know from this? This player is that this player has goes a specific features and then in the ground truth in the in the original data set, what I have as the target value is that they scored an average 1.7 points in in a game. So my model is then going to look like exactly the same kind of blue machine or the blue box that I was showing you that it is parameterized by all of those parameters that I have. And then the input feature vector is going to go in and the input feature. You can verify that this is .4 right here, this is .8 right here, and this is 1.5. And what I'm expecting is to get a prediction that hopefully this prediction Y hat is going to be as close as possible to this ground truth value that I have. And we're going to talk about how to build parameters that are listed here so that this prediction is as close as possible to the ground truth values. And then how can we make sure that in the entire data set, on average all of our predictions are.",
        "Recent Video": "Lec 12 - DS100 Fa24 - Ordinary\nx\nlec12.ipynb - JupyterLab\n\u00d7\n+\nC\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g1163459c7f0_0_564\nNarges Norouzi\n..\nO\n7\nMultiple Linear Regression Model\nT\n*\nab\nA\nHow many points does an athlete score per game?\n2931334\nPTS (average points/game)\n--\nTo name a few factors:\nFG: average # 2 point field goals\nAST: average # of assists\n3PA\nFG\n3PA: average # 3 point field goals attempted\nassist: a pass to a teammate\nthat directly leads to a goal\nFG AST 3PA\nPTS\n0.4\nQ1\n1 1.8\n0.6\n4.1\n5.3\n0.8\n2\n0.4\n0.8\n1.5\n1.7\n1.5\n:\nOp\n3\n1.1\n1.9\n2.2\n3.2\n4\n6.0\n1.6\n0.0\n13.9\n5 3.4\n2.2\n0.2\n8.9\n6 0.6\n0.3\n1.2\n1.7\nRows correspond to\nindividual players.\n10\n2024-10-08 11:19:29\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\ndocs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g1163459c7f0_0_564\nM\nNarges Norouzi\n..\nO\n7\nMultiple Linear Regression Model\n*\nab\nT\n.7\nHow many points does an athlete score per game?\n2931334\nPTS (average points/game)\nTo name a few factors:\nFG: average # 2 point field goals\nAST: average # of assists\n3PA\nFG\n3PA: average # 3 point field goals attempted\nassist: a pass to a teammate\nthat directly leads to a goal\nFG AST 3PA\nPTS\n0.4\nQ1\n1\n1.8\n0.6\n4.1\n5.3\n1\n0.8\ny\n2\n0.4\n0.8\n1.5\n1.7\n1.5\n:\n3\n1.1\n1.9\n2.2\n3.2\nOp\n4\n6.0\n1.6\n0.0\n13.9\n5 3.4\n2.2\n0.2\n8.9\n6 0.6\n0.3\n1.2\n1.7\nRows correspond to\nindividual players.\n10\n2024-10-08 11:19:34\n",
        "Retrieved Audio": "This player is that this player has goes a specific features and then in the ground truth in the in the original data set, what I have as the target value is that they scored an average 1.7 points in in a game. So my model is then going to look like exactly the same kind of blue machine or the blue box that I was showing you that it is parameterized by all of those parameters that I have. And then the input feature vector is going to go in and the input feature.\nThis is kind of the data set or the shape of the data that we want to deal with and we want to use this information in order to be able to predict that. Having information about field goal assist 3 PA for another different player can be learned parameters so that we can calculate a weighted combination of those features together in order to predict the average number of points that they would score in a given in a given game.\nThe input feature values that we have are going to be field goal. So the field goal is the number of two pointers that these players would with the score on average in a game. We also have a feature AST or the number of average number of assists that they have in a given game. So this is the assist column. And then there is also three PA or the number of tree pointers that they would score. And this is also the number of and this is exactly that feature. So you can imagine that.",
        "Retrieved Notes": "Discussion on Data Granularity\nFrom the description:\n* Monthly measurements are averages of average day measurements.\n* The NOAA GML website has datasets for daily/hourly measurements too.\nThe data you present depends on your research question.\n**How do CO<sub>2</sub> levels vary by season?**\n* You might want to keep average monthly data.\n**Are CO<sub>2</sub> levels rising over the past 50+ years, consistent with global warming predictions?**\n* You might be happier with a **coarser granularity** of average \nhe dataset, the next task is to determine what exactly the data represents. We'll do so by considering the data's granularity, scope, and temporality.\n### Granularity\nThe **granularity** of a dataset is what a single row represents. You can also think of it as the level of detail included in the data. To determine the data's granularity, ask: what does each row in the dataset represent? Fine-grained data contains a high level of detail, with a single row representing a small individual unit. For example, ea\nch record may represent one person. Coarse-grained data is encoded such that a single row represents a large individual unit \u00e2\u0080\u0093 for example, each record may represent a group of people.\n### Scope\nThe **scope** of a dataset is the subset of the population covered by the data. If we were investigating student performance in Data Science courses, a dataset with a narrow scope might encompass all students enrolled in Data 100 whereas a dataset with an expansive scope might encompass all students in California",
        "Retrieved Video": "a goal\nFG AST 3PA\nPTS\n0.4\nQ1\n1 1.8\n0.6\n4.1\n5.3\n0.8\n2\n0.4\n0.8\n1.5\n1.7\n1.5\n:\nOp\n3\n1.1\n1.9\n2.2\n3.2\n4\n6.0\n1.6\n0.0\n13.9\n5 3.4\n2.2\n0.2\n8.9\n6 0.6\n0.3\n1.2\n1.7\nRows correspond to\nindividual players.\n10\n2024-10-08 11:19:29\n\no a goal\nFG AST 3PA\nPTS\n0.4\n1 1.8\n0.6\n4.1\n5.3\n0.8\n2\n0.4\n0.8\n1.5\n1.7\n1.5\n:\nOp\n3\n1.1\n1.9\n2.2\n3.2\n4\n6.0\n1.6\n0.0\n13.9\n5 3.4\n2.2\n0.2\n8.9\n6 0.6\n0.3\n1.2\n1.7\nRows correspond to\nindividual players.\n10\n2024-10-08 11:19:19\n\ndirectly leads to a goal\nFG AST 3PA\nPTS\n1 1.8\n0.6 4.1\n5.3\n2 0.4\n0.8\n1.5\n1.7\n3\n1.1\n1.9\n2.2\n3.2\n4\n6.0\n1.6\n0.0\n13.9\n5 3.4\n2.2\n0.2\n8.9\n6 0.6\n0.3\n1.2\n1.7\nRows correspond to\nindividual players.\n9\n2024-10-08 11:17:05\n",
        "Question": "would the granularity of this dataset be  scores for individual players?",
        "Timestamp": 564,
        "TA Response": "Yes (\"average points\" instead of \"scores\", to be more precise)",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, the variability of predictions in a multiple linear regression model can be assessed using the variance of the prediction errors, also known as the residual variance. This involves calculating the differences between the observed values and the predicted values (using the input vector \\( x \\) and the known parameter vector \\( \\theta \\)), and then computing the variance of these residuals. Additionally, the standard error of the estimate can provide insight into the variability of predictions around the regression line.",
        "Recent Audio": "I have as the target value is that they scored an average 1.7 points in in a game. So my model is then going to look like exactly the same kind of blue machine or the blue box that I was showing you that it is parameterized by all of those parameters that I have. And then the input feature vector is going to go in and the input feature. You can verify that this is .4 right here, this is .8 right here, and this is 1.5. And what I'm expecting is to get a prediction that hopefully this prediction Y hat is going to be as close as possible to this ground truth value that I have. And we're going to talk about how to build parameters that are listed here so that this prediction is as close as possible to the ground truth values. And then how can we make sure that in the entire data set, on average all of our predictions are. As possible to the target values that we have, that's our goal and we're going to actually use linear algebra to just write equations that will do that for us exactly. Again, plugging in these features and those attributes is going to give me this model exactly that I have set to 0, Tetra 1, and then again, each one of these, each one of these features are corresponding to one specific attribute right here. And then we're also going to tag along the set to 0, which is.",
        "Recent Video": "Lec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\ndocs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g1163459c7f0_0_564\nM\nNarges Norouzi\n..\nO\n7\nMultiple Linear Regression Model\n*\nab\nT\n.7\nHow many points does an athlete score per game?\n2931334\nPTS (average points/game)\nTo name a few factors:\nFG: average # 2 point field goals\nAST: average # of assists\n3PA\nFG\n3PA: average # 3 point field goals attempted\nassist: a pass to a teammate\nthat directly leads to a goal\nFG AST 3PA\nPTS\n0.4\nQ1\n1\n1.8\n0.6\n4.1\n5.3\n1\n0.8\ny\n2\n0.4\n0.8\n1.5\n1.7\n1.5\n:\n3\n1.1\n1.9\n2.2\n3.2\nOp\n4\n6.0\n1.6\n0.0\n13.9\n5 3.4\n2.2\n0.2\n8.9\n6 0.6\n0.3\n1.2\n1.7\nRows correspond to\nindividual players.\n10\n2024-10-08 11:19:34\n\nLec 12 - DS100 Fa24 - Ordinary\n\u00d7\nlec12.ipynb - JupyterLab\n\u00d7\n+\nC \u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g1163459c7f0_0_564\nNarges Norouzi\nO\nV\n..\n7\nMultiple Linear Regression Model\nab\nT\nO\nHow many points does an athlete score per game?\n2931334\nPTS (average points/game)\n0\nTo name a few factors:\nFG: average # 2 point field goals\nAST: average # of assists\n3PA\nFG\n3PA: average # 3 point field goals attempted\nassist: a pass to a teammate\n00\nthat directly leads to a goal\nFG AST 3PA\nPTS\n0.4\n01\n1 1.8\n0.6\n4.1\n5.3\n1\n0.8\ny\n2\n0.4\n0.8\n1.5\n1.7\n1.5\n:\n3\n1.1\n1.9\n2.2\n3.2\nOp\n4\n6.0\n1.6\n0.0\n13.9\n5 3.4\n2.2\n0.2\n8.9\ny = 00 + 01x1 + +2x2 + . . . + Opp\n6 0.6\n0.3\n1.2\n1.7\n=00 + 01x1 + +2x2 ++3X3\nRows correspond to\nindividual players.\nFG\nAST\n3PA\n10\n2024-10-08 11:20:02\n",
        "Retrieved Audio": "This vector X bar and then they are weighted using this parameter vector that this parameter vector is Theta 0, Theta 1 all the way to Theta P And then obviously at the end we know that we're getting out of this machine. What you were getting is the prediction. So notice that we have multi dimensional input, multi dimensional parameter model. And then what we're getting as the output is just a single prediction and that's the scalar value or a value in real.\nStretch herself to get different lengths and then collect another data point and so on and so forth. And what I'll do is I'll plug in all of these attributes that I have into a multiple, multiple regression model, assuming that I have the parameters known for the weight and the length, right? So this is the parameter for the length. For example, this is a parameter for the weight. If I plug in any feature, any entry, or any observation for a cat, such as again, the length and the weight.\nUp until the point that I am able to do a prediction while I had on a single data point that I have. So what are the parameters for this multiple linear regression model? What we have is set to 0 or the same Y intercept bias term that we have been talking about previously both in the in the constant model and also in the simple linear regression case. And then also Theta 1 tetta 2 all the way to teta P If I have P different features, I have one Theta zero and then P different parameters for.",
        "Retrieved Notes": "uct of the parameter vector and *first* observation vector. To make a prediction from the *second* observation, we would repeat this process to find the dot product of the parameter vector and the *second* observation vector. If we wanted to find the model predictions for each observation in the dataset, we'd repeat this process for all $n$ observations in the data. \n$$\\hat{y}_1 = \\theta_0 + \\theta_1 x_{11} + \\theta_2 x_{12} + ... + \\theta_p x_{1p} = [1,\\:x_{11},\\:x_{12},\\:x_{13},\\:...,\\:x_{1p}] \\theta$$\n$$\nta_0 + \\theta_1 x_i$$\nThe examples above are known as **parametric models**. They relate the collected data, $x_i$, to the prediction we make, $\\hat{y}_i$. A few parameters ($\\theta$, $\\theta_0$, $\\theta_1$) are used to describe the relationship between $x_i$ and $\\hat{y}_i$.\nNotice that we don't immediately know the values of these parameters. While the features, $x_i$, are taken from our observed data, we need to decide what values to give $\\theta$, $\\theta_0$, and $\\theta_1$ ourselves. This is the heart \nfrac{1}{n} \\sum_{i=1}^n (\\frac{x_i-\\bar{x}}{\\sigma_x})(\\frac{y_i-\\bar{y}}{\\sigma_y}) \\right)$ and standard deviation $\\left(\\sigma_x = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2} \\right)$, we can conclude:\n$$r \\sigma_x \\sigma_y = \\hat{\\theta}_1 \\times \\sigma_x^2$$\n$$\\hat{\\theta}_1 = r \\frac{\\sigma_y}{\\sigma_x}$$\nJust as was given in Data 8! \nRemember, this derivation found the optimal model parameters for SLR when using the MSE cost function. If we had used a different model or different loss function",
        "Retrieved Video": "Lec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\nx | +\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ2oTy4GI7GDyyq1TLL59a0Q/edit#slide=id.g22f001ce500_0_15\nNarges Norouzi\n..\n7\nMultiple Linear Regression\nab\nT\nDefine the multiple linear regression model:\n2931334\n4 = 00 + +1x1 + +2x2 + .. . + Opp\nPredicted\nvalue of y\nYes! This is a linear combination of\nOi's, each scaled by x j.\n7\nCC\nBY\nNC\nSA\n2024-10-08 11:14:12\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\nG\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g22f001ce500_0_15\nM\nNarges Norouzi\n..\nO\nV\n7\nMultiple Linear Regression\nab\nT\nDefine the multiple linear regression model:\n2931334\ny = 00 + +1x1 + +2x2 + ... + Opp\nPredicted\nvalue of y\n7\nCC\nBY\nNC\nSA\n2024-10-08 11:13:14\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\nx | +\n->\nC\n5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g22f001ce500_0_15\nNarges Norouzi\n..\nO\n7\nMultiple Linear Regression\nab/\nT\nDefine the multiple linear regression model:\n2931334\ny = 00 + 01x1 + +2x2 + .. . + Opp\nPredicted\nvalue of y\nYes! This is a linear combination of\n0's, each scaled by x j.\nx1\nx =\n:\nIp\n:\nOp\nsingle\nsingle observation\nprediction\n(p features)\n7\nBY\nNC\nSA\n2024-10-08 11:14:36\n",
        "Question": "Is there a way to measure the variability of the prediction depending on the inputted x vector and known theta vector?",
        "Timestamp": 587,
        "TA Response": "This is a great question that has quite a bit of nuance to it, as there are a few different things we could consider to be our \"variance\". So I won't answer your question here, but in a few lectures we will discuss these different variances in these models in greater detail.",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The midterm will cover material up to the lecture before the midterm date, which is on Wednesday, October 23.",
        "Recent Audio": "then XP and so on and so forth which is going to give me exactly this. OK, I'm just going to leave you with that to think about it for a SEC and just verify that the dot product between those two column vectors in fact is equal to that summation that I have at the top. Because if you calculate the dot product between or if you calculate the, if you. Multiply element wise these two vectors, you're multiplying that to 0, but one that to one with X, one that to two with X2 and so on and so forth. And then obviously that's going to give you your multiple regression equation. OK, So this is our trick. So what we were able to do was to just write our prediction task or that multiple regression equation in terms of the dot product between two vectors X and Theta, which we know that the dot product between X. The vector X and also the vector Theta can be calculated using transposing X and multiplying it with Theta as a normal matrix multiplication. So that's Theta, this is X and then we'll be on our way. So what I can do is I can actually go ahead and apply exactly the same thing to all of these predictions that I wanted to do for remember that for data .1, for data .2 for data point and all of the rows that I had in the data frame I had to calculate. This long summation individually.",
        "Recent Video": "Lec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\n5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g22f0a068c0e_0_8\nNarges Norouzi\n..\nV\n7\nMatrix Notation\nab\nT\n\u00a71 = 00 + 01x11 + 02x12 + . . . + 0px1p\n2931334\ny2 = 00 + 01x21 + 02x22 + .. . + 0px2p\n.\nUn = 00 + Q1xnl + O2xn2 + ... + 0pxnp\n18\nCC\nBY\nNC\nSA\n2024-10-08 11:38:12\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\nX\n+\n->\nC \u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g22f0a068c0e_0_8\nNarges Norouzi\n..\n71\nK\nMatrix Notation\nT\n*\nab\ny1 = 00 + 01011 + 02012 + . . . + 0px1p\n2931334\ny2 = 00 + 010021 + 02022 + . .. +0px2p\n.. .\nUn = 00 + 01xnl + O2Xn2 + ... + OpXnp\ny1 = x\nwhere X = |1\nT\n\u20be11\n\u20be12\n...\n1p is datapoint/observation 1\n\u00a72 = x2 0\nwhere X2 = |1\n\u20be21\n\u20be22\n...\nX2p | is datapoint/observation 2\n.\n.\nUn = x where In\nT\n! = 1\nXml Xn 2\n...\nInp is datapoint/observation n\n18\nCC\nBY\nNC\nSA\n2024-10-08 11:38:36\n",
        "Retrieved Audio": "Involved lecture, but the good news is that from this point and on everything is going to get easier and it's going to be more on the applied side of thing. And then we're going to see in the second-half of DATA 100 after the midterm, we're going to see different applications or different machine learning.\nSo today's lecture is going to use a data set.\nMe reviewing or reminding of some linear algebra concepts and then using them, and then I'll keep building on top of the previous concept that we built. So it's going to be pretty.",
        "Retrieved Notes": "\n> -   **Midterm** on **Wednesday, October 23** 7-9 PM PST.\n> -   **Final** on **Wednesday, December 18** 8-11 AM PST.\n\n**All exams must be taken in-person.** There will be no alternate exams offered.\n\n### Graduate Final Project\n\nAll students enrolled in the graduate version of the course (CS C200A or Stat C200C, i.e. Data 200; Data 200S) will be graded according to the Graduate grading scheme, which includes a team-based **Graduate Final Project** distributed in the second half of the semester. More detail\nlectures** held on Tuesdays and Thursdays, 11:00am-12:30pm, hybrid: in-person in Wheeler 150 and broadcasted live on Zoom. All session recordings, slides, activities, and examples will be uploaded to the course website within 24 hours of the lecture.\n\n**Lecture participation**: While synchronous lecture attendance (in-person or through Zoom) is not mandatory, we expect you to \u00e2\u0080\u009cparticipate\u00e2\u0080\u009d in lecture by answering lecture poll questions in one of two ways: (1) synchronously during the lecture as poll q\nhere will be no alternate final exam time beyond the standard time assigned by scheduling.\n\n___\n\n## What is the midterm schedule for this class?\n\nThe exact midterm exam dates will be announced on the first day of class.\n\n___\n\n## What is the conflict policy for the course?\n\nThe basic policy is the following:\n\n-   ENROLLMENT TIME CONFLICTS: This class will NOT allow time conflicts.\n    \n-   FINAL EXAM CONFLICTS: This class will NOT accommodate final exam conflicts.\n    \n\n___\n\n## I have a conflict for the fina",
        "Retrieved Video": "11:31:14\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nXn1\nIn2\nUnp\nyn\n. . .\n14\nCC\nBY\nNC\nSA\n2024-10-08 11:26:36\n\n4-10-08 11:31:00\n",
        "Question": "Until which lecture will the midterm be?",
        "Timestamp": 1687,
        "TA Response": "Lecture 13 (Gradient Descent), which will be this Thursday's lecture.",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "In the context of the lecture, option 5 is considered \"linear in theta\" because it can be expressed as a matrix product where the theta parameters are separated into a vector. The expression involves a matrix of features and a vector of theta parameters, which can be multiplied if the dimensions are compatible. The confusion might arise from the dimensions mentioned; typically, the feature matrix should be transposed to match the dimensions of the theta vector, allowing for valid matrix multiplication. Therefore, the multiplication is possible when the dimensions are correctly aligned, such as multiplying a 3x4 matrix by a 4x1 vector.",
        "Recent Audio": "out the Theta vector as one vector and then multiply all the weights and all of the parameters or features as a separate matrix to it. And in that case, we would call a model a linear model in PETA. So the question for you is that which one of these five options are linear and teta based on based on the description that I gave you? And I will stay here and will let you respond. OK, let's take a look. You're saying four and five. Four and five are obvious because they take exactly the same form of our linear regression, right? So they have a matrix that is multiplied by a vector of all of those sets of values that I have. But we can go and take a look at 1-2 and three and then see which one of them are linear in sets and which one of them are not linear in PETA. So starting with the first one. The first one is linear in teta because I can write it as the dot product between the two vectors and the two vectors are one vector fully have all the Theta values and non transformative values. And then another vector has all of those other values that I don't care if they're linear, they're not linear and whatever value they have. The only thing that I care about is that I can write my equation in a way that all the Theta values can be factored out into a vector. So.",
        "Recent Video": "Lec 12 - DS100 Fa24 - Ordinary\nx\nlec12.ipynb - JupyterLab\nx | +\nC\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ2oTy4GI7GDyyq1TLL59a0Q/edit#slide=id.g1163459c7f0_0_424\nNarges Norouzi\n..\n7\nLinear in Theta\nab\nT\nAn expression is \"linear in theta\" if it is a linear combination of parameters 0 = [00, 01, . . ., Op]\n2931334\ny1\n1234\n00\n1. \u00a7 = 00 + 01 (2) + 02 (4 \u00b7 8) + 03 (log 42)\n01\n4.\ny2\n=\n1567\ny3\n1890\n02\n03\n00\n2.g=00+01x1 ++2x2x3 +03. log(x4)\nL\n1 x11 X12 013\n5.\ny2\n=\n1 x21 X22 X23\n93\n1 x31 X32 \u00a333\nQ2\nQ3\n\"Linear in theta\" means the expression\n3. \u00fd = 00 + 01x1 + log(02)22 + +304\ncan separate into a matrix product of two\nterms: a vector of thetas, and a\nmatrix/vector not involving thetas.\n28\nCC\n$\nBY\nNC\nSA\n2024-10-08 11:51:24\n\nLec 12 - DS100 Fa24 - Ordinary\nx\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\nC \u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ2oTy4GI7GDyyq1TLL59a0Q/edit#slide=id.g1163459c7f0_0_424\nNarges Norouzi\n..\nO\n7\nLinear in Theta\nT\nAn expression is \"linear in theta\" if it is a linear combination of parameters 0 = [00, 01, . .., p]\n2931334\ny1\n1234\n00\n1. \u00a7 = 00 + 01 (2) + 02 (4 \u00b7 8) + 03 (log 42)\n4.\ny2\n=\n1567\n01\n00\n\u00a73\n1890\n= [1 2 4.8 log(42)]\n01\nQ3\n02\nQ3\n2.g= 00 + 01x1 + +2x2x3 + 03. log(x4)\n00\nL\n1 x11 X12 013\n5.\n92\n=\n1 x21 X22 X23\n93\n1 x31 X32 \u00a333\nA2\nQ3\n\"Linear in theta\" means the expression\n3.y = 00 + 01x1 + log(02)x2 + 0304\ncan separate into a matrix product of two\nterms: a vector of thetas, and a\nmatrix/vector not involving thetas.\n28\nCC\nBY\nNC\nSA\n2024-10-08 11:51:29\n",
        "Retrieved Audio": "You're saying four and five. Four and five are obvious because they take exactly the same form of our linear regression, right? So they have a matrix that is multiplied by a vector of all of those sets of values that I have. But we can go and take a look at 1-2 and three and then see which one of them are linear in sets and which one of them are not linear in PETA. So starting with the first one.\nWhich is going to give you 1 * 1 + 2 * 1 + 3 * 1 which is equal to 6. And in fact, if you were to actually do it exactly but just doing the element wise product and then do the summation, you would also get the value 6 as a result. Something that I'll use later in today's lecture is that another way to think about the dog product is that you can calculate the dog product by.\nAlthough we're multiplying a bunch of different terms and we're summing them all altogether.",
        "Retrieved Notes": "atrices. Just like with real numbers, there are infinite ways to decompose a matrix into a product of two matrices. For example, $9.9$ can be decomposed as $1.1 * 9$, $3.3 * 3.3$, $1 * 9.9$, etc. Additionally, the sizes of the 2 decomposed matrices can vary drastically. In the example below, the first factorization (top) multiplies a $3x2$ matrix by a $2x3$ matrix while the second factorization (bottom) multiplies a $3x3$ matrix by a $3x3$ matrix; both result in the original matrix on the right.\n<center><im\ng src = \"images/matrix_decomp.png\" width=\"550vw\"></center>\n<br> \nWe can even expand the $3x3$ matrices to $3x4$ and $4x3$ (shown below as the factorization on top), but this defeats the point of dimensionality reduction since we're adding more \"useless\" dimensions. On the flip side, we also can't reduce the dimension to $3x1$ and $1x3$ (shown below as the factorization on the bottom); since the rank of the original matrix is greater than 1, this decomposition will not result in the original matrix. \n<center\ntorization.png\" width=\"500vw\"></center>\nFirst, consider the matrix multiplication example below:\n<center><img src = \"images/matmul.png\" width=\"700vw\"></center>\n* For table 1, each row of the fruits matrix represents one bowl of fruit; for example, the first bowl/row has 2 apples, 2 lemons, and 2 melons.\n* For table 2, each column of the dollars matrix represents the cost of fruit at a store; for example, the first store/column charges 2 dollars for an apple, 1 dollar for a lemon, and 4 dollars for a melon.\n",
        "Retrieved Video": "Lec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n25 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g22f0a068c0e_0_8\nNarges Norouzi\n..\n7\nMatrix Notation\n*\nab\u0131>\nT\nO\n91 = 00 + 01x11 + 02212 + . .. +0px1p\n2931334\ny2 =00 + 01021 + 02222 + ... +0p 2p\n. . .\nyn = 00 + 01xn1 + 02xn2 + . . . + Opanp\n18\nCC\nS\nBY\nNC\nSA\n2024-10-08 11:38:07\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\n5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g22f0a068c0e_0_8\nNarges Norouzi\n..\nV\n7\nMatrix Notation\nab\nT\n\u00a71 = 00 + 01x11 + 02x12 + . . . + 0px1p\n2931334\ny2 = 00 + 01x21 + 02x22 + .. . + 0px2p\n.\nUn = 00 + Q1xnl + O2xn2 + ... + 0pxnp\n18\nCC\nBY\nNC\nSA\n2024-10-08 11:38:12\n\nLec 12 - DS100 Fa24 - Ordinary\nx\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\nC\n5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g2311b8f33d3_0_0\nNarges Norouzi\n..\nOL\nO\n7\nVector Notation\nab\nT\ny = 00 + 01001 + 0222 + . . . + 0ptp\n2931334\nWe want to collect\nall the 0;'s into a\nThis part looks a little like a dot product ...\nsingle vector.\n02\nX2\n= 00 . 1 + 01001 + 0222 + . . . + 0pxp\n.\n.\n.\n.\n.\n.\nOp\nXp.\nWhat about\nthis one ???\n16\nCC\nO\nBY\nNC\nSA\n2024-10-08 11:36:31\n",
        "Question": "Why is 5 correct? Aren\u2019t we multiplying a 3x4 by a 3x1 which is not possible?",
        "Timestamp": 2486,
        "TA Response": "This slide shows that theta 4 elements but the previous slide showed only 3",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "No, you cannot factor out theta from log(theta) because the logarithm function is non-linear. In the context of determining if an expression is linear in theta, all terms involving theta must be able to be expressed as a linear combination, which means they can be separated into a matrix product of a vector of thetas and a matrix/vector not involving thetas. Since log(theta) involves a non-linear transformation of theta, it cannot be factored out in this way.",
        "Recent Audio": "Here they're factored out into a vector and option one is linear in Theta. Option 2 is also linear in Theta. Because again, it doesn't matter that for example, in this case I am, I have petted 2 and then I'm multiplying to two with X2 and X3. That doesn't matter. What is happening basically is that when you're looking at your feature 2. So this is the bias term this is. One, and this is feature 2, it just happens that you did feature engineering that you're gonna see I think next lecture or two lectures from now that we did feature engineering and we decided that instead of using X2 and X3 separately, I'm gonna use the product of those two terms together as a feature. So the features can be non-linear, but the parameter vector should stay intact and should stay linear and. Untransformed. What we're seeing in the last one is, the last one is not. Linear and Theta. There is no way that you can just factor out all the tetas into one vector and be able to create. This term of 10 to three times multiplied by Theta 4. So only the the third option is the only option that's not linear in feta. So kind of to set up the expectation that when we say that the model is linear in PETA, what we mean exactly is that collect everything else into a matrix, leaving alone all the tetas intact and in linear form into one vector and just.",
        "Recent Video": "Lec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\nC\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g1163459c7f0_0_424\nNarges Norouzi\n...\nK\n7\nLinear in Theta\nT\nAn expression is \"linear in theta\" if it is a linear combination of parameters 0 = [00, 01 ,..., 0p]\n2931334\n1\ny1\n1234\n1. \u00a7 = 00 + 01 (2) + 02 (4 \u00b7 8) + +3 (log 42)\n4.\ny2\n15 67\nQ1\n=\ny3\n1890\n= [1 2 4.8 log(42)]\nQ3\n2. y = 00 + 01x1 + +2x2x3 + 03. log(x4)\n7\ny\u0131l\n1 X11 X12 013\n00\n5.\ny2\n=\n1 x21 X22 X23\n=[1 x] [2x3 log(x4)]\n\u00a73\n1 x31 X32 \u00a333\nQ3\n3.g = 00 + 01x1 + log(02)22 + 0304\n\"Linear in theta\" means the expression\ncan separate into a matrix product of two\nX\nterms: a vector of thetas, and a\nmatrix/vector not involving thetas.\n28\nCC\n$\nBY\nINC\nSA\n2024-10-08 11:53:05\n\nV\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\nx | +\n\u20ac\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g113dfce000f_0_2330\nNarges Norouzi\nV\n..\n7\nab\nT\n2931334\nOLS Problem Formulation\nMultiple Linear Regression Model\nMean Squared Error\nGeometric Derivation\nPerformance: Residuals, Multiple R2\nOLS Properties\nMean Squared\nResiduals\nThe Bias/Intercept Term\nError\nExistence of a Unique Solution\nUniqueness of the OLS Solution\nLecture 12, Data 100 Fall 2024\n29\nCC\nO\nBY\nNC\nSA\n2024-10-08 11:53:43\n",
        "Retrieved Audio": "Factor out the Theta vector as one vector and then multiply all the weights and all of the parameters or features as a separate matrix to it.\nFactor that into a vector. Here they're factored out into a vector and option one is linear in Theta. Option 2 is also linear in Theta. Because again, it doesn't matter that for example, in this case I am, I have petted 2 and then I'm multiplying to two with X2 and X3. That doesn't matter. What is happening basically is that when you're looking at your feature 2. So this is the bias term this is.\nThis term of 10 to three times multiplied by Theta 4. So only the the third option is the only option that's not linear in feta. So kind of to set up the expectation that when we say that the model is linear in PETA, what we mean exactly is that collect everything else into a matrix, leaving alone all the tetas intact and in linear form into one vector and just.",
        "Retrieved Notes": "a = [\\theta_0, \\theta_1, \\theta_2]$\n* $\\hat{y} = \\theta_0 + 2\\theta_1 + 3\\theta_2$ is linear in theta, and we can separate it into a matrix product of two terms:\n$$\\hat{y} = \\begin{bmatrix} 1 \\space 2 \\space 3 \\end{bmatrix} \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\theta_2 \\end{bmatrix}$$\n* $\\hat{y} = \\theta_0\\theta_1 + 2\\theta_1^2 + 3log(\\theta_2)$ is *not* linear in theta, as the $\\theta_1$ term is squared, and the $\\theta_2$ term is logged. We cannot separate it into a matrix product of two terms.\n:::\n###\nnsformed `\"Age\"`. In other words, we are applying the transformation $z_i = \\log{(y_i)}$. Notice that the resulting model is still **linear in the parameters** $\\theta = [\\theta_0, \\theta_1]$. The SLR model becomes:\n$$\\hat{\\log{y}} = \\theta_0 + \\theta_1 x$$\n$$\\hat{z} = \\theta_0 + \\theta_1 x$$\nIt turns out that this linearized relationship can help us understand the underlying relationship between $x$ and $y$. If we rearrange the relationship above, we find:\n$$\\log{(y)} = \\theta_0 + \\theta_1 x$$\n$$y = e^{\\th\ntionship:\n$$\\log{(\\frac{p}{1-p})} = x^{\\top} \\theta$$\nRemember that our goal is to predict the probability of a datapoint belonging to Class 1, $p$. Let's rearrange this relationship to uncover the original relationship between $p$ and our input data, $x^{\\top}$.\n\\begin{align}\n\\log{(\\frac{p}{1-p})} &= x^T \\theta\\\\\n\\frac{p}{1-p} &= e^{x^T \\theta}\\\\\np &= (1-p)e^{x^T \\theta}\\\\\np &= e^{x^T \\theta}- p e^{x^T \\theta}\\\\\np(1 + e^{x^T \\theta}) &= e^{x^T \\theta} \\\\\np &= \\frac{e^{x^T \\theta}}{1+e^{x^T \\theta}}\\\\\np &= ",
        "Retrieved Video": "3\n=[1 x] [2x3 log(x4)]\n\u00a73\n1 x31 X32 \u00a333\nQ3\n3.g = 00 + 01x1 + log(02)22 + 0304\n\"Linear in theta\" means the expression\ncan separate into a matrix product of two\nX\nterms: a vector of thetas, and a\nmatrix/vector not involving thetas.\n28\nCC\n$\nBY\nINC\nSA\n2024-10-08 11:53:05\n\n3. \u00a7 = 00 + +1x1 + log(02) 22 + +304\nWhich of the following\nexpressions are linear in theta?\n51%\n#2931 334\n2\n25%\n3\n2024-10-08 11:51:00\n\n1 X22 X23\n93\n1 x31 X32 \u00a333\nA2\nQ3\n\"Linear in theta\" means the expression\n3.y = 00 + 01x1 + log(02)x2 + 0304\ncan separate into a matrix product of two\nterms: a vector of thetas, and a\nmatrix/vector not involving thetas.\n28\nCC\nBY\nNC\nSA\n2024-10-08 11:51:29\n",
        "Question": "On answer three, could you factor out theta of log(theta)",
        "Timestamp": 2595,
        "TA Response": "There is no way to do this\u2014 log is a non-linear function and we cannot linearly factor it. Another example of a function like this would be 2^theta, or theta^2. In all of these cases, we cannot separate them into a matrix product of a vector of thetas and a matrix/vector not involving thetas.",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, there is a reference sheet available for the course, which can be found on the course resources page. However, the context provided does not specify if this reference sheet is specifically for the midterm. You might want to check the course website or ask the instructor for clarification on whether the reference sheet is applicable to the midterm exam.",
        "Recent Audio": "error vector that you have. In a general sense, if I have an N dimensional vector, so here I have an N dimensional vector X with the values X1X2 all the way to XN. The L2 norm of this vector is going to be a square root of each one of the summation of XI is squares. OK, so this is again a square root of the summation of all of these XI squared. That I have over all of the dimensions that I have every time that I want to show L2 norm, I'll show it with this double vertical line. And then the subscript 2 is showing you that you're taking the 2nd order or the L2 norm or you're calculating the length based on L2 norm. OK, one thing that you might notice here again, try to build intuition from everything that you know and every time that you're looking at the equation or. Or some objective function. I'm talking about L2 norm here. The format of what you're seeing in the right hand side of the value for the L2 norm is kind of similar to the squared error function that you had seen before, right? So if you were thinking about X is as being errors, and if you go ahead and square the errors and sum up all of the squares of the errors, this is kind of telling you that this is kind of a ^2 of the errors. Or we can use this notation to calculate the scores.",
        "Recent Video": "Lec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\nx | +\n25 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ2oTy4GI7GDyyq1TLL59a0Q/edit#slide=id.g113dfce000f_0_2382\nM\nNarges Norouzi\n..\nO\nK\n7\n[Linear Algebra] Vector Norms and the L2 Vector Norm\nab\nT\n11\nD\nThe norm of a vector is some measure of that vector's size/length.\n2931334\nThe two norms we need to know for Data 100 are the L1 and L2 norms (sound familiar?).\nToday, we focus on L2 norm. We'll define the Li norm another day.\nL\nFor the n-dimensional vector x =\nx2\n, the L2 vector norm is\nUn\nn\n2.0\nx2=Vacy+x2+ ... +x2\n=\ni=1\n31\nCC\nBY\nNC\nSA\n2024-10-08 11:56:36\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\nC \u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ2oTy4GI7GDyyq1TLL59a0Q/edit#slide=id.g113dfce000f_0_2396\nMI\nVarges Norouzi\n..\nO\n7\n[Linear Algebra] The L2 Norm as a Measure of Length\n*\nab\nT\nThe L2 vector norm is\n2931334\nn\na generalization of the\nPythagorean theorem\n|||2=\u221ax2+x2+.+20%=\n\u2265(x2)\nIn R\n2\ninto n dimensions.\ni=1\na\nVa2+ b2\nb\n32\nBY\nNC\nSA\n2024-10-08 11:57:19\n",
        "Retrieved Audio": "So today's lecture is going to use a data set.\nInvolved lecture, but the good news is that from this point and on everything is going to get easier and it's going to be more on the applied side of thing. And then we're going to see in the second-half of DATA 100 after the midterm, we're going to see different applications or different machine learning.\nOK, let's take a look.",
        "Retrieved Notes": "s/#data-science-education)\n-   [Local Setup (Old)](https://ds100.org/fa24/resources/#local-setup-old)\n\n## Reference Sheet\n\nHere is the Spring 2024 Final [Reference Sheet](https://ds100.org/fa24/resources/assets/exams/sp24/ds100-sp24-final-ref.pdf).\n\n## Supplementary Course Notes\n\nAlongside each lecture are supplementary [Course Notes](https://ds100.org/course-notes/).\n\nLecture notes will be updated on a weekly basis, prior to the lecture. If you spot any errors or would like to suggest any changes, please e\narious concepts and skills covered in the class. Learning by reading is a key part of being a well-rounded data scientist. We will not assign mandatory reading but instead encourage you to look at these _and other_ materials. If you find something helpful, post it on EdStem, and consider contributing it to the course website.\n\nJump to:\n\n-   [Reference Sheet](https://ds100.org/fa24/resources/#reference-sheet)\n-   [Supplementary Course Notes](https://ds100.org/fa24/resources/#supplementary-course-notes)\n-   [\n\n> -   **Midterm** on **Wednesday, October 23** 7-9 PM PST.\n> -   **Final** on **Wednesday, December 18** 8-11 AM PST.\n\n**All exams must be taken in-person.** There will be no alternate exams offered.\n\n### Graduate Final Project\n\nAll students enrolled in the graduate version of the course (CS C200A or Stat C200C, i.e. Data 200; Data 200S) will be graded according to the Graduate grading scheme, which includes a team-based **Graduate Final Project** distributed in the second half of the semester. More detail",
        "Retrieved Video": ".\n.\n.\n.\n.\n.\n.\n.\n.\n.\nXn1\nIn2\nUnp\nyn\n. . .\n14\nCC\nBY\nNC\nSA\n2024-10-08 11:26:36\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nUn1\nIn2\nUnp\n. .\nyn\nFeature 2\n{x12, x22 ,... Xn2}\n14\nCC\nS\nB'\nNC\nSA\n2024-10-08 11:27:34\n\n\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\nIn1\nIn2\nXnp\nyn\n. . .\nFeature 2\n{x12, x22, ... Xn2}\n14\nCC\nS\nBY\nNC\nSA\n2024-10-08 11:27:14\n",
        "Question": "Do we get a reference sheet for the midterm ",
        "Timestamp": 2815,
        "TA Response": "Yes\u2014 it will be made available sometime next week. Keep an eye out for the midterm logistics post on Ed!",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "For synchronous lecture attendance, you need to complete at least one participation poll question during the live lecture timeslot to receive attendance credit. The grading is on a 0/1 basis, meaning you just need to submit a response to get credit, and it does not depend on the accuracy of your answer.",
        "Recent Audio": "the right hand side of the value for the L2 norm is kind of similar to the squared error function that you had seen before, right? So if you were thinking about X is as being errors, and if you go ahead and square the errors and sum up all of the squares of the errors, this is kind of telling you that this is kind of a ^2 of the errors. Or we can use this notation to calculate the scores. The summation of the squares of the errors, right? I would say summation of the squares of the errors, but let's see how we can use it in practice. So this L2 norm can be used in order to in order to again measure the length or size of a vector. And it's also kind of a generalization of Pythagorean theorem. So in Pythagorean theorem, you had that, OK, if one side is a, one side is B, this other side is going to have the length of. A ^2 + b ^2. What we also know is that we can use and generalize this notation and say that OK, if you have a vector of A&B, which means that across the X dimension you have a units of distance, across the Y dimension you have B units of distance. The length of this vector from this equation is exactly going to be a sqrt a ^2 + b ^2. So kind of it will tell you that this notion is going to is.",
        "Recent Video": "Lec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\nx | +\nC \u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g113dfce000f_0_2396\nNarges Norouzi\n..\nO\n7\n[Linear Algebra] The L2 Norm as a Measure of Length\nK\n* *\nab\nT\nD\nThe L2 vector norm is\n2931334\nn\na generalization of the\n2\nPythagorean theorem\n||x|2 = x2+ x2+ ... +x2 =\n\u2265(x2)\nIn R\ninto n dimensions.\ni=1\na\nVa2+ b2\n6.\nIt can therefore be used as a measure of length of a vector\n.\nThe vector on the right has length |v|2 = Va2 +b2\nVa+b2\n(a, b)\n5\n\u00e0\n32\nCC\nS\nBY\nNC\nSA\n2024-10-08 11:58:07\n\nLec 12 - DS100 Fa24 - Ordinary\nx\nlec12.ipynb - JupyterLab\nx | +\n>\nC \u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g258983dec17_1_19\nNarges Norouzi\nV\n...\nO\n7\n[Linear Algebra] The L2 Norm as a Measure of Distance\nab\u0131\nT\nThe L2 vector norm is\n2931334\nn\na generalization of the\n=\nIn R\n2\n21\n2\nPythagorean theorem\ninto n dimensions.\ni=1\na\nVa2+b2\nb\nIt can also be used as a measure of distance between two vectors.\nIn Rn\n.\nFor n-dimensional vectors a, b , their distance is | |a - b| |2.\na\nb\n33\nCC\nBY\nNC\nSA\n2024-10-08 11:58:17\n",
        "Retrieved Audio": "Just use this specific notation question.\nSo today's lecture is going to use a data set.\nThe output or the result?",
        "Retrieved Notes": "uestions are launched; or (2) asynchronously using a link provided after lecture.\n\n-   Lecture participation is graded using poll responses on a 0/1 basis.\n    -   Synchronous Participation: complete at least one participation poll question during the live lecture timeslot (11:00am-12:30pm, Tuesdays and Thursdays). As long as you submit a response to at least one poll question in this timeframe, you will receive synchronous attendance credit.\n    -   Asynchronous Participation: complete all participation po\nd you view the corresponding lecture as the course notes are not as comprehensive for this portion of the lecture.\n### 5. Results and Conclusions\n::: {.callout-note}\n## Driving Questions\n- How successful is the system for each goal?\n    - Accuracy/uniformity of the model\n    - Fairness and transparency that eliminates regressivity and engenders trust\n- How do you know? \n:::\nUnfortunately, it may be naive to hope that a more accurate and transparent algorithm will translate into more fair outcomes in practic\ntion. One solution is to leverage a randomized response:\nFirst, you can ask the individual to secretly flip a fair coin; you (the surveyor) *don't* know the outcome of the coin flip.\nThen, you ask them to **answer \"Yes\"** if the coin landed heads and to **answer truthfully** if the coin landed tails.\n  \nThe surveyor doesn't know if the **\"Yes\"** means that the **person cheated** or if it means that the **coin landed heads**. The individual's sensitive information remains secret. However, if the response is ",
        "Retrieved Video": ".\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nUn1\nIn2\nUnp\n. .\nyn\nFeature 2\n{x12, x22 ,... Xn2}\n14\nCC\nS\nB'\nNC\nSA\n2024-10-08 11:27:34\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ2oTy4GI7GDyyq1TLL59a0Q/edit#slide=id.g257e8642ed5_0_1\nNarges Norouzi\n..\n71\nFrom One Feature to Many Features\nab\nT\nDataset\n2931334\nfor SLR\ny 4\nX1\ny1\nX2\ny2\n. . .\n. . .\nUn\nUn\nFG |PTS\n1\n1.8\n5.3\n2 0.4\n1.7\n3 1.1\n3.2\n4\n6.0\n13.9\n5 3.4\n8.9\n13\nCC\n...\nBY\nNC\nSA\n...\n...\n2024-10-08 11:24:12\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\nC \u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g257e8642ed5_0_1\nNarges Norouzi\n..\n71\nK\nFrom One Feature to Many Features\nab\nT\nDataset\nDataset for\n2931334\nfor SLR\nConstant Model\nx\ny 4\ny\n\u20be1\ny1\ny1\ny2\ny2\n.\n.\n.\n.\n.\n.\n.\n.\n.\nUn\nyn\nyn\nFG |PTS\nPTS\n1\n1.8\n5.3\n1\n5.3\n2 0.4\n1.7\n2\n1.7\n3 1.1\n3.2\n3\n3.2\n4\n6.0\n13.9\n4\n13.9\n5 3.4\n8.9\n5\n8.9\n13\nBY\nNC\n...\nSA\n...\n...\n...\n.. .\n2024-10-08 11:24:31\n",
        "Question": "For lecture attendance is it just answering one slido question, but does it depend on accuracy?",
        "Timestamp": 2873,
        "TA Response": "Yes, answering only one Slido question counts, and no, it does not depend on accuracy.",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The context provided does not mention any upcoming polls. It discusses historical polling methods and the Gallup Poll's techniques for mitigating biases. If you are referring to a specific event or poll, please provide more details.",
        "Recent Audio": "mean of the scores of the errors. OK, from this previous slide, remember that this was the notation for me to calculate the L2 norm of a vector. So effectively what you're seeing here inside the summation, I can just replicate this summation using this L2 norm part that I had from before. So again, assuming that what I'm doing in the in terms of calculating why I'm by is minus YYI hats, it's equal to me saying that I have a vector of all of the Y values, grand first values, I have a vector of all of the prediction values. Go ahead and calculate. To norm of the this this delta vector. So this is my delta vector. Go ahead and calculate the L2 norm. But because this L2 norm has this square root and we want to get rid of that, we will square it one more time. So let me clean this up and let's take a look at it one more time. So again, I have all the Y is, I have all of the Yi hats. I need to calculate residuals and square the residuals. The place that I can get this summation is exactly going to be this L2 norm of this vector. And then is square this L2 norm because I don't have a square root in this case. So then again, what's happening is that this is going to be this summation right here is going to be equal to me taking L2 norm of Y minus.",
        "Recent Video": "Lec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\nC\n25 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g113dfce000f_0_2478\nVarges Norouzi\n..\nO\n7\nK\nMean Squared Error with L2 Norms\nab\nT\nWe can rewrite mean squared error as a squared L2 norm:\n2931334\nR(O) = 1 E(yi - gi)2\nn\nn\n=\ni=1\ni=1\n7\n== (\\\\Y_}||2)2\n1\nn\n34\nCC\nBY\nNC\nSA\n2024-10-08 12:01:14\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\nX\n+\n->\nC\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ2oTy4GI7GDyyq1TLL59a0Q/edit#slide=id.g113dfce000f_0_2478\nNarges Norouzi\n..\nO\n7\nK\nMean Squared Error with L2 Norms\nab\nT\nWe can rewrite mean squared error as a squared L2 norm:\n2931334\nn\nR(O) = \" E(i - gi)2\nn\n18\n= 1\n2\ni=1\ni=1\n== (|\\Y-}|2)2\n1\nn\n34\nCC\nBY\nNC\nSA\n2024-10-08 12:01:19\n",
        "Retrieved Audio": "The output or the result?\nSo.\nOK, let's take a look.",
        "Retrieved Notes": "y had a dismal response rate (about 24\\%); who knows how the other non-respondents would have polled? The *Digest* folded just 18 months after this disaster.\nAt the same time, George Gallup, a rising statistician, also made predictions about the 1936 elections. Despite having a smaller sample size of \u00e2\u0080\u009conly\u00e2\u0080\u009d 50,000 (this is still more than necessary; more when we cover the Central Limit Theorem), his estimate that 56\\% of voters would choose Roosevelt was much closer to the actual result (61\\%). Gallup\n**\"No\"**, then the surveyor knows the **individual didn't cheat**. We assume the individual is comfortable revealing this information. \nGenerally, we can assume that the coin lands heads 50% of the time, masking the remaining 50% of the \"No\" answers. We can therefore **double** the proportion of \"No\" answers to estimate the **true** fraction of \"No\" answers.\n**Election Polls**\nToday, the *Gallup Poll* is one of the leading polls for election results. The many sources of biases -- who responds to polls? Do v\noters tell the truth? How can we predict turnout? -- still remain, but the *Gallup Poll* uses several tactics to mitigate them. Within their sampling frame of \"civilian, non-institutionalized population\u00e2\u0080\u009d of adults in telephone households in continental U.S., they use random digit dialing to include both listed/unlisted phone numbers and to avoid selection bias. Additionally, they use a within-household selection process to randomly select households with one or more adults. If no one answers, re-call mul",
        "Retrieved Video": ".\n.\n.\n.\n.\n.\n.\n.\n.\n.\nXn1\nIn2\nUnp\nyn\n. . .\n14\nCC\nBY\nNC\nSA\n2024-10-08 11:26:36\n\n\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\nIn1\nIn2\nXnp\nyn\n. . .\nFeature 2\n{x12, x22, ... Xn2}\n14\nCC\nS\nBY\nNC\nSA\n2024-10-08 11:27:14\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nUn1\nIn2\nUnp\n. .\nyn\nFeature 2\n{x12, x22 ,... Xn2}\n14\nCC\nS\nB'\nNC\nSA\n2024-10-08 11:27:34\n",
        "Question": "Will there be another poll?",
        "Timestamp": 3097,
        "TA Response": "Yes",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, options B and C essentially describe the same concept in the context of the Ordinary Least Squares (OLS) problem. Both involve minimizing the difference between the true values and the predicted values. Option B refers to minimizing the distance between these values, while option C describes minimizing the length of the residual vector, which is the difference between the true and predicted values. Both aim to ensure that predictions are as close as possible to the actual data points.",
        "Recent Audio": "making predictions, what are we exactly doing in the OLS problem? How do we interpret this objective function? Let's take a look at those options that we have, minimizing mean squared error, minimizing the distance, minimizing. So on and so forth, let me know. OK, I'm going to stay on this slide so that you can still respond and just go through each one of these options. The first option says that minimize the mean squared error for the linear model Y had equal to X times Theta. That was the definition. We said that ordinary release the score is is the is the is the model that is multiple linear regression, which would derive the equation to be Y hat equal to design matrix times Theta. And then we know that the object. Function is going to be mini squared error, so a is correct. What about B? So in B we're saying that minimizing the distance between the predicted values. Between the true and the predictive values, meaning that if you have a vector of all of the predictions and you have a vector of all of the ground truth values, you you might want to make sure that these two vectors are as close, as close to each other as possible, which is exactly what we're trying to do, irrespective of the model. So in all of the models that you'll be training, you want to make sure that predictions are as close as possible to the ground truth values. That is correct.",
        "Recent Video": "V\nLec 12 - DS100 Fa24 - Ordinary X\nlec12.ipynb - JupyterLab\nX\n+\nC\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g1163459c7f0_0_481\nNarges Norouzi\n..\nO\n7\n:= Active poll\nSynchronous\nab\nT\n0 8\nHow should we interpret the\nOLS problem?\n\u00b7 A\nA. Minimize the mean squared error for the linear model Y = X0\n.\nB\nB. Minimize the distance\nbetween true and predicted values Y and\n91 - 91\nC. Minimize the length of the residual vector, e = Y - \u00dd =\n92 - 92\n\u00b7 C\n...\nJoin at\nD. All of the above\nLyn-Un\nslido.com\nE. Something else\n\u00b7 D\n#2931 334\n.\nE\n2024-10-08 12:03:53\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\nC\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ2oTy4GI7GDyyq1TLL59a0Q/edit#slide=id.SLIDES_API380301065_0\nNarges Norouzi\n..\n7\n:= Active poll\nSynchronous\nab\nT\n213 83\nD\nHow should we interpret the\nOLS problem?\n\u00b7 A\nA. Minimize the mean squared error for the linear model Y = X0\n\u00b7 B\nB. Minimize the distance\nbetween true and predicted values YY and\n91 - 91\nC. Minimize the length of the residual vector, e = YY - Y =\n92 - 92\n\u00b7 C\n...\nJoin at\nD. All of the above\nLyn-Un\nslido.com\nE. Something else\n\u00b7 D\n#2931 334\n.\nE\n2024-10-08 12:04:22\n",
        "Retrieved Audio": "Just use this specific notation question.\nFunction is going to be mini squared error, so a is correct. What about B? So in B we're saying that minimizing the distance between the predicted values.\nPlus U2 times B2 plus U 3 * b three. Why am I writing it? But this notation is because I want you to notice that every time you've seen that you're calculating a summation of a stuff that they are the same set of elements that are multiplied together but with different indices. You can think about two vectors being kind of that you're calculating the dot product between those two vectors. And this is kind of an intuition.",
        "Retrieved Notes": "$x$,$y$) pairs. Additionally, causal questions implicitly involve **counterfactuals**, events that didn't happen. For example, we could ask, **would** the **same** breastfed babies have been less healthy **if** they hadn\u00e2\u0080\u0099t been breastfed? Explanation 1 from above implies they would be, but explanations 2 and 3 do not. \n### Confounders\nLet T represent a treatment (for example, alcohol use) and Y represent an outcome (for example, lung cancer).\n<img src=\"images/confounder.png\" alt='confounder' width='600'>\ner><img src = \"images/vis_9.png\" width=300px></img></center>\n<br />\nLet's frame this problem through the lens of the data science lifecycle.\n<center><img src = \"images/data_life_cycle.PNG\" width=300px></img></center>\n### 1. Question/Problem Formulation\n::: {.callout-note}\n#### Driving Questions\n- What do we want to know?\n- What problems are we trying to solve?\n- What are the hypotheses we want to test?\n- What are our metrics for success? \n:::\nThe old system was unfair because it was systemically inaccurate;\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of ",
        "Retrieved Video": "D. All of the above\nLyn-Un\nslido.com\nE. Something else\n\u00b7 D\n#2931 334\n.\nE\n2024-10-08 12:04:22\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nUn1\nIn2\nUnp\n. .\nyn\nFeature 2\n{x12, x22 ,... Xn2}\n14\nCC\nS\nB'\nNC\nSA\n2024-10-08 11:27:34\n\nf the above\nLyn-Un\nslido.com\nE. Something else\n\u00b7 D\n#2931 334\n.\nE\n2024-10-08 12:03:53\n",
        "Question": "Isn't B and C the same thing for this slide question?",
        "Timestamp": 3308,
        "TA Response": "Yes, they're the same! B is saying to minimize [y1, y2, ... , yn] - [y1_hat, y2_hat, ... , yn_hat], and C is saying to minimize [y1 - y1_hat, y2 - y2_hat, ... , yn - yn_hat]. From basic vector operations we know that these are exactly the same thing.",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The span of a matrix \\(\\mathbb{X}\\) is a subspace of \\(\\mathbb{R}^n\\) formed by all possible linear combinations of its columns. If \\(\\mathbb{X}\\) has fewer columns than the dimension \\(n\\), it cannot span the entire \\(\\mathbb{R}^n\\) because it lacks the necessary degrees of freedom to reach every point in that space. In the example from class, the vectors were in \\(\\mathbb{R}^2\\), and thus they could not span all of \\(\\mathbb{R}^3\\) because they were confined to a plane within the three-dimensional space. This illustrates that the span is limited by the number of independent vectors available.",
        "Recent Audio": "this, this Z dimension because they don't have Z dimension in a sense. So you can imagine that what's happening is that this kind of X that we're creating, if you assume that the matrix that we're dealing with is my design matrix, we're thinking about all the possible places in the span of X that we can achieve or in the span of X that we can go with a weighted combination of columns of that. Matrix and if each one of these columns are north dimensional, these are in RN and this is also in RN. Again, you can imagine that the span of X is going to be a subset of RN and not including the entire RN. In the same way that I was not able to achieve any vertical point or any vertical kind of vector here it is a subspace or it's a subset of our tree and not the entire artery that I have. So why is it helpful in our case when we're talking about how the model is going to do production? It is helpful because if you think about how it was getting all the predictions, there was one way that you would think about the fact that I'm getting all the predictions by multiplying these rows by this column vector. There's also another way that you can think about me calculating each one of these or getting all of these productions. By waiting or by multiplying this weight factor to each one of these columns.",
        "Recent Video": "Lec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\nC \u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g24af9f57044_0_15\nNarges Norouzi\nIE\n. .\nab\u0131>\nT\n[Linear Algebra] Matrix-Vector Multiplication\nO\n2931334\nApproach 1: So far, we've thought of our model as horizontally stacked predictions per datapoint:\nT\nn\nT\np+1\n2\nH\n.. .\n.T\n1\nn\nApproach 2: However, it is helpful sometimes to think of matrix-vector multiplication as performed\nby columns. We can also think of y as a linear combination of feature vectors, scaled by\nparameters.\nn\nn\np+1\nX:1 X.2\n=\n01 X:1 + 02 X ., 2\n1\n44\np+1\nCC\nBY\nINC\nSA\n2024-10-08 12:13:24\n\nLec 12 - DS100 Fa24 - Ordinary\n\u00d7\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\nC\n25 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g24af9f57044_0_15\nNarges Norouzi\nO\nIE\n. .\n7\nK\n[Linear Algebra] Matrix-Vector Multiplication\nab\nT\n1\n2931334\nApproach 1: So far, we've thought of our model as horizontally stacked predictions per datapoint:\nT\nn\nT\np+1\nH\n.. .\n.T\n1\nn\nApproach 2: However, it is helpful sometimes to think of matrix-vector multiplication as performed\nby columns. We can also think of y as a linear combination of feature vectors, scaled by\nparameters.\nn\nn\np+1\nX,1 X ., 2\n01 X: 1 + 02 X:2\n1\n44\n1\np+1\nCC\nBY\nNC\nSA\n2024-10-08 12:13:34\n",
        "Retrieved Audio": "Right, I was showing you this example and my subspace or my span was entirely this floor. I would not be able to reach any point above this floor exactly with these two vectors. So you can imagine that in three-dimensional space, the span of X is a subspace of R3 because I was not able to, for example, find out any kind of. If you're assuming that they're flat, there's no way that I can calculate a weighted combination of these two.\nMatrix and if each one of these columns are north dimensional, these are in RN and this is also in RN. Again, you can imagine that the span of X is going to be a subset of RN and not including the entire RN. In the same way that I was not able to achieve any vertical point or any vertical kind of vector here it is a subspace or it's a subset of our tree and not the entire artery that I have.\nBut what we also call is we call a span of a matrix. A span of a matrix means that assume that each one of the columns are vectors and let's decide about what is the subspace or what is the area that is achievable by the weighted linear combination of all of the columns of that matrix. It's the kind of a more advanced version or a generalized format of what I was showing you here with two vectors in three-dimensional space.",
        "Retrieved Notes": " that the columns of $\\mathbb{X}$ define a subspace of $\\mathbb{R}^n$, where each point in the subspace can be reached by a linear combination of $\\mathbb{X}$'s columns. The prediction vector $\\mathbb{\\hat{Y}}$ lies somewhere in this subspace.\n<div align=\"middle\">\n  <table style=\"width:100%\">\n    <tr align=\"center\">\n      <td>\n        <img src=\"images/span.png\" alt='span' width='600'>\n      </td>\n    </tr>\n  </table>\n</div>\nExamining this diagram, we find a problem. The vector of true values, $\\mathbb{Y}$, \nerivation\n::: {.callout-tip collaps=\"false\"}\n### Linear Algebra: Span \nRecall that the **span** or **column space** of a matrix $\\mathbb{X}$ (denoted $span(\\mathbb{X})$) is the set of all possible linear combinations of the matrix's columns. In other words, the span represents every point in space that could possibly be reached by adding and scaling some combination of the matrix columns. Additionally, if each column of $\\mathbb{X}$ has length $n$, $span(\\mathbb{X})$ is a subspace of $\\mathbb{R}^{n}$.\n:::\n:\nw approach is useful because it allows us to take advantage of the properties of linear combinations.\nBecause the prediction vector, $\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$, is a **linear combination** of the columns of $\\mathbb{X}$, we know that the **predictions are contained in the span of $\\mathbb{X}$**. That is, we know that $\\mathbb{\\hat{Y}} \\in \\text{Span}(\\mathbb{X})$.\nThe diagram below is a simplified view of $\\text{Span}(\\mathbb{X})$, assuming that each column of $\\mathbb{X}$ has length $n$. Notice",
        "Retrieved Video": "Lec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\nx | +\n->\nC\nNarges Norouzi\nO\n7\n[Linear Algebra] Span\nab\nT\nThe set of all possible linear combinations of the columns of X\n2931334\nis called the span of the columns of X (denoted span(X)),\nalso called the column space.\nIntuitively, this is all of the vectors\nyou can \"reach\" using the columns of X.\nIf each column of X has length n,\nspan (X) is a subspace of Rn.\nXX: 2\nSubspace of R\u2122 spanned by XX\n41\nCC\nBY\nINC\nSA\n2024-10-08 12:09:43\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nUn1\nIn2\nUnp\n. .\nyn\nFeature 2\n{x12, x22 ,... Xn2}\n14\nCC\nS\nB'\nNC\nSA\n2024-10-08 11:27:34\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n->\nC \u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ2oTy4GI7GDyyq1TLL59a0Q/edit#slide=id.g113dfce000f_0_2396\nMI\nVarges Norouzi\n..\nO\n7\n[Linear Algebra] The L2 Norm as a Measure of Length\n*\nab\nT\nThe L2 vector norm is\n2931334\nn\na generalization of the\nPythagorean theorem\n|||2=\u221ax2+x2+.+20%=\n\u2265(x2)\nIn R\n2\ninto n dimensions.\ni=1\na\nVa2+ b2\nb\n32\nBY\nNC\nSA\n2024-10-08 11:57:19\n",
        "Question": "If each column of X has length n, why can't span(X) = Rn. For the example in class the vectors were in R2 so they could not span all of R3.",
        "Timestamp": 3789,
        "TA Response": "In order to have span(X) = R^n, X must have n linearly independent columns, each of which has length n. If you are familiar with the term \"rank\", this means X would have to be an nxn full rank matrix in order to span R^n. For the purpose of OLS, we operate under the assumption that X does not have this property and that span(X) is not equal to R^n. Otherwise, OLS would become trivial, since we would be able to construct our exact Y vector in span(X) and we would not need to find an estimation.",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, as you add more features to your dataset, you effectively increase the dimensionality of the space in which your data resides. Each feature can be thought of as a dimension, and the span of these features represents the space that can be covered by linear combinations of these feature vectors. In the context of linear algebra, having more features means you have more column vectors, which can potentially span a larger space. However, the actual span depends on the linear independence of these vectors.",
        "Recent Audio": "you have Y, you can solve this set of equations that you have right here in order to find out what is the optimum Theta has values. But we'll take it one one step further. We would say that what if I multiply both sides by X transpose times X inverse? If I multiply both sides by X transpose times X inverse, then on the left hand side, I am going to be left with only teta hat. So this is going to be. Hat equal to X transpose times X, inverse times X transpose times Y And this is the final solution for how to arrive at the optimum set of values in the teta vector having had the design matrix X and then having had your target values Y vector. And this is a closed form solution for it. And this closed form solution exists if and only if X transpose times X is invertible. And we're going to talk about what is the condition that X transpose times X is going to be invertible. If it's not invertible, then in that case, we can't actually have this term and we can directly solve it. It means that there's not going to be a unique solution, but there will be a solution. So this is kind of the clean version of this. This is the final kind of last piece of this linear regression or multiple linear regression that we wanted to see. That deserves one final slide or one separate slide by itself.",
        "Recent Video": "v\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\nx | +\n->\nC\nNarges Norouzi\nO\n7\nab\nT\n2931334\n1\n=\nXX\n-1\nXY\nT\nThis result is so important that it deserves its own slide.\nIt is the least squares estimate and the solution to the normal equation XTX = XTY .\n56\nCC\n$\nBY\nNC\nSA\n2024-10-08 12:25:10\n\nV\nLec 12 - DS100 Fa24 - Ordinary\nx\nlec12.ipynb - JupyterLab\nx | +\nC\n\u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g116071039b0_0_12\nNarges Norouzi\nO\n...\n7\nab\nT\n1\n2931334\nOLS Problem Formulation\nMultiple Linear Regression Model\nMean Squared Error\nGeometric Derivation\nPerformance: Residuals, Multiple R2\nOLS Properties\nResiduals\nPerformance\nThe Bias/Intercept Term\nExistence of a Unique Solution\nUniqueness of the OLS Solution\nLecture 12, Data 100 Fall 2024\n58\nCC\n$\nO\nBY\nNC\nSA\n2024-10-08 12:25:19\n",
        "Retrieved Audio": "As the size of the data set grows and as the number of features also grows, then the complexity of this model is going to grow and we're going to be in this higher dimensional space, which is hard to directly understand what's happening in it. So the last kind of reminder from linear algebra is spent when I say I want to calculate a span of two.\nThat are applied to the column vectors and that's the definition of a span of X. So exactly what's happening when we're undoing the prediction is that I'm living in this N dimensional space. I have multiple different features. So this is feature one, for example, that is north dimensional. So for each one of the data points that you have, you have do you have a value for feature one? And this is for example your feature 2 and then so on and so forth. You can think about this is kind of.\nferent features.",
        "Retrieved Notes": "e problem is that many datasets come with more than 5 features \u00e2\u0080\u0094\u00e2\u0080\u0094 hundreds, even. Is it still possible to visualize all those features? \n## Dimensionality\nSuppose we have a dataset of:\n- $N$ observations (datapoints/rows)\n- $d$ attributes (features/columns)\nLet's \"rename\" this in terms of linear algebra so that we can be more clear with our wording. Using linear algebra, we can view our matrix as: \n- $N$ row vectors in a $d$-Dimensional space, OR\n- $d$ column vectors in an $N$-Dimensions space\nThe **i\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of \nure ($\\hat{y}_i = \\theta_0 + \\theta_1 x_i$) or zero features ($\\hat{y}_i = \\theta_0$). As data scientists, we usually have access to datasets containing *many* features. To make the best models we can, it will be beneficial to consider all of the variables available to us as inputs to a model, rather than just one. In today's lecture, we'll introduce **multiple linear regression** as a framework to incorporate multiple features into a model. We will also learn how to accelerate the modeling process \u00e2\u0080\u0093 spe",
        "Retrieved Video": "\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\nIn1\nIn2\nXnp\nyn\n. . .\nFeature 2\n{x12, x22, ... Xn2}\n14\nCC\nS\nBY\nNC\nSA\n2024-10-08 11:27:14\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\n\u00d7\n+\n25 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ2oTy4GI7GDyyq1TLL59a0Q/edit#slide=id.g257e8642ed5_0_1\nNarges Norouzi\n..\nO\nV\n7\nFrom One Feature to Many Features\n*\nab\nT\n2931334\n13\nCC\nBY\nNC\nSA\n2024-10-08 11:23:53\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nUn1\nIn2\nUnp\n. .\nyn\nFeature 2\n{x12, x22 ,... Xn2}\n14\nCC\nS\nB'\nNC\nSA\n2024-10-08 11:27:34\n",
        "Question": "The more features we have, we have more spans?",
        "Timestamp": 4492,
        "TA Response": "Good question! It depends\u2014 if the features you add are linearly independent from your existing features, then you will increase span(X). If they are linearly dependent, then span(X) stays the same.",
        "Lecture": 12
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, the lecture series will cover feature engineering, including how to choose features and interaction terms. This will involve understanding how to combine features, such as using the product of two features as a new feature, as mentioned in the audio. The interpretation of these features and their impact on model performance will also be discussed, particularly in the context of predictive modeling and understanding the variance captured by the model.",
        "Recent Audio": "that talks about what is the variance of the fitted values divided by the variance of the ground truth values. In essence, what it does is it will tell you that what percentage, what, what proportion of the variance in the underlying data are you capturing. So we want to make sure that we capture as much as possible of the underlying variance in the data so that we modeled everything and all the pattern that happened in the data. We can actually go ahead. And if you take a look at the demo notebook, what you will see is that if I go ahead and just have a model that is a simple linear regression that just does the prediction based on the ASD feature, the coefficient of determination is going to be .45. And as you keep adding features, for example, here I'm adding features of ASD and I'm using features of ASD and three PA, then the coefficient of determination is going to get to go higher. If you throw in all of those 3 features together, the coefficient of determination is going to be very close to 1, meaning that you explained the entire variance that you had in the entire data set that you have had. So there are some properties of this OLS or, or this model that we derived today. They're going to be simple. Some of them you're going to prove in the homework, but effectively all of them are going to be derived from the fact that.",
        "Recent Video": "Lec 12 - DS100 Fa24 - Ordinary\n\u00d7\nlec12.ipynb - JupyterLab\nx | +\n->\nC \u00ba5 docs.google.com/presentation/d/1UDm7Hwc4d1ihTnO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g1163459c7f0_0_131\nNarges Norouzi\nO\n..\n7\nab\nResidual Properties\nT\nWhen using the optimal parameter vector, our residuals e = YY - X\u00d4 are\n2931334\northogonal to span(X) .\nProof: First line of our OLS estimate proof (slide).\n67\nCC\nBY\nNC\nSA\n2024-10-08 12:29:05\n\nLec 12 - DS100 Fa24 - Ordinary\nX\nlec12.ipynb - JupyterLab\nx | +\nC\n25 docs.google.com/presentation/d/1UDm7Hwc4d1ihTO_iGfJQ20Ty4GI7GDyyq1TLL59a0Q/edit#slide=id.g1163459c7f0_0_131\nNarges Norouzi\nO\n..\n7\nResidual Properties\nab\nT\nA\nWhen using the optimal parameter vector, our residuals e = Y - X\u00d4 are\n2931334\northogonal to span(X) .\nProof: First line of our OLS estimate proof (slide).\nFor all linear models:\nSince our predicted response \u00a7 is in span(X) by definition, \u0176 e = 0 , and hence it is orthogonal\nto the residuals.\n67\nCC\nBY\nNC\nSA\n2024-10-08 12:29:10\n",
        "Retrieved Audio": "One, and this is feature 2, it just happens that you did feature engineering that you're gonna see I think next lecture or two lectures from now that we did feature engineering and we decided that instead of using X2 and X3 separately, I'm gonna use the product of those two terms together as a feature. So the features can be non-linear, but the parameter vector should stay intact and should stay linear and.\nTwo or three specific kind of features, but the notation that we'll be using is going to be generalizable that you can throw in any number of features that you have and any number of rows that you have and then you will be able to do the modeling. And then me escort error is also going to be clear because you have target values, you have predictions, you can calculate maintenance scored error between the two. Bless you. So what we will do as this as the turn step is going to be.\nSo that's kind of how we should think about how we're combining all of those features, four different observations in order to get all the predictions that we have. So I'm just going to move this bottom piece at the at the top and the next slide. And then we're going to take a look at how we can summarize it. But before I start summarizing it, I'm going to do a quick reminder of a dot product and what dot product needs.",
        "Retrieved Notes": "\nOur first step is choosing a model: defining the mathematical rule that describes the relationship between the features, $x_i$, and predictions $\\hat{y}_i$. \nIn [Data 8](https://inferentialthinking.com/chapters/15/4/Least_Squares_Regression.html), you learned about the **Simple Linear Regression (SLR) model**. You learned that the model takes the form:\n$$\\hat{y}_i = a + bx_i$$\nIn Data 100, we'll use slightly different notation: we will replace $a$ with $\\theta_0$ and $b$ with $\\theta_1$. This will allow us\nve looked into the early stages of the data science lifecycle, focusing on the programming tools, visualization techniques, and data cleaning methods needed for data analysis.\nThis lecture marks a shift in focus. We will move away from examining datasets to actually *using* our data to better understand the world. Specifically, the next sequence of lectures will explore predictive modeling: generating models to make some predictions about the world around us. In this lecture, we'll introduce the conceptual \n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000::: {.callout-note collapse=\"false\"}\n## Learning Outcomes\n* Define a random variable in terms of its distribution\n* Compute the expectation and variance of a random variable\n* Gain familiarity with the Bernoulli and binomial random variables\n:::\nIn the past few lectures, we've examined the role of complexity in influencing model performance. We've considered model complexity in the context of ",
        "Retrieved Video": "\n.\n.\n.\n.\n..\n.\n.\n.\n.\n.\nIn1\nIn2\nXnp\nyn\n. . .\nFeature 2\n{x12, x22, ... Xn2}\n14\nCC\nS\nBY\nNC\nSA\n2024-10-08 11:27:14\n\nope, but\nhere's a link if you're interested).\nInstead, we will derive @ using a\n4. Evaluate model\nVisualize,\ngeometric argument.\nperformance\nRoot MSE\nMultiple R2\n40\nCC\nBY\nNC\nSA\n2024-10-08 12:08:46\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nUn1\nIn2\nUnp\n. .\nyn\nFeature 2\n{x12, x22 ,... Xn2}\n14\nCC\nS\nB'\nNC\nSA\n2024-10-08 11:27:34\n",
        "Question": "are we going to go into how we choose features, interaction terms, interpretation etc",
        "Timestamp": 4714,
        "TA Response": "Yes, lecture 14 is about feature engineering and your first project will also give you a lot of practice with this!",
        "Lecture": 12
    }
]