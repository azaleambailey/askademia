[
    {
        "Conversation History": "\t",
        "GPT Response": "The lecture recordings, slides, activities, and examples will be uploaded to the course website within 24 hours of the lecture. You can access these materials to catch up on anything you missed during the Zoom webinar.",
        "Recent Audio": "curious what's probably A4 blue, 2 green, and one red, I still compute these probabilities, but I add this multinomial coefficient which lets me calculate the probability of just seeing this cluster of marbles. I don't care which order I sampled them in. This kind of multinomial analysis allows us to reason about the kinds of things we could generate directly from our population if we knew these proportions. And we can work backwards if we see these things. What's something about our what are the characteristics of our population, Right. You need to know this, I think, for upcoming assignments and build it in so you'd understand this expression here. We will give it to you. So we'll give you something like this if PS are my proportions that I know if I had them for my population and KS are my counts in my sample. And is my population size that I can compute the the probability of seeing a sample of those proportions in it in my sample? All right, so this is a multinomial. All right, we made it through the main concepts in lecture, which is great. So important summary again for everyone, you need to understand these different kinds of sampling strategies, the sample frame, the population, the sampling process and we're going to have samples and so on. And some of the code we use in today's lecture are like sampling from a data frame. You'll use an upcoming assignments. See you all next Tuesday. All right, one second.",
        "Recent Video": "x\nJoey Gonzalez (he, him, his)\n2024-09-26 12:29:44\n\nV\nLec 10 - DS100 Fa24 - Modeling\nX\nlec10.ipynb - JupyterLab\nx +\n->\nC\n5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00ils/edit#slide=id.g10ed28599e7_0_0\nNarges Norouzi\n...\nO\n0\nK\nH\nab\nT\n1380664\nLECTURE 10\nIntroduction to Modeling, SLR\nUnderstanding the usefulness of models and the simple linear regression model\nData 100/Data 200, Fall 2024 @ UC Berkeley\nNarges Norouzi and Joseph E. Gonzalez\nContent credit: Acknowledgments\n2\nCC\nHY\nNC\nA\n2024-10-01 11:10:42\n",
        "Retrieved Audio": "",
        "Retrieved Notes": "lectures** held on Tuesdays and Thursdays, 11:00am-12:30pm, hybrid: in-person in Wheeler 150 and broadcasted live on Zoom. All session recordings, slides, activities, and examples will be uploaded to the course website within 24 hours of the lecture.\n\n**Lecture participation**: While synchronous lecture attendance (in-person or through Zoom) is not mandatory, we expect you to \u00e2\u0080\u009cparticipate\u00e2\u0080\u009d in lecture by answering lecture poll questions in one of two ways: (1) synchronously during the lecture as poll q\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
        "Retrieved Video": "V\nLec 10 - DS100 Fa24 - Modeling\nX\nlec10.ipynb - JupyterLab\nx +\n->\nC\n5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00ils/edit#slide=id.g10ed28599e7_0_0\nNarges Norouzi\n...\nO\n0\nK\nH\nab\nT\n1380664\nLECTURE 10\nIntroduction to Modeling, SLR\nUnderstanding the usefulness of models and the simple linear regression model\nData 100/Data 200, Fall 2024 @ UC Berkeley\nNarges Norouzi and Joseph E. Gonzalez\nContent credit: Acknowledgments\n2\nCC\nHY\nNC\nA\n2024-10-01 11:10:42\n\nV\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\nC\n5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g10ed28599e7_0_0\nNarges Norouzi\n...\nO\nK\nH\nab\nT\n1380664\nLECTURE 10\nIntroduction to Modeling, SLR\nUnderstanding the usefulness of models and the simple linear regression model\nData 100/Data 200, Fall 2024 @ UC Berkeley\nNarges Norouzi and Joseph E. Gonzalez\nContent credit: Acknowledgments\n2\n2024-10-01 11:10:44\n",
        "Question": "The zoom webinar seems to have ended.",
        "Timestamp": 0,
        "TA Response": "restarted again",
        "Lecture": 10
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Yes, supplementary course notes are available alongside each lecture. You can find them on the course website, and they are updated on a weekly basis prior to the lecture. If you spot any errors or have suggestions, you are encouraged to provide feedback.",
        "Recent Audio": "this if PS are my proportions that I know if I had them for my population and KS are my counts in my sample. And is my population size that I can compute the the probability of seeing a sample of those proportions in it in my sample? All right, so this is a multinomial. All right, we made it through the main concepts in lecture, which is great. So important summary again for everyone, you need to understand these different kinds of sampling strategies, the sample frame, the population, the sampling process and we're going to have samples and so on. And some of the code we use in today's lecture are like sampling from a data frame. You'll use an upcoming assignments. See you all next Tuesday. All right, one second. All right, welcome Data 100. This is going to be a very fun lecture. So just before I start, it looks like that our Zoom has an issue that it stops itself before the start of the lecture. So we're trying to get that fixed. So thank you for all of those who are online. And they rejoined the Zumba winner again, so kind of telling you. Where we're at today and where we're going in the next couple of lectures is that we're starting actually digging deeper into the modeling phase of DATA 100. So it's going to be really exciting. I'm going to try to do a lot of math derivation and also building different models with you and also evaluating.",
        "Recent Video": "V\nLec 10 - DS100 Fa24 - Modeling\nX\nlec10.ipynb - JupyterLab\nx +\n->\nC\n5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00ils/edit#slide=id.g10ed28599e7_0_0\nNarges Norouzi\n...\nO\n0\nK\nH\nab\nT\n1380664\nLECTURE 10\nIntroduction to Modeling, SLR\nUnderstanding the usefulness of models and the simple linear regression model\nData 100/Data 200, Fall 2024 @ UC Berkeley\nNarges Norouzi and Joseph E. Gonzalez\nContent credit: Acknowledgments\n2\nCC\nHY\nNC\nA\n2024-10-01 11:10:42\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nX\n+\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk|4RvnqKxYS00i6ls/edit#slide=id.g11197671c7e_0_209\nNarges Norouzi\n...\nO\nO\nPlan for Next Few Lectures: Modeling\nH\nab\nT\n1380664\nAsk a question\nObtain data\nUnderstand the\nUnderstand\nworld\nthe data\nReports, Decisions,\nand Solutions\n(today)\nModeling I:\nModeling II:\nModeling III:\nIntro to Modeling, Simple\nDifferent models, loss\nMultiple Linear\nLinear Regression\nfunctions, linearization\nRegression\n3\nNC\n2024-10-01 11:11:16\n",
        "Retrieved Audio": "Where we're at today and where we're going in the next couple of lectures is that we're starting.\nWhere we're at today and where we're going in the next couple of lectures is that we're starting actually digging deeper into the modeling phase of DATA 100. So it's going to be really exciting. I'm going to try to do a lot of math derivation and also building different models with you and also evaluating.\nActually digging deeper into the modeling phase of Data 100. So it's going to be really exciting. I'm going to try to do a lot of math derivation and also building different models with you and also evaluating different the performance of different models with you throughout today's lecture and also in the next two lectures to come. So this is where we are. We're going to start by reviewing what you've seen in Data 8. We're going to start by taking a look at what was the simple linear.",
        "Retrieved Notes": "section or on their own time.** The content covered in these sections is designed to solidify understanding of key lecture concepts and prepare students for homework assignments. It is to your benefit to actively participate in all discussions.\n\nEach week, we will provide a conceptual review covering the topics pertinent to the discussion worksheet at [this Youtube playlist](https://www.youtube.com/playlist?list=PLQCcNQgUcDfplNp0itu2QqVjoDE9u5iow). Slides for the content review are linked [here](https://dri\nlectures** held on Tuesdays and Thursdays, 11:00am-12:30pm, hybrid: in-person in Wheeler 150 and broadcasted live on Zoom. All session recordings, slides, activities, and examples will be uploaded to the course website within 24 hours of the lecture.\n\n**Lecture participation**: While synchronous lecture attendance (in-person or through Zoom) is not mandatory, we expect you to \u00e2\u0080\u009cparticipate\u00e2\u0080\u009d in lecture by answering lecture poll questions in one of two ways: (1) synchronously during the lecture as poll q\ns/#data-science-education)\n-   [Local Setup (Old)](https://ds100.org/fa24/resources/#local-setup-old)\n\n## Reference Sheet\n\nHere is the Spring 2024 Final [Reference Sheet](https://ds100.org/fa24/resources/assets/exams/sp24/ds100-sp24-final-ref.pdf).\n\n## Supplementary Course Notes\n\nAlongside each lecture are supplementary [Course Notes](https://ds100.org/course-notes/).\n\nLecture notes will be updated on a weekly basis, prior to the lecture. If you spot any errors or would like to suggest any changes, please e",
        "Retrieved Video": "V\nLec 10 - DS100 Fa24 - Modeling\nX\nlec10.ipynb - JupyterLab\nx +\n->\nC\n5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00ils/edit#slide=id.g10ed28599e7_0_0\nNarges Norouzi\n...\nO\n0\nK\nH\nab\nT\n1380664\nLECTURE 10\nIntroduction to Modeling, SLR\nUnderstanding the usefulness of models and the simple linear regression model\nData 100/Data 200, Fall 2024 @ UC Berkeley\nNarges Norouzi and Joseph E. Gonzalez\nContent credit: Acknowledgments\n2\nCC\nHY\nNC\nA\n2024-10-01 11:10:42\n\nV\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\nC\n5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g10ed28599e7_0_0\nNarges Norouzi\n...\nO\nK\nH\nab\nT\n1380664\nLECTURE 10\nIntroduction to Modeling, SLR\nUnderstanding the usefulness of models and the simple linear regression model\nData 100/Data 200, Fall 2024 @ UC Berkeley\nNarges Norouzi and Joseph E. Gonzalez\nContent credit: Acknowledgments\n2\n2024-10-01 11:10:44\n\nression\n3\nNC\nSA\n2024-10-01 11:11:12\n",
        "Question": "are there notes for this lecture?",
        "Timestamp": 64,
        "TA Response": "The course notes are only updated through lecture 9 right now but the slides are here: https://ds100.org/fa24/lecture/lec10/",
        "Lecture": 10
    },
    {
        "Conversation History": "\t",
        "GPT Response": "No, the residual is not the \"r\" in the slope formula. The residual is defined as the observed value minus the estimated value, representing the difference between the actual data point and the predicted value from the regression model. In contrast, \"r\" in the slope formula refers to the correlation coefficient, which measures the strength and direction of the linear relationship between two variables.",
        "Recent Audio": "we're gonna we're gonna just plug in the value that we want to do prediction on into the equation. And the equation was intercept plus slope times the value. And then that would give you the regression estimation. There is a concept of residual that we're gonna play a lot with. And residual is defined as observed value minus the estimated value. OK, so the residuals are actually shown here on the plot. This is a residual. This is a residual, this is a residual and so on and so forth. What we're trying to do in data 100 and throughout the modeling is that we're looking at the residuals that we're accumulating throughout the entire data set and we're trying to minimize the amount of residual. And I'm going to define what amount of residual means formally using a function, mathematical function later on in today's lecture. But just know that if if a point is not on the line, that's an imperfect prediction and we're going to measure how imperfect. And we're going to try to make sure that the model that we're fitting to the data is passing through all the data points as accurately as possible. One observation here is that some residuals are positive, some residuals are negative. What you're seeing here is that, for example, for this specific data point, the estimation is exactly this. The, sorry, the actual value is this value right here on the Y axis, and the estimated value is the projection of that point onto.",
        "Recent Video": "Lec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g208c9a1170a_0_29\nNarges Norouzi\nO\n...\nO\nV\n[Data 8 Review] The Regression Line\nabo\nT\nFrom Data 8 (textbook):\n1380664\n45000\nThe regression line is the unique straight line\nthat minimizes the mean squared error of\n40000\nestimation among all straight lines.\n35000\n\u0e3f0000\nslope\nSD of y\n=\nr :\n25000\nSD of x\n20000\nCharacters\nintercept = average of y\n15000\n- slope x average of x\n10000\nregression estimate = intercept + slope x x\n100\n200\n-300\n400\nX=\nPeriods\nresidual\n=\nobserved y\nFor every chapter of the novel Little Women,\n-regression estimate\nEstimate the # of characters y based on the\nnumber of periods X in that chapter.\n11\nINC\nSA\n2024-10-01 11:23:20\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g208c9a1170a_0_29\nNarges Norouzi\n...\nO\nE\n[Data 8 Review] The Regression Line\nH\nab\nT\nFrom Data 8 (textbook):\n1380664\n45000\nThe regression line is the unique straight line\ny\nthat minimizes the mean squared error of\n40000\ny\nestimation among all straight lines.\n35000\nB0000\nslope\nSD of y\nr :\n25000\nSD of x\n20000\nCharacters\nintercept = average of y\n15000\n- slope x average of x\n10000\nregression estimate = intercept + slope x x\n100\n200\n-300\n400\nX=\nPeriods\nresidual\nobserved y\nFor every chapter of the novel Little Women,\n-regression estimate\nEstimate the # of characters y based on the\nnumber of periods X in that chapter.\n11\nCC\n11\n> :\n...\nS\nNC\n2024-10-01 11:24:47\n",
        "Retrieved Audio": "Is that we have a model and we're gonna we're gonna just plug in the value that we want to do prediction on into the equation. And the equation was intercept plus slope times the value. And then that would give you the regression estimation. There is a concept of residual that we're gonna play a lot with. And residual is defined as observed value minus the estimated value. OK, so the residuals are actually shown here on the plot. This is a residual.\nThere is a concept of residual that we're going to play a lot with and residual is defined as observed value minus the estimated value. OK, So the residuals are actually shown here on the plot. This is a residual, this is a residual, this is a residual and so on and so forth. What we're trying to do in data 100 and throughout the modeling is that we're looking at the residuals that we're accumulating throughout the entire data set and we're trying to minimize the.\nThe in data 8 we defined a slope as correlation or R times the standard division of y / a standard division of X.",
        "Retrieved Notes": "es/residual_plot.png\" alt='residual_plot' width='500'>\n      </td>\n    </tr>\n  </table>\n</div>\n### Multiple $R^2$\nFor SLR, we used the correlation coefficient to capture the association between the target variable and a single feature variable. In a multiple linear model setting, we will need a performance metric that can account for multiple features at once. **Multiple $R^2$**, also called the **coefficient of determination**, is the **proportion of variance** of our **fitted values** (predictions) $\\hat{\nture to understand the behavior of residuals. When working with several features in multiple linear regression, it no longer makes sense to consider a single feature in our residual plots. Instead, multiple linear regression is evaluated by making plots of the residuals against the predicted values. As was the case with SLR, a multiple linear model performs well if its residual plot shows no patterns.\n<div align=\"middle\">\n  <table style=\"width:100%\">\n    <tr align=\"center\">\n      <td>\n        <img src=\"imag\ny}_i$ to our true values $y_i$. It ranges from 0 to 1 and is effectively the *proportion* of variance in the observations that the **model explains**. \n$$R^2 = \\frac{\\text{variance of } \\hat{y}_i}{\\text{variance of } y_i} = \\frac{\\sigma^2_{\\hat{y}}}{\\sigma^2_y}$$\nNote that for OLS with an intercept term, for example $\\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_px_p$, $R^2$ is equal to the square of the correlation between $y$ and $\\hat{y}$. On the other hand for SLR, $R^2$ is equal to $",
        "Retrieved Video": "slope x average of x\n10000\nregression estimate = intercept + slope x x\n100\n200\n-300\n400\nX=\nPeriods\nresidual\n=\nobserved y\nFor every chapter of the novel Little Women,\n-regression estimate\nEstimate the # of characters y based on the\nnumber of periods X in that chapter.\n11\nINC\nSA\n2024-10-01 11:23:20\n\npe x average of x\n10000\nregression estimate = intercept + slope x x\n100\n200\n-300\n400\nX =\nPeriods\nresidual\n= observed y\nFor every chapter of the novel Little Women,\n-regression estimate\nEstimate the # of characters y based on the\nnumber of periods X in that chapter.\n11\nINC\nSA\n2024-10-01 11:23:22\n\ne x average of x\n10000\nregression estimate = intercept + slope x x\n100\n200\n-300\n400\nX =\nPeriods\nFor every chapter of the novel Little Women,\nEstimate the # of characters y based on the\nnumber of periods I in that chapter.\n11\nNC\nSA\n2024-10-01 11:23:01\n",
        "Question": "residual is the r in the slope formula correct?",
        "Timestamp": 842,
        "TA Response": "or she said she's going to talk about what r is? I missed what she defined r as.",
        "Lecture": 10
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Dividing by the standard deviation when converting a variable into standard units normalizes the data, ensuring that the transformed variable has a standard deviation of 1. This process centers the data around a mean of 0 and scales it so that each unit change in the standard units corresponds to one standard deviation in the original units. This standardization allows for meaningful comparisons between different datasets and is essential for calculating correlation, as it ensures that both variables are on the same scale.",
        "Recent Audio": "we needed to have correlation in order to figure out the relationship between X&X&Y. What we also know that is that the correlation shows us the linear association. It does not show us non-linear association. It does not show us causation in any sense. It just shows shows us the linear association or the degree of linear association between. Two variables X&Y and when the correlation is high enough, then in that case it will make sense to to just run simple linear regression or any regression analysis. Otherwise, if you don't see a linear relationship between two variables, why would you go and model it using a regression line? A regression value is always between -1 and one. You can prove it because we standardized all of these all of these data points. You can prove that the applying the equation is going to give you always a value between -1 and one. When are or, the correlation is close to one. That's. That shows a perfectly positive linear association. When the correlation is equal to -1, that's equal to perfect negative association. What is positive? What does negative mean? Positive means that by increasing, XY is also always increasing, so that's a positive is low, right? Negative correlation means that by negative. A strong correlation means that by increasing. X your Y is going to decrease, so that's a negative slope, and that's why correlation that you have is going to control whether you're going to get a positive slope or a negative slope.",
        "Recent Video": "Lec 10 - DS100 Fa24 - Modeling\nIx\nlec10.ipynb - JupyterLab\nx +\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g1150ea2fb2b_0_4101\nNarges Norouzi\n0\n[Data 8 Review] Correlation\nab/\nT\nFrom Data 8 (textbook):\nDefine the following:\n1380664\nThe correlation ? is the average of the\nproduct of x and y, both measured in\ndata D = {(x1, y1), (x2, y2), . , (xn, yn)}\nstandard units.\nmeans \", y standard deviations 0 x, Oy\nxi -x\n1\nn\nyi - y\n\"Ci in standard units:\nr =\n\" is also known as Pearson's correlation coefficient.\nn\ni=1\nO x\nOy\nSide note: covariance is\n(xi-x) (yi - y) = roxy\nn\ni=1\nCorrelation measures the strength of a linear association between two variables.\nIt ranges between -1 and 1.\n14\nNC\nSA\n2024-10-01 11:30:18\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\n\u00d7\n+\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g1150ea2fb2b_0_4101\nNarges Norouzi\nK\n...\n0\n[Data 8 Review] Correlation\nT\nFrom Data 8 (textbook):\nDefine the following:\n1380664\nThe correlation ? is the average of the\nproduct of x and y, both measured in\ndata D = {(x1, 91), (x2, y2), . , (xn, yn)}\nstandard units.\nmeans x, y standard deviations 0 x , Oy\nxi -x\n1\nn.\nCi - x\nyi - y\n\"Ci in standard units:\nr\n=\nT is also known as Pearson's correlation coefficient.\nn\ni=1\nOy\nSide note: covariance is\n(xi -x)(yi - y) = roxy\nn\nCorrelation measures the strength of a linear association between two variables.\nIt ranges between -1 and 1.\n0\nr = 1 indicates perfect linear association; r = - 1 perfect negative association.\nO\nThe closer r is to 0, the weaker the linear association is.\nIt says nothing about causation or nonlinear association.\n14\nSA\n2024-10-01 11:31:30\n",
        "Retrieved Audio": "Right, three pieces of information and let me tell you what they are. It's starting from measuring in a standard unit. When we say that a variable is in a standard unit, that means that it's 0 center to standard division of one, right? So the average of all the values should be 0 and then the standard division of all of the values should be equal to 1. And the way that we actually standardize the data is to go through this process right here that you take.\nBy standard deviation of that entire column or that set of values, this ensures that the standard division is equal to 1 as well. So that's what that's what we mean by a standardization. So that's a standard unit. So when we say product of X&Y both measured in a standard unit, this means that X measured in a standard unit is right here, Y measured in a standard unit is right here. So that's X&Y measured in standard unit and then.\nA column of values, or one set of values, you calculate their average. You subtract the average from each individual sample. So that's how you will make sure that it's 0 centered, meaning that the mean is at 0. And then in order to make sure that the standard deviation is equal to 1, you divide all of these transformations by standard deviation of that entire column or that set of values. This ensures that the standard division is equal to 1 as well. So that's what that's what we mean by.",
        "Retrieved Notes": "les $x$ and $y$ representing our data $\\mathcal{D} = \\{(x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\}$, we denote their means/averages as $\\bar x$ and $\\bar y$ and standard deviations as $\\sigma_x$ and $\\sigma_y$.\n#### Standard Units\nA variable is represented in standard units if the following are true:\n1. 0 in standard units is equal to the mean ($\\bar{x}$) in the original variable's units.\n2. An increase of 1 standard unit is an increase of 1 standard deviation ($\\sigma_x$) in the original variable's units.\n\nTo convert a variable $x_i$ into standard units, we subtract its mean from it and divide it by its standard deviation. For example, $x_i$ in standard units is $\\frac{x_i - \\bar x}{\\sigma_x}$.\n#### Correlation\nThe correlation ($r$) is the average of the product of $x$ and $y$, both measured in *standard units*.\n$$r = \\frac{1}{n} \\sum_{i=1}^n (\\frac{x_i - \\bar{x}}{\\sigma_x})(\\frac{y_i - \\bar{y}}{\\sigma_y})$$\n1. Correlation measures the strength of a **linear association** between two variables.\n2. Correlation\n00$) is tighter around the mean than the distribution with a smaller sample size ($n=200$). Try plugging in these values into the standard deviation equation for the sample mean to make sense of this! \nApplying the CLT allows us to make sense of all of this and resolve this issue. By drawing many samples, we can consider how the sample distribution varies across multiple subsets of the data. This allows us to approximate the properties of the population without the need to survey every single member. \nGiven",
        "Retrieved Video": "e x average of x\n10000\nregression estimate = intercept + slope x x\n100\n200\n-300\n400\nX =\nPeriods\nFor every chapter of the novel Little Women,\nEstimate the # of characters y based on the\nnumber of periods I in that chapter.\n11\nNC\nSA\n2024-10-01 11:23:01\n\nx average of x\n10000\nregression estimate = intercept + slope x x\n100\n200\n-300\n400\nX=\nPeriods\nresidual\nobserved y\nFor every chapter of the novel Little Women,\nregression estimate\nEstimate the # of characters y based on the\nnumber of periods X in that chapter.\n11\nNC\n2024-10-01 11:24:58\n\nslope x average of x\n10000\nregression estimate = intercept + slope x x\n100\n200\n-300\n400\nX=\nPeriods\nresidual\n=\nobserved y\nFor every chapter of the novel Little Women,\n-regression estimate\nEstimate the # of characters y based on the\nnumber of periods X in that chapter.\n11\nINC\nSA\n2024-10-01 11:23:20\n",
        "Question": "Could you explain what dividing by the SD to make standard units does? ",
        "Timestamp": 1248,
        "TA Response": "Standardizing our units restricts r to be between -1 and 1, which allows us to make conclusions about the relationship in the data. Without the standardization, r could be in any range and we would not be able to make useful conclusions about it.",
        "Lecture": 10
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The context provided does not mention any specific recent polls. However, it discusses the historical context of election polls, specifically the Gallup Poll, and its methods for predicting election outcomes. If you are asking about current polls, you might need to check a news source or polling website for the latest updates.",
        "Recent Audio": "Hats on top of teta means that there was a model that now this teta parameter for that model is the optimum set of parameters. And I'm going to show you what that means if a model is not optimum. Because technically when you define a model parameter or when you're defining a parametric model, you're just defining a function or a mathematic function. You're not trying to find out what is the optimum mathematic. That models your entire data set. So this is what it means. When we have a model, we are basically trying to have could define the family of functions that can be used in order to model our relationship. We would say that OK if I have the model parameters to be Theta, then the my model equation is going to be Y hat equal to Theta 0 plus Theta 1 * X. When I say that I have the best optimum Theta value I will go ahead and put. Had on top of it, which basically means that this time if you go ahead and calculate Y hat values using these optimum values, this is the best prediction that you can have an average on the entire data set that we have. Again, this right here is going to be a family of the models that can be applied to our data set, but there is only one set of values that is teta 0 hat and Theta one hat that are going to be optimal. OK and that's that's.",
        "Recent Video": "Lec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx | +\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00ils/edit#slide=id.g1150ea2fb2b_0_2561\nNarges Norouzi\n0\n7\n:\nModels in Data 100: Parametric Models\nab/\nT\nParametric models are described by a few parameters (00, 01, etc.)\n1380664\nNo one tells us the parameters: the data informs us about them.\nThe x, y values are not parameters because we directly observe them.\nSample-based estimate of parameter 0 is written as \u00ea.\nThe \"hat\" here is different from the \"hat\" in @: one means estimate and one means\nO\nprediction.\nUsually, we pick the parameters that appear \"best\" according to some criterion we\nchoose.\nModel parameter(s)\nAny linear model with\nparameters 0 = [00, 01]\nEstimated parameter(s),\n\"best\" fit to data in some sense\nThe \"best\" fitting linear model\nD\nwith parameters A= [00,0]\n22\nNC\n2024-10-01 11:47:30\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g1150ea2fb2b_0_2561\nNarges Norouzi\nV\n...\nO\nModels in Data 100: Parametric Models\nH\nab\nT\nParametric models are described by a few parameters (00, 01, etc.)\n1380664\nNo one tells us the parameters: the data informs us about them.\nThe x, y values are not parameters because we directly observe them.\nSample-based estimate of parameter 0 is written as \u00ea.\nO\nThe \"hat\" here is different from the \"hat\" in y: one means estimate and one means\nprediction.\nUsually, we pick the parameters that appear \"best\" according to some criterion we\nchoose.\nModel parameter(s)\nAny linear model with\nparameters 0 = [00, 01]\nEstimated parameter(s),\n\"best\" fit to data in some sense\nThe \"best\" fitting linear model\nD\nD\nwith parameters 0 = [00, 01]\n22\n2024-10-01 11:47:40\n",
        "Retrieved Audio": "all follow the trend that we're going to.\nKind of estimations of the values that we want.\nHigh enough then basically.",
        "Retrieved Notes": "oters tell the truth? How can we predict turnout? -- still remain, but the *Gallup Poll* uses several tactics to mitigate them. Within their sampling frame of \"civilian, non-institutionalized population\u00e2\u0080\u009d of adults in telephone households in continental U.S., they use random digit dialing to include both listed/unlisted phone numbers and to avoid selection bias. Additionally, they use a within-household selection process to randomly select households with one or more adults. If no one answers, re-call mul\ny had a dismal response rate (about 24\\%); who knows how the other non-respondents would have polled? The *Digest* folded just 18 months after this disaster.\nAt the same time, George Gallup, a rising statistician, also made predictions about the 1936 elections. Despite having a smaller sample size of \u00e2\u0080\u009conly\u00e2\u0080\u009d 50,000 (this is still more than necessary; more when we cover the Central Limit Theorem), his estimate that 56\\% of voters would choose Roosevelt was much closer to the actual result (61\\%). Gallup\n**\"No\"**, then the surveyor knows the **individual didn't cheat**. We assume the individual is comfortable revealing this information. \nGenerally, we can assume that the coin lands heads 50% of the time, masking the remaining 50% of the \"No\" answers. We can therefore **double** the proportion of \"No\" answers to estimate the **true** fraction of \"No\" answers.\n**Election Polls**\nToday, the *Gallup Poll* is one of the leading polls for election results. The many sources of biases -- who responds to polls? Do v",
        "Retrieved Video": "08\n\nA\n2024-10-01 11:34:42\n\n 0 ms\n9\nNC\nSA\n2024-10-01 11:19:16\n",
        "Question": "Any polls yet?",
        "Timestamp": 2225,
        "TA Response": "Nope",
        "Lecture": 10
    },
    {
        "Conversation History": "\t",
        "GPT Response": "In the context of the lecture, \\( x_i \\) is not an index but rather represents the \\( i \\)-th data point or feature in a dataset. The subscript \\( i \\) is used to denote individual elements within a collection of data points, such as features or observations, in a parametric model. This notation helps in understanding the relationship between each feature and its corresponding weight in the model.",
        "Recent Audio": "Theta and you pass me Theta or you pass me Theta hat. I know what model you're using, I know what parameters you're using and I can go ahead and use those those parameters, right? Instead of just go through ABCD and so on and so forth. This way just using by using the subscript of of I Tata 0, Tata 1, Tata 2. And so on and so forth. What we can do is we can consolidate them into a teta vector and then we can index into the relationship between each feature has what weight more accurately or more easily. So that's kind of the notation that we'll be using. I said here we have a parametric model. We have seen non parametric models before. So some statistical models do not have parameters. For example, we have seen Kdes K DS were models or kernel density. But we did not set any parameters. We would just estimate alpha and then we would just run a process and we would have K, DS, K nearest neighbours is also another, another model that you did not need to set parameters or there was not no parameter to learn. So those are the cases that there are models that are non parametric, there are models that are parametric. What we're dealing with is parametric models and what we're going to do. Throughout this multiple lecture is to learn those parameters, How to optimally learn those parameters? How to derive those parameters so that we can uniquely have.",
        "Recent Video": "Lec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g2b98b5b8964_0_21\nNarges Norouzi\n0\n:\nModels in Data 100: Parametric Models\nabo\nT\nParametric models are described by a few parameters (00, 01, etc.)\n1380664\nNo one tells us the parameters: the data informs us about them.\nThe x, y values are not parameters because we directly observe them.\nSample-based estimate of parameter 0 is written as \u00f4.\nThe \"hat\" here is different from the \"hat\" in y: one means estimate and one means\nO\nprediction.\nUsually, we pick\nNote: Not all statistical models have parameters!\nhe criterion we\nchoose.\nKDEs, k-Nearest Neighbor classifiers are\nModel paramet\nnon-parametric models.\ninear model with\nparameters\n0 = [00, 0]\n1\nA\nEstimated parameter(s),\n\"best\" fit to data in some sense\ny = 00 + \u00d41x\nThe \"best\" fitting linear model\nwith parameters 0 = [00, 0]\nA\n23\n2024-10-01 11:49:06\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nX\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g1150ea2fb2b_0_3035\nNarges Norouzi\n-\n:\nO\nK\nThe Modeling Process\nH\nb\nT\n1380664\n1. Choose a model\nHow should we represent the world?\n24\nC\nHY\nNC\n2024-10-01 11:49:59\n",
        "Retrieved Audio": "",
        "Retrieved Notes": "",
        "Retrieved Video": "",
        "Question": "is xi the index?",
        "Timestamp": 2370,
        "TA Response": "xi is our ith input datapoint",
        "Lecture": 10
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Using multiple loss functions simultaneously can be challenging because each loss function may prioritize different aspects of the model's performance, potentially leading to conflicting objectives. For instance, one loss function might focus on minimizing overall error, while another might emphasize reducing errors for specific cases, such as outliers or false negatives. Optimizing for all of them individually could result in a model that doesn't perform well on any single metric, while an aggregate method might dilute the importance of critical errors. Therefore, the choice of a loss function should align with the specific goals and priorities of the modeling task.",
        "Recent Audio": "from Y, in the cases that the predictions are the line was far from the actual individual values, we would give a high loss. That's the general idea. Her given data point, we would calculate loss, we would calculate the error, and we need to have a function that will actually penalize. The model based on each one of those miscalculations or mis estimations that it has. So the choice of the loss function is really important in data 100, we're going to cover I think 3 or 4 different types of loss functions that will actually do exactly the same thing. It will tell us how bad our predictions were and it will basically tell us that it will penalize us more if our prediction is far from the actual value and it will penalize us less if our prediction is close to the actual target value but. Choice of the loss function rule, it depends on multiple different metrics and different types of modelling that you're doing. For example, if you're doing a classification task versus a regression task, the loss function is going to be very different. If you're doing a classification task, for example, if you're doing cancer identification, in a sense, you really don't want to miss any potential cancer patients. So in those cases you really want to have a different kinds of loss function that accounts for all of those cases. As well. So we're going to take a look at all of those considerations when we go into.",
        "Recent Video": "Lec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\nC \u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g1150ea2fb2b_0_3079\nNarges Norouzi\n-\nO\nLoss Functions\nH\nab\nT\nWe need some metric of how \"good\" or \"bad\" our predictions are.\n1380664\nA loss function characterizes the cost, error, or fit resulting from a particular choice of model\nor model parameters.\nLoss quantifies how bad a prediction is for a single observation.\nIf our prediction y is close to the actual value y, we want low loss.\nL(y,9)\nIf our prediction y is far from the actual value y, we want high loss.\n32\nNC\n2024-10-01 11:56:18\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nX\n->\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g1150ea2fb2b_0_3079\nNarges Norouzi\n...\n0\nLoss Functions\nT\nWe need some metric of how \"good\" or \"bad\" our predictions are.\n1380664\nA loss function characterizes the cost, error, or fit resulting from a particular choice of model\nor model parameters.\nLoss quantifies how bad a prediction is for a single observation.\nIf our prediction y is close to the actual value y, we want low loss.\nL(y,\u0177)\nIf our prediction y is far from the actual value y, we want high loss.\nThere are many definitions of loss functions!\nThe choice of loss function:\nAffects the accuracy and computational cost of estimation.\nDepends on the estimation task:\nAre outputs quantitative or qualitative?\nO\nDo we care about outliers?\nO\nAre all errors equally costly? (e.g ., false negative on cancer test)\n32\nNC\nSA\n2024-10-01 11:56:47\n",
        "Retrieved Audio": "Choosing a loss function, how can we quantify how good or bad a function?\nSo choosing a loss function, how can we quantify how good or bad a function?\nAs well. So we're going to take a look at all of those considerations when we go into.",
        "Retrieved Notes": "ut ideally, we want to understand how our model performs across our *entire* dataset. A natural way to do this is to compute the average loss across all data points in the dataset. This is known as the **cost function**, $\\hat{R}(\\theta)$:\n$$\\hat{R}(\\theta) = \\frac{1}{n} \\sum^n_{i=1} L(y_i, \\hat{y}_i)$$\nThe cost function has many names in the statistics literature. You may also encounter the terms:\n* Empirical risk (this is why we give the cost function the name $R$)\n* Error function\n* Average loss\nWe can s\n{-1}\\mathbb{X}^T \\mathbb{Y}$.\nOne thing to note, however, is that the techniques we used above can only be applied if we make some big assumptions. For the calculus approach, we assumed that the loss function was differentiable at all points and that we could algebraically solve for the zero points of the derivative; for the geometric approach, OLS *only* applies when using a linear model with MSE loss. What happens when we have more complex models with different, more complex loss functions? The techniques\nThere are 4 ways to pick $K$: the elbow method, silhouette scores, and by harnessing real-world metrics.\nThere are many machine learning problems. Each can be addressed by many different solution techniques. Each has many metrics for evaluating success / loss. Many techniques can be used to solve different problem types. For example, linear models can be used for regression and classification.\nWe\u00e2\u0080\u0099ve only scratched the surface and haven\u00e2\u0080\u0099t discussed many important ideas, such as neural networks and deep",
        "Retrieved Video": "al value y, we want low loss.\nL(y, g)\nIf our prediction y is far from the actual value y, we want high loss.\nThere are many definitions of loss functions!\nThe choice of loss function:\nAffects the accuracy and computational cost of estimation.\nDepends on the estimation task:\nAre outputs quantitative or qualitative?\nO\nDo we care about outliers?\nO\nAre all errors equally costly? (e.g ., false negative on cancer test)\n32\nNC\nSA\n2024-10-01 11:56:48\n\nl value y, we want low loss.\nL(y,\u0177)\nIf our prediction y is far from the actual value y, we want high loss.\nThere are many definitions of loss functions!\nThe choice of loss function:\nAffects the accuracy and computational cost of estimation.\nDepends on the estimation task:\nAre outputs quantitative or qualitative?\nO\nDo we care about outliers?\nO\nAre all errors equally costly? (e.g ., false negative on cancer test)\n32\nNC\nSA\n2024-10-01 11:56:47\n\nfer association not causation\ncreating models that are simple\nan uninterpretable model. These\nand interpretable, allowing us to\nare sometimes called black-box\nThis won't be the focus of this\nunderstand what the relationships\nmodels, and are common in fields\nclass, but will be if you go on to\nbetween our variables are.\nlike deep learning.\ntake more advanced classes (Stat\n156, Data 102)\nMost of the time, we want to strike a balance between\ninterpretability and accuracy.\n7\nNC\n2024-10-01 11:17:56\n",
        "Question": "Why can't we use a set of loss functions and try to optimise for all of them? Either individually or in an aggregate method?",
        "Timestamp": 2833,
        "TA Response": "I believe using a set of loss functions has been considered for training certain neural networks, but in most cases and for the purposes of this class we tend to only optimize for 1. Using multiple loss functions together can be problematic if not done carefully, as one loss may optimize for different things than the other ones",
        "Lecture": 10
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Squared loss is often preferred over absolute loss when you want to penalize larger errors more heavily. This is because squared loss increases the penalty quadratically with the size of the error, making it more sensitive to large deviations. However, the drawback is that squared loss can be disproportionately affected by outliers, potentially skewing the model. Absolute loss, on the other hand, treats all errors linearly, which can be more robust to outliers but might not emphasize larger errors as much.",
        "Recent Audio": "residuals proportionally, what is the loss fund? Which loss function is more ideal? Do you want to use absolute loss or do you want to use this word loss? Looks like a score loss is the winner because of the example that I gave, right? So if you really want to penalize large miscalculations or mispredictions, you really want to use this squared loss in those cases. What would be the drawback if we penalize large deviations so much? There is one drawback in that case. Just a quick hint, would be an outlier. Anything about outliers might be an issue here. Yeah. Yeah. So the answer was that the outliers are generally far from the trend of the data and the model. If model wants to try to avoid having that squared large loss that it might incur, it might tilt the line or the model from the actual trend that it needs to be to the trend that's kind of meets halfway in between. So that it gets some small loss on the actual trend of the data and also gets closer to. To the to the outlier data point that it has. What about this question? Why don't we use the actual residual values directly? Why do we score them? Why do we calculate absolute value? Why do we want all of them to be all of the cost values to be positive? Why do we want to get rid of negatives? Sum will be 0 residual sum to zero. That is correct.",
        "Recent Video": "v\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n>\nC\ndocs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00ils/edit#slide=id.SLIDES_API155760666_0\nNarges Norouzi\n0\ny\n:\nK\n\u00b7\u00b7 \u00b7 80 participants typing\nSynchronous\nH\nabp\nT\n58 g\nWhy don't we use residual error directly and instead we use absolute loss or\nsquared loss?\nerrors can cancel out this ensures that it will be positive\ncost\nto penalize and also see the absolute differences\nSum will be 0.\nget cancel\nSo they don't cancel out\nStandardize\nto compare\nFor comparison\nnegatives\nto standardize\ndistance\nprecision\nJoin at\nStandardization\nwould equal 0\nidk\nto get rid of negative\nslido.com\nAvoid negative\nnegative\nget rid of negative\nstandarization\n#1380 664\nTo not have negatives\nFelix\nwe want a positive cost\nresiduals sum to zero\nso the errors don't cancel out\nstandardize errors\nUniformity\n0\nminimal\n2024-10-01 12:03:06\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\n\u00d7\n+\n->\nC\n5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.SLIDES_API155760666_0\nNarges Norouzi\n...\n\u00b7\u00b7\u00b7 100+ participants typing\nSynchronous\nH\nabo\nT\n113 g\nWhy don't we use residual error directly and instead we use absolute loss or\nsquared loss?\nsum of resiauls will be 0\nalso see the absolute difference\nget cancel\nlarge\nto compare\nSo they don't cancel out\nyknow\nresiduals sum to zero\nto measure\nmay\nto standardize\npositive\nSum will be 0.\nFor comparison\nCancel out\ncancel\nstandardize\ndistance\nerror\nJoin at\nAvoid negative\nwould equal 0\nprecision\nidk\nvalues\nstandardization\nmean make\nslido.com\nrid\nnegative\nget rid of negative\nAvoid canceling out\n#1380 664\nminimal\nstandardize errors\nstuf\nFelix\nstandarization\nnegatives\nreality\nthis ensures that it will be positive\ntoeget nidaqfostgativesfor standard\nsun is going to be 0!\nto get rid of negative\nno negative\nlarge\n2024-10-01 12:03:11\n",
        "Retrieved Audio": "Looks like a score loss is the winner because of the example that I gave, right? So if you really want to penalize large miscalculations or mispredictions, you really want to use this squared loss in those cases. What would be the drawback if we penalize large deviations so much? There is one drawback in that case.\nLooks like a score loss is the winner because of the example that I gave, right? So if you really want to penalize large miscalculations or mispredictions, you really want to use this squared loss in those cases. What would be the drawback if we penalize large deviations so much? There is one drawback in that case.\nWhat about this question? Why don't we use the actual residual values directly? Why do we score them? Why do we calculate absolute value? Why do we want all of them to be all of the cost values to be positive?",
        "Retrieved Notes": "ss and absolute loss. \n**Squared loss**, also known as **L2 loss**, computes loss as the square of the difference between the observed $y_i$ and predicted $\\hat{y}_i$:\n$$L(y_i, \\hat{y}_i) = (y_i - \\hat{y}_i)^2$$\n**Absolute loss**, also known as **L1 loss**, computes loss as the absolute difference between the observed $y_i$ and predicted $\\hat{y}_i$:\n$$L(y_i, \\hat{y}_i) = |y_i - \\hat{y}_i|$$\nL1 and L2 loss give us a tool for quantifying our model's performance on a single data point. This is a good start, b\nm and a (barely perceptible) local minimum in the loss surface above. This means that there is the risk of gradient descent converging on the local minimum of the loss surface, missing the true optimum parameter $\\theta_1$.\n<center><img src=\"images/global_local_min.png\" alt='reg' width='400'></center>\n2. Squared loss is *bounded* for a classification task. Recall that each true $y$ has a value of either 0 or 1. This means that even if our model makes the worst possible prediction (e.g. predicting $p=0$ for \nubstitute our L1 and L2 loss into the cost function definition. The **Mean Squared Error (MSE)** is the average squared loss across a dataset:\n$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\nThe **Mean Absolute Error (MAE)** is the average absolute loss across a dataset:\n$$\\text{MAE}= \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$$\n## Fitting the Model\nNow that we've established the concept of a loss function, we can return to our original goal of choosing model parameters. Specifically, we want ",
        "Retrieved Video": "v\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.SLIDES_API155760666_0\nNarges Norouzi\n0\nK\n7 :\n\u00b7\u00b7\u00b7 40 participants typing\nSynchronous\nH\nabo\nT\nA\n6 83\nWhy don't we use residual error directly and instead we use absolute loss or\nsquared loss?\nStandardization\nidk\nJoin at\nto get rid of negative\nslido.com\nminimal\n#1380 664\nget rid of negative\nA\n2024-10-01 12:02:56\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.SLIDES_API155760666_0\nNarges Norouzi\n...\nO\nK\n\u00b7\u00b7\u00b7 34 participants typing\nSynchronous\nA\nH\nabo\nT\n3 8\nWhy don't we use residual error directly and instead we use absolute loss or\nsquared loss?\nidk\nJoin at\nto get rid of negative\nslido.com\nget rid of negative\n#1380 664\nA\n2024-10-01 12:02:53\n\ncomparison\nnegatives\nto standardize\nSo they don't cancel out distance precision\nJoin at\ncost is positive\nwould equal 0\no idk\nAvoid negative\nstandardization\nslido.com\nminimal\nnegative\nget rid of negative\nstandarization\n#1380 664\nresiduals sum to zero\nFelix\nwe want a positive cost\nso the errors don't cancel out\nnegatives will skew the data\nto get rid of negative for standard\nUniformity\nstandardize errors\nto get rid of negative\n2024-10-01 12:03:08\n",
        "Question": "I am confused why we should use squared but not absolute loss from the last question",
        "Timestamp": 3148,
        "TA Response": "We take the absolute value because we don't want large negative residuals to cancel out large positive residuals when we sum them up. This cancelling out would make the true error appear smaller than it really is. Taking the absolute value gives us the magnitude of each error.",
        "Lecture": 10
    },
    {
        "Conversation History": "\t",
        "GPT Response": "The lecture is discussing the concept of empirical risk, which is the average loss over the entire data set. This involves calculating the loss for each data point, summing these losses, and then dividing by the number of data points to get the average. This approach ensures that the size of the data set does not influence the decision about which model performs better. The focus is on evaluating the model's predictions across the entire data set rather than just individual points.",
        "Recent Audio": "values are positive, some residual values are negative, right? This is what we saw before. And if we just actually get the residual values and sum them up. And the issue is going to be they're going to cancel each other out. And again, we need to calculate the cost and aggregate in the entire data set. So that's why we really want to make sure that all of them are positive cost values. And that's why we're using either absolute loss or a ^2 loss. In all of these cases. I said we need to make sure that we calculate the loss for all of the data points. In fact, the way that we do it is that we will go ahead and sum up all of these. Sum of all of these lost values for all of the data points. So you see YIYI hat that means that the prediction for data point I and assuming that we have N different data points, we will sum up all of those lost values that we have and then we will divide it by N, which is going to be calculating the average, right? Summing up makes sense because I want to take a look at the entire data set when I'm dividing it by N This is the case that if I'm comparing the model performances on different. I don't want the the size of the data set to be influencing my decision about which model is performing better right? Or which model is performing.",
        "Recent Video": "Lec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n>\nC \u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk|4RvnqKxYS00i6ls/edit#slide=id.g1150ea2fb2b_0_3532\nNarges Norouzi\n:\nEmpirical Risk is Average Loss over Data\nH\nab\nT\nWe care about how bad our model's predictions are for our entire data set, not just for one\n1380664\npoint.\nA natural measure, then, is of the average loss (aka empirical risk) across all points.\nGiven data D = {(x1, 91), (x2, y2), . .., (In, yn)} :\nR(0) = = >L(yi, Vi)\nn\n2\n38\nNC\nSA\n2024-10-01 12:03:59\n\nLec 10 - DS100 Fa24 - Modeling X\nlec10.ipynb - JupyterLab\nx +\n3% docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g1150ea2fb2b_0_3532\nNarges Norouzi\nO\n:\nEmpirical Risk is Average Loss over Data\nab\nT\nWe care about how bad our model's predictions are for our entire data set, not just for one\n1380664\npoint.\nA natural measure, then, is of the average loss (aka empirical risk) across all points.\nGiven data D = {(x1, 91), (x2, y2), . .., (In, yn) } :\nR(A) =\n1\n-\nL(yi, \u011fi)\nn\n2\n38\nNC\n2024-10-01 12:04:08\n",
        "Retrieved Audio": "Yeah.\nYeah.\nthat.",
        "Retrieved Notes": "\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\n\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000",
        "Retrieved Video": "08\n\n-01 11:43:32\n\n-01 11:25:35\n",
        "Question": "Audio cut out",
        "Timestamp": 3262,
        "TA Response": "I believe it should be back?",
        "Lecture": 10
    },
    {
        "Conversation History": "\t",
        "GPT Response": "Mean Squared Error (MSE) is actually quite effective at amplifying larger errors because it squares the residuals, which means that larger errors have a disproportionately larger impact on the overall error metric. This is why MSE is often used when you want to penalize large deviations more heavily. However, the drawback is that it can be overly sensitive to outliers, which might not always be desirable depending on the context of the model's application. In contrast, a linear model without squared loss might not emphasize larger errors as much, potentially leading to a different balance in error handling.",
        "Recent Audio": "moving this summation out is going to give me 1 / n times the summation of I from 1 up to north of the derivative of what I had, which is why I minus Theta 0 minus Theta one XI to the power of 2 with respect to Theta 0, right? So first step is easy. I just basically took out the summation and said that. The derivative of the summation is equal to some of the derivatives, so I'm taking some of those derivatives. So the next step is that you're seeing that I have a second order function here. If I want to take a derivative with respect to teta 0, teta 0 inside is raised to the power of 2. So you know that in this case I need to apply chain rule first, meaning that I basically try to take the derivative of the outer function and then take the take the derivative of the inner function, which in this case is going to be me rewriting. The coefficient right here 1 / n times the summation of I from. One up to north of. So if I apply the chain rule, that's function to the power of two. If I take the derivative, that's equal to two times function times derivative of function, right? If you think about this as function to the power of 2. So what I'll do is I'll say that two times the function which is equal to Yi minus Theta 0 minus Theta one times XI.",
        "Recent Video": "Lec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00ils/edit#slide=id.g157c66916f5_0_328\nNarges Norouzi\nO\n:\nO\nK\nPartial Derivative of MSE with Respect to 00, 01\nT\nV\nab\nn\nyi\n1380664\n20%\nMSE\n0 1\n=\nC(yi-00-012;)\n0\ni=1\nn\nDerivative of\n1[ (y ;- 00 -Q1x;)2\nsum is sum\nof derivatives\nn ;= 1\n' do.\n44\nNC\n2024-10-01 12:12:04\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g157c66916f5_0_328\nNarges Norouzi\nK\n:\nO\nPartial Derivative of MSE with Respect to 00, 01\nH\nab\nT\nMSE =\n0 1\nn\nyi\nC(yi-00-0120;)2\n1380664\n0\ni=1\nn\nDerivative of\n12 (y ;- 00-Qx;)\nsum is sum\nof derivatives\nn ; = 1\ndo.\n7\n[2(y-60-01x;)\nn\nChain rule\nn\ni=\n44\nHY\nSA\n2024-10-01 12:12:56\n",
        "Retrieved Audio": "Why I hat which is the residual to the power of 2 that I'm calculating which is mean a ^2 error that I want to calculate?\nLooks like a score loss is the winner because of the example that I gave, right? So if you really want to penalize large miscalculations or mispredictions, you really want to use this squared loss in those cases. What would be the drawback if we penalize large deviations so much? There is one drawback in that case.\nLooks like a score loss is the winner because of the example that I gave, right? So if you really want to penalize large miscalculations or mispredictions, you really want to use this squared loss in those cases. What would be the drawback if we penalize large deviations so much? There is one drawback in that case.",
        "Retrieved Notes": "h='600'></center><br>\nNote that the validation error behaves quite differently from the training error explored previously. As the model becomes more complex, it makes better predictions on the training data; the variance of the model typically increases as model complexity increases. Validation error, on the other hand, decreases *then increases* as we increase model complexity. This reflects the transition from under- to overfitting: at low model complexity, the model underfits because it is not complex e\nng, Test, and Validation Sets\n<center><img src=\"images/simple_under_overfit.png\" alt='train-test-split' width='400'></center><br>\nFrom the last lecture, we learned that *increasing* model complexity *decreased* our model's training error but *increased* its  variance. This makes intuitive sense: adding more features causes our model to fit more closely to data it encountered during training, but it generalizes worse to new data that hasn't been seen before. For this reason, a low training error is not alway\nt Mean Square Error (RMSE). However, is RMSE a good proxy for fairness in this context?\nBroad metrics of error like RMSE can be limiting when evaluating the \"fairness\" of a property appraisal system. RMSE does not tell us anything about the distribution of errors, whether the errors are positive or negative, and the relative size of the errors. It does not tell us anything about the regressivity of the model, instead just giving a rough measure of our model's overall error. \nEven with a low RMSE, we can't g",
        "Retrieved Video": "v\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.SLIDES_API155760666_0\nNarges Norouzi\n0\nK\n7 :\n\u00b7\u00b7\u00b7 40 participants typing\nSynchronous\nH\nabo\nT\nA\n6 83\nWhy don't we use residual error directly and instead we use absolute loss or\nsquared loss?\nStandardization\nidk\nJoin at\nto get rid of negative\nslido.com\nminimal\n#1380 664\nget rid of negative\nA\n2024-10-01 12:02:56\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.SLIDES_API155760666_0\nNarges Norouzi\n...\nO\nK\n\u00b7\u00b7\u00b7 34 participants typing\nSynchronous\nA\nH\nabo\nT\n3 8\nWhy don't we use residual error directly and instead we use absolute loss or\nsquared loss?\nidk\nJoin at\nto get rid of negative\nslido.com\nget rid of negative\n#1380 664\nA\n2024-10-01 12:02:53\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.SLIDES_API155760666_0\nNarges Norouzi\n...\n0\nE\n\u00b7\u00b7\u00b7 14 participants typing\nSynchronous\nabo\nT\nA\n2 8\nWhy don't we use residual error directly and instead we use absolute loss or\nsquared loss?\nidk\nJoin at\nto get rid of negative\nslido.com\n#1380 664\n2024-10-01 12:02:52\n",
        "Question": "I am confused why mean squared error is not better than linear model for amplifying larger errors.",
        "Timestamp": 3749,
        "TA Response": "Hi, could you please clarify your question? Are you asking why squaring the residuals is better than taking the absolute value of residuals for amplifying larger errors?",
        "Lecture": 10
    },
    {
        "Conversation History": "\t",
        "GPT Response": "We set the partial derivatives of the Mean Squared Error (MSE) to zero to find the critical points, which help us determine the optimal parameter values for our model. In the context of MSE, we are specifically looking for the minimum because MSE is a convex function, and its critical point corresponds to a global minimum. MSE is a loss function that measures the average squared difference between the observed actual outcomes and the outcomes predicted by the model. By minimizing MSE, we aim to find the model parameters that result in the best fit to the data.",
        "Recent Audio": "0 minus Theta one XI times XI, right? Look at both sides, look at that function F thing that's in common between them. So one is effectively telling me that if I go ahead and set this to be equal to 01, the the left hand side is effectively telling me that the summation of all of the residuals should be. To 0 because that's residual and that's why they were canceling each other out. That's why they couldn't be a cost function, right? This is telling me that at the point that tetanus 0 is optimum, the summation of residuals should be equal to 0 and then the right hand side is basically telling me that. The summation of residuals times X is should be equal to 0, right? We're going to get to that. And what I need to do right now is that I have two equations. And I have two unknowns. And what are the unknowns that I have? So I have two equations. There is one unknown teta zero and there is one unknown teta one. I said it. I said most of them equal to 0. And then I solve it for the optimum test to 0 and optimum Theta one. And that will give me Theta 0 hat and Theta one hat which are the optimum values that we want. Again, you'll get this, this slide with animation and fancy animation. So you will get a clean version of all of these. So this is where we left off.",
        "Recent Video": "Lec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx +\n->\nC\n5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00ils/edit#slide=id.g1e81cf05998_0_0\nNarges Norouz\n0\n7\n:\nK\nPartial Derivative of MSE with Respect to 00, 01\nab\nT\n0 1\nn\nn\n1380664\nMSE =\n0 1\n1\ndoon\n(yi-00-01x;)2\n001\nMSE =\nafin\n(yi-00-01x;)2\ni=1\ni=1\nn\nn.\n00\nDerivative of\n1\nDerivative of\n1\nsum is sum\n(yi-00-01x;)2\nsum is sum\n=\nyi-00 -012;)2\nof derivatives\nn\nof derivatives\nn\n201\ni=1\ni=1\n1\nn\n1\nn\nChain rule\n=\n2(yi-00-01xi)(-1)\nChain rule\n=\n2(yi - 00 - 01xi)(-Xi)\nn\nn\ni=1\ni=1\nn\nSimplify\n2\nn\n2\n>(yi -00 -0,20;)\nSimplify\n\u2211(yi-00-01xi)xi\nconstants\nconstants\nn\nn\ni=1\ni=1\n45\nINC\n2024-10-01 12:17:16\n\nLec 10 - DS100 Fa24 - Modeling\nlec10.ipynb - JupyterLab\nx +\nx\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g208c9a1170a_0_803\nNarges Norouzi\n0\n:\nEstimating Equations\nab\nT\nTo find the best values, we set derivatives equal to zero to obtain the optimality conditions:\n1380664\n0 =\n2\nn\n200\nMSE =\n-\nE(yi -00 -01xi)\nn\ni=1\nn\n60\n0 =\nMSE\n-\n->(yi -00 -A1x; ) xi\nn\ni=1\n47\nNC\nSA\n2024-10-01 12:17:20\n",
        "Retrieved Audio": "Data points in the data set and my my entire empirical risk function is going to be MSE or mean squared error. Again, this is the mean and this is the squared error of all of those predictions that we have. This is why I This is why I had that we have based on the simple linear regression. What you know from data 8 is that you know that they called this the linear regression that.\nData points in the data set and my my entire empirical risk function is going to be MSE or mean scored error. Again, this is the mean and this is the squared error of all of those predictions that we have. This is why I This is why I had that we have based on the simple linear regression. What you know from data 8 is that you know that they called this the linear regression that.\nSimple linear regression pattern or function together with the optimization function that is mean squared error is called least squares regression because we're trying to find a function that minimizes these scores of the errors that we have over the entire population. So now down to the fun part. I said that if you have the two parts in the model, you have the model.",
        "Retrieved Notes": "ve function:\n1. Differentiate with respect to $\\theta$.\n2. Set equal to 0.\n3. Solve for $\\hat{\\theta}$.\n4. (If we have multiple parameters) repeat steps 1-3 with partial derivatives.\nRecall critical points from calculus: $R(\\hat{\\theta})$ could be a minimum, maximum, or saddle point!\n- We should technically also perform the second derivative test, i.e., show $R''(\\hat{\\theta}) > 0$.\n- MSE has a property\u00e2\u0080\u0094**convexity**\u00e2\u0080\u0094that guarantees that $R(\\hat{\\theta})$ is a global minimum.\n- The proof of convexity \nubstitute our L1 and L2 loss into the cost function definition. The **Mean Squared Error (MSE)** is the average squared loss across a dataset:\n$$\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\nThe **Mean Absolute Error (MAE)** is the average absolute loss across a dataset:\n$$\\text{MAE}= \\frac{1}{n} \\sum_{i=1}^n |y_i - \\hat{y}_i|$$\n## Fitting the Model\nNow that we've established the concept of a loss function, we can return to our original goal of choosing model parameters. Specifically, we want \n(y_i - \\theta_0 - \\theta_1 x_i)x_i$$\nLet's walk through these derivations in more depth, starting with the derivative of MSE with respect to $\\theta_0$.\nGiven our MSE above, we know that:\n$$\\frac{\\partial}{\\partial \\theta_0} \\text{MSE} = \\frac{\\partial}{\\partial \\theta_0} \\frac{1}{n} \\sum_{i=1}^{n} {(y_i - \\theta_0 - \\theta_1 x_i)}^{2}$$\nNoting that the derivative of sum is equivalent to the sum of derivatives, this then becomes:\n$$ = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial}{\\partial \\theta_0} {(y_i - \\th",
        "Retrieved Video": "?\nMSE for SLR\nThe combination of model + loss\n4. Evaluate model\nHow do we evaluate\nperformance\nwhether this process gave\nthat we focus on today is known as\nleast squares regression.\nrise to a good model?\n40\nINC\n2024-10-01 12:07:59\n\nLec 10 - DS100 Fa24 - Modelinc X\nlec10.ipynb - JupyterLab\nx | +\nC\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g157c66916f5_0_328\nNarges Norouzi\nO\nK\n:\n0\nV\nPartial Derivative of MSE with Respect to 00, 01\nH\nab\nT\nn\n1380664\nMSE =\n0 1\n(yi-00-012;)2\ni=1\n44\nHY\nNC\nSA\n2024-10-01 12:10:05\n\nLec 10 - DS100 Fa24 - Modeling\nX\nlec10.ipynb - JupyterLab\nx +\n->\n\u00ba5 docs.google.com/presentation/d/10yDDiJGWhNdOG5K7WpHqYtT39mk14RvnqKxYS00i6ls/edit#slide=id.g208c9a1170a_0_803\nNarges Norouzi\n:\nEstimating Equations\nab\nT\nTo find the best values, we set derivatives equal to zero to obtain the optimality conditions:\n1380664\nn\n0 =\nMSE =\n2\n--\nyi - \u00d40 - 1xi )\n60\nn\ni=1\n60\n0 =\n2\nn\n201\nMSE\n--\n>(yi -Do - O(Ii ) xi\nn\ni=1\n47\nNC\nSA\n2024-10-01 12:17:22\n",
        "Question": "Why are we setting the partial derive of MSE = 0. Are we trying to get the minimum or maximum? What is MSE",
        "Timestamp": 4002,
        "TA Response": "If you remember from calculus, we set the derivative to zero in order to find the \"extreme\" values of the function. Here, we want our model to minimize the MSE, which is the mean squared error (our loss function!).",
        "Lecture": 10
    }
]